<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>pipapr.stitcher API documentation</title>
<meta name="description" content="Submodule containing classes and functions relative to **stitching** …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pipapr.stitcher</code></h1>
</header>
<section id="section-intro">
<p>Submodule containing classes and functions relative to <strong>stitching</strong>.</p>
<p>With this submodule the user can stitch a previously parsed dataset, typically the autofluorescence channel:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; import pipapr
&gt;&gt;&gt; tiles_autofluo = pipapr.parser.tileParser(path_to_autofluo, frame_size=1024, overlap=25)
&gt;&gt;&gt; stitcher = pipapr.stitcher.tileStitcher(tiles_autofluo)
&gt;&gt;&gt; stitcher.compute_registration_fast()
</code></pre>
<p>Others channel can then easily stitched using the previous one as reference:</p>
<pre><code class="language-python-repl">&gt;&gt;&gt; tiles_signal = pipapr.parser.tileParser(path_to_data, frame_size=1024, overlap=25)
&gt;&gt;&gt; stitcher_channel = pipapr.stitcher.channelStitcher(stitcher, tiles_autofluo, tiles_signal)
&gt;&gt;&gt; stitcher_channel.compute_rigid_registration()
</code></pre>
<p>Doing that each tile in the second data set will be registered to the corresponding autofluorescence tile and
then their spatial position will be adjusted.</p>
<p>WARNING: when stitching, the expected overlap must be HIGHER than the real one. To enforce this, a margin of 20% is
automatically taken (this margin can be set lower by the user for speed improvement). In order to get the best stitching
quality it requires to have a good estimate of the overlap, hence why the full volume is not considered.</p>
<p>This submodule also contains a class for merging and reconstructing the data. It was intended to be used at lower
resolution for atlasing. The generated data can quickly become out of hands, use with caution!</p>
<p>By using this code you agree to the terms of the software license agreement.</p>
<p>© Copyright 2020 Wyss Center for Bio and Neuro Engineering – All rights reserved</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Submodule containing classes and functions relative to **stitching**.

With this submodule the user can stitch a previously parsed dataset, typically the autofluorescence channel:

&gt;&gt;&gt; import pipapr
&gt;&gt;&gt; tiles_autofluo = pipapr.parser.tileParser(path_to_autofluo, frame_size=1024, overlap=25)
&gt;&gt;&gt; stitcher = pipapr.stitcher.tileStitcher(tiles_autofluo)
&gt;&gt;&gt; stitcher.compute_registration_fast()

Others channel can then easily stitched using the previous one as reference:

&gt;&gt;&gt; tiles_signal = pipapr.parser.tileParser(path_to_data, frame_size=1024, overlap=25)
&gt;&gt;&gt; stitcher_channel = pipapr.stitcher.channelStitcher(stitcher, tiles_autofluo, tiles_signal)
&gt;&gt;&gt; stitcher_channel.compute_rigid_registration()

Doing that each tile in the second data set will be registered to the corresponding autofluorescence tile and
then their spatial position will be adjusted.

WARNING: when stitching, the expected overlap must be HIGHER than the real one. To enforce this, a margin of 20% is
automatically taken (this margin can be set lower by the user for speed improvement). In order to get the best stitching
quality it requires to have a good estimate of the overlap, hence why the full volume is not considered.

This submodule also contains a class for merging and reconstructing the data. It was intended to be used at lower
resolution for atlasing. The generated data can quickly become out of hands, use with caution!

By using this code you agree to the terms of the software license agreement.

© Copyright 2020 Wyss Center for Bio and Neuro Engineering – All rights reserved
&#34;&#34;&#34;
import cv2
import numpy as np
from scipy.sparse.csgraph import minimum_spanning_tree, depth_first_order
from scipy.sparse import csr_matrix
import pandas as pd
import cv2 as cv
from skimage.exposure import equalize_adapthist, rescale_intensity
from skimage.transform import warp, AffineTransform, downscale_local_mean
from skimage.metrics import normalized_root_mse, structural_similarity, peak_signal_noise_ratio
from skimage.filters import gaussian
from matplotlib.colors import hsv_to_rgb
import dill
import pipapr
import matplotlib.pyplot as plt
import pyapr
# from skimage.registration import phase_cross_correlation
from scipy.signal import correlate
import os
from pathlib import Path
import warnings
from tqdm import tqdm


def max_sum_over_single_max(reference_image, moving_image, d):
    &#34;&#34;&#34;
    This function is a reliability metric which works well for sparse data. It computes the 99 percentile of the sum
    of reference and shifted image divided by twice the 99 percentile of the reference image.

    Parameters
    ----------
    reference_image: (array) 2D array of the reference image
    moving_image: (array) 2D array of the moving image image
    d_correct: (array) registration parameters

    Returns
    -------

    &#34;&#34;&#34;

    shifted_image = warp(moving_image, AffineTransform(translation=[d[1], d[0]]), mode=&#39;wrap&#39;, preserve_range=True)

    e = (2*np.percentile(reference_image, 99))/np.percentile(reference_image+shifted_image, 99)

    return e


def mse(reference_image, moving_image, d):

    shifted_image = warp(moving_image, AffineTransform(translation=[d[1], d[0]]), mode=&#39;wrap&#39;,
                         preserve_range=True)

    return normalized_root_mse(reference_image, shifted_image, normalization=&#39;mean&#39;)


def reconstruct_middle_frame(tiles,
                             database,
                             downsample,
                             debug=False,
                             z=None):

    if isinstance(database, pipapr.stitcher.tileStitcher):
        database = database.database
    elif isinstance(database, pd.DataFrame):
        database = database
    elif isinstance(database, str):
        database = pd.read_csv(database)
    else:
        raise TypeError(&#39;Error: unknown type for database.&#39;)

    level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

    tile = tiles[0]
    tile.lazy_load_tile(level_delta=level_delta)
    frame_size = tile.lazy_data.shape[1:]
    x_pos = database[&#39;ABS_H&#39;].to_numpy()
    nx = int(np.ceil((x_pos.max() - x_pos.min())/downsample + frame_size[1]))
    y_pos = database[&#39;ABS_V&#39;].to_numpy()
    ny = int(np.ceil((y_pos.max() - y_pos.min())/downsample + frame_size[0]))

    if z is None:
        z = int(tile.lazy_data.shape[0] / 2)

    merged_data = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

    H_pos = database[&#39;ABS_H&#39;].to_numpy()
    H_pos = (H_pos - H_pos.min()) / downsample
    V_pos = database[&#39;ABS_V&#39;].to_numpy()
    V_pos = (V_pos - V_pos.min()) / downsample

    for i, tile in enumerate(tqdm(tiles), desc=&#39;Merging&#39;):
        tile.lazy_load_tile(level_delta=level_delta)
        data = tile.lazy_data[z]

        # In debug mode we highlight each tile edge to see where it was
        if debug:
            data[0, :] = 2 ** 16 - 1
            data[-1, :] = 2 ** 16 - 1
            data[:, 0] = 2 ** 16 - 1
            data[:, -1] = 2 ** 16 - 1

        x1 = int(H_pos[i])
        x2 = int(H_pos[i] + data.shape[1])
        y1 = int(V_pos[i])
        y2 = int(V_pos[i] + data.shape[0])

        merged_data[y1:y2, x1:x2] = np.maximum(merged_data[y1:y2, x1:x2], data)

    return merged_data


def phase_cross_correlation(reference_image,
                            moving_image,
                            upsample_factor=1,
                            return_error=True):
    &#34;&#34;&#34;
    Phase cross correlation. Because skimage function compute the NORMAL cross correlation to estimate the shift I
    modified it to compute the TRUE phase cross correlation, as per the standard definition.

    Parameters
    ----------
    reference_image : array
        Reference image.
    moving_image : array
        Image to register. Must be same dimensionality as
        ``reference_image``.
    upsample_factor : int, optional
        Upsampling factor. Images will be registered to within
        ``1 / upsample_factor`` of a pixel. For example
        ``upsample_factor == 20`` means the images will be registered
        within 1/20th of a pixel. Default is 1 (no upsampling).
        Not used if any of ``reference_mask`` or ``moving_mask`` is not None.
    return_error : bool, optional
        Returns error and phase difference if on, otherwise only
        shifts are returned. Has noeffect if any of ``reference_mask`` or
        ``moving_mask`` is not None. In this case only shifts is returned.

    Returns
    -------
    shifts : ndarray
        Shift vector (in pixels) required to register ``moving_image``
        with ``reference_image``. Axis ordering is consistent with
        numpy (e.g. Z, Y, X)
    error : float
        Translation invariant normalized RMS error between
        ``reference_image`` and ``moving_image``.
    phasediff : float
        Global phase difference between the two images (should be
        zero if images are non-negative).
    &#34;&#34;&#34;

    # images must be the same shape
    if reference_image.shape != moving_image.shape:
        raise ValueError(&#34;images must be same shape&#34;)

    src_freq = np.fft.fftn(reference_image)
    target_freq = np.fft.fftn(moving_image)

    # Whole-pixel shift - Compute cross-correlation by an IFFT
    shape = src_freq.shape
    image_product = src_freq * target_freq.conj()
    eps = np.finfo(image_product.real.dtype).eps
    image_product /= (np.abs(image_product) + eps)
    cross_correlation = np.fft.ifftn(image_product)

    # Locate maximum
    maxima = np.unravel_index(np.argmax(np.abs(cross_correlation)),
                              cross_correlation.shape)
    midpoints = np.array([np.fix(axis_size / 2) for axis_size in shape])

    shifts = np.stack(maxima).astype(np.float64)
    shifts[shifts &gt; midpoints] -= np.array(shape)[shifts &gt; midpoints]

    if upsample_factor == 1:
        if return_error:
            src_amp = np.sum(np.real(src_freq * src_freq.conj()))
            src_amp /= src_freq.size
            target_amp = np.sum(np.real(target_freq * target_freq.conj()))
            target_amp /= target_freq.size
            CCmax = cross_correlation[maxima]
    # If upsampling &gt; 1, then refine estimate with matrix multiply DFT
    else:
        raise ValueError(&#39;Error: upsampled phase cross corrrelation not implemented here, use skimage.&#39;)

    # If its only one row or column the shift along that dimension has no
    # effect. We set to zero.
    for dim in range(src_freq.ndim):
        if shape[dim] == 1:
            shifts[dim] = 0

    if return_error:
        error = np.real(1.0 - CCmax * CCmax.conj())
        phase_diff = np.arctan2(CCmax.imag, CCmax.real)
        return shifts, np.sqrt(np.abs(error)), phase_diff
    else:
        return shifts

def phase_cross_correlation_cv(reference_image, moving_image):
        &#34;&#34;&#34;
        Compute openCV to compute the phase cross correlation. It is around 16 times faster than the implementation using
        numpy FFT (same as skimage).

        Parameters
        ----------
        reference_image : array
            Reference image.
        moving_image : array
            Image to register. Must be same dimensionality as
            ``reference_image``.

        Returns
        -------
        shifts : ndarray
            Shift vector (in pixels) required to register ``moving_image``
            with ``reference_image``. Axis ordering is consistent with
            numpy (e.g. Z, Y, X)
        error : float
            Peak response (see opencv description here:
            https://docs.opencv.org/4.5.3/d7/df3/group__imgproc__motion.html#ga552420a2ace9ef3fb053cd630fdb4952)
        &#34;&#34;&#34;

        d, e = cv.phaseCorrelate(reference_image.astype(np.float32), moving_image.astype(np.float32))

        d_correct = [-np.round(d[1]).astype(np.int), -np.round(d[0]).astype(np.int)]

        return d_correct


class baseStitcher():
    &#34;&#34;&#34;
    Base class for stitching multi-tile data.

    &#34;&#34;&#34;
    def __init__(self,
                 tiles: pipapr.parser.tileParser,
                 overlap_h: (int, float),
                 overlap_v: (int, float)):
        &#34;&#34;&#34;
        Constructor for the baseStitcher class.

        Parameters
        ----------
        tiles: (tileParser) tileParser object containing the dataset to stitch.
        overlap_h: (float) expected horizontal overlap in %
        overlap_v: (float) expected vertical overlap in %

        &#34;&#34;&#34;
        self.tiles = tiles
        self.ncol = tiles.ncol
        self.nrow = tiles.nrow
        self.n_vertex = tiles.n_tiles
        self.n_edges = tiles.n_edges
        self.frame_size = tiles.frame_size

        self.expected_overlap_h = int(overlap_h/100*self.frame_size)
        self.expected_overlap_v = int(overlap_v/100*self.frame_size)

        self.overlap_h = int(self.expected_overlap_h*1.2)
        if self.expected_overlap_h &gt; self.frame_size:
            self.expected_overlap_h = self.frame_size
        self.overlap_v = int(self.expected_overlap_v*1.2)
        if self.expected_overlap_v &gt; self.frame_size:
            self.expected_overlap_v = self.frame_size

        self.mask = False
        self.threshold = None

        self.segment = False
        self.segmenter = None

        self.reg_x = int(self.frame_size*0.05)
        self.reg_y = int(self.frame_size*0.05)
        self.reg_z = 20

        self.z_begin = None
        self.z_end = None

    def activate_mask(self, threshold):
        &#34;&#34;&#34;
        Activate the masked cross-correlation for the displacement estimation. Pixels above threshold are
        not taken into account.

        Parameters
        ----------
        threshold: (int) threshold for the cross-correlation mask as a percentage of pixel to keep (e.g. 95 will
                    create a mask removing the 5% brightest pixels).

        &#34;&#34;&#34;
        self.mask = True
        self.threshold = threshold

    def deactivate_mask(self):
        &#34;&#34;&#34;
        Deactivate the masked cross-correlation and uses a classical cross correlation.

        &#34;&#34;&#34;
        self.mask = False
        self.threshold = None

    def save_database(self, path):
        &#34;&#34;&#34;
        Save database at the given path. The database must be built before calling this method.

        Parameters
        ----------
        path: (str) path to save the database.

        &#34;&#34;&#34;

        if self.database is None:
            raise TypeError(&#39;Error: database can&#39;&#39;t be saved because it was not created. &#39;
                            &#39;Please call build_database() first.&#39;)

        self.database.to_csv(path)

    def activate_segmentation(self,
                              segmenter):
        &#34;&#34;&#34;
        Activate the segmentation. When a tile is loaded it is segmented before the stitching is done.

        Parameters
        ----------
        segmenter: (tileSegmenter) segmenter object for segmenting each tile.

        &#34;&#34;&#34;
        self.segment = True
        self.segmenter = segmenter

    def deactivate_segmentation(self):
        &#34;&#34;&#34;
        Deactivate tile segmentation.

        &#34;&#34;&#34;

        self.segment = False

    def reconstruct_slice(self, loc=None, n_proj=0, dim=0, downsample=1, color=False, debug=False, plot=True):

        if dim == 0:
            return self._reconstruct_z_slice(z=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)
        elif dim == 1:
            return self._reconstruct_y_slice(y=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)
        elif dim == 2:
            return self._reconstruct_x_slice(x=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)

    def set_regularization(self, reg_x, reg_y, reg_z):
        &#34;&#34;&#34;
        Set the regularization for the stitching to prevent aberrant displacements.

        Parameters
        ----------
        reg_x: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                     reg_x (in pixel unit) then the expected displacement (from motor position) is taken.
        reg_y: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                     reg_z (in pixel unit) then the expected displacement (from motor position) is taken.
        reg_z: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                     reg_z (in pixel unit) then the expected displacement (from motor position) is taken.

        Returns
        -------

        &#34;&#34;&#34;

        self.reg_x = reg_x
        self.reg_y = reg_y
        self.reg_z = reg_z

    def reconstruct_z_color(self, z=None, n_proj=0, downsample=1, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        z: (int) reconstruction depth
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at depth z.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)

        if z is None:
            z = int(tile.lazy_data.shape[0] / 2)

        if z &gt; tile.lazy_data.shape[0]:
            raise ValueError(&#39;Error: z is too large ({}), maximum depth at this downsample is {}.&#39;.format(z, tile.lazy_data.shape[0]))

        frame_size = tile.lazy_data.shape[1:]
        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + frame_size[1]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + frame_size[0]))

        H = np.zeros((ny, nx), dtype=&#39;uint16&#39;)
        S = np.ones((ny, nx), dtype=&#39;uint16&#39;) * 0.7
        V = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

        H_pos = (x_pos - x_pos.min()) / downsample
        V_pos = (y_pos - y_pos.min()) / downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            zf = min(z+n_proj, tile.lazy_data.shape[0])
            data = tile.lazy_data[z:zf]
            v = data.max(axis=0)
            h = np.argmax(data, axis=0)

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                v[0, :] = 2**16-1
                v[-1, :] = 2**16-1
                v[:, 0] = 2**16-1
                v[:, -1] = 2**16-1

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + v.shape[1])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + v.shape[0])

            V[y1:y2, x1:x2] = np.maximum(V[y1:y2, x1:x2], v)
            H[y1:y2, x1:x2] = np.maximum(H[y1:y2, x1:x2], h)

        H = rescale_intensity(gaussian(H, sigma=5), out_range=np.float64)*0.66
        V = np.log(V + 200)
        vmin, vmax = np.percentile(V[V &gt; np.log(100)], (1, 99.9))
        V = rescale_intensity(V, in_range=(vmin, vmax), out_range=np.float64)
        S = S * V
        rgb = hsv_to_rgb(np.dstack((H,S,V)))
        rescale_intensity(rgb, out_range=&#39;uint8&#39;)

        if plot:
            plt.figure()
            plt.imshow(rgb)

        return rgb

    def _reconstruct_z_slice(self, z=None, n_proj=0, downsample=1, color=False, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        z: (int) reconstruction depth
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at depth z.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)

        if z is None:
            z = int(tile.lazy_data.shape[0] / 2)

        if z &gt; tile.lazy_data.shape[0]:
            raise ValueError(&#39;Error: z is too large ({}), maximum depth at this downsample is {}.&#39;.format(z, tile.lazy_data.shape[0]))

        frame_size = tile.lazy_data.shape[1:]
        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + frame_size[1]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + frame_size[0]))

        if color:
            merged_data = np.ones((ny, nx, 3), dtype=&#39;uint16&#39;)
            merged_data[:, :, 2] = 0
        else:
            merged_data = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

        H_pos = (x_pos - x_pos.min()) / downsample
        V_pos = (y_pos - y_pos.min()) / downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            zf = min(z+n_proj, tile.lazy_data.shape[0])
            if zf &gt; z:
                data = tile.lazy_data[z:zf].max(axis=0)
            else:
                data = tile.lazy_data[z]

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :] = 2**16-1
                data[-1, :] = 2**16-1
                data[:, 0] = 2**16-1
                data[:, -1] = 2**16-1

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + data.shape[1])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + data.shape[0])

            if color:
                if tile.col % 2:
                    if tile.row % 2:
                        merged_data[y1:y2, x1:x2, 0] = np.maximum(merged_data[y1:y2, x1:x2, 1], data)
                    else:
                        merged_data[y1:y2, x1:x2, 1] = np.maximum(merged_data[y1:y2, x1:x2, 0], data)
                else:
                    if tile.row % 2:
                        merged_data[y1:y2, x1:x2, 1] = np.maximum(merged_data[y1:y2, x1:x2, 1], data)
                    else:
                        merged_data[y1:y2, x1:x2, 0] = np.maximum(merged_data[y1:y2, x1:x2, 0], data)
            else:
                merged_data[y1:y2, x1:x2] = np.maximum(merged_data[y1:y2, x1:x2], data)

        if plot:
            plt.figure()
            if color:
                plt.imshow(self._process_RGB_for_display(merged_data))
            else:
                plt.imshow(np.log(merged_data), cmap=&#39;gray&#39;)

        return merged_data

    def _reconstruct_y_slice(self, y=None, n_proj=0, downsample=1, color=False, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        y: (int) reconstruction location in y
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at position y.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)
        tile_shape = tile.lazy_data.shape

        if y is None:
            y = int(tile_shape[1]*self.tiles.nrow/2)

        if y &gt; tile.lazy_data.shape[1]*self.tiles.nrow:
            raise ValueError(&#39;Error: y is too large ({}), maximum depth at this downsample is {}.&#39;
                             .format(y, tile.lazy_data.shape[1]*self.tiles.nrow))

        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + tile_shape[2]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + tile_shape[1]))
        z_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        nz = int(np.ceil((z_pos.max() - z_pos.min()) / downsample + tile_shape[0]))

        # Determine tiles to load
        tiles_to_load = []
        tiles_pos = []
        for x_loc, y_loc, z_loc, tile in zip(x_pos/downsample, y_pos/downsample, z_pos/downsample, self.tiles):
            if (y &gt; y_loc) and (y &lt; y_loc+tile_shape[1]):
                tiles_to_load.append(tile)
                tiles_pos.append([z_loc, y_loc, x_loc])
        tiles_pos = np.array(tiles_pos).astype(&#39;uint64&#39;)

        if color:
            merged_data = np.ones((nz, nx, 3), dtype=&#39;uint16&#39;)
            merged_data[:, :, 2] = 0
        else:
            merged_data = np.zeros((nz, nx), dtype=&#39;uint16&#39;)

        for i, tile in enumerate(tqdm(tiles_to_load, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            y_tile = int(y - tiles_pos[i, 1])
            yf = min(y_tile+n_proj, tiles_pos[i, 1]+tile.lazy_data.shape[1])
            if yf &gt; y:
                data = tile.lazy_data[:, y_tile:yf, :].max(axis=1)
            else:
                data = tile.lazy_data[:, y_tile, :]

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :] = 2**16-1
                data[-1, :] = 2**16-1
                data[:, 0] = 2**16-1
                data[:, -1] = 2**16-1

            x1 = int(tiles_pos[i, 2])
            x2 = int(tiles_pos[i, 2] + data.shape[1])
            z1 = int(tiles_pos[i, 0])
            z2 = int(tiles_pos[i, 0] + data.shape[0])
            
            if color:
                if tile.col % 2:
                    if tile.row % 2:
                        merged_data[z1:z2, x1:x2, 0] = np.maximum(merged_data[z1:z2, x1:x2, 1], data)
                    else:
                        merged_data[z1:z2, x1:x2, 1] = np.maximum(merged_data[z1:z2, x1:x2, 0], data)
                else:
                    if tile.row % 2:
                        merged_data[z1:z2, x1:x2, 1] = np.maximum(merged_data[z1:z2, x1:x2, 1], data)
                    else:
                        merged_data[z1:z2, x1:x2, 0] = np.maximum(merged_data[z1:z2, x1:x2, 0], data)
            else:
                merged_data[z1:z2, x1:x2] = np.maximum(merged_data[z1:z2, x1:x2], data)

        if plot:
            plt.figure()
            if color:
                plt.imshow(self._process_RGB_for_display(merged_data))
            else:
                plt.imshow(np.log(merged_data), cmap=&#39;gray&#39;)

        return merged_data

    def _reconstruct_x_slice(self, x=None, n_proj=0, downsample=1, color=False, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        x: (int) reconstruction location in x
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at position x.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)
        tile_shape = tile.lazy_data.shape

        if x is None:
            x = int(tile_shape[2]*self.tiles.ncol/2)

        if x &gt; tile.lazy_data.shape[2]*self.tiles.ncol:
            raise ValueError(&#39;Error: y is too large ({}), maximum depth at this downsample is {}.&#39;
                             .format(x, tile.lazy_data.shape[2]*self.tiles.ncol))

        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + tile_shape[2]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + tile_shape[1]))
        z_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        nz = int(np.ceil((z_pos.max() - z_pos.min()) / downsample + tile_shape[0]))

        # Determine tiles to load
        tiles_to_load = []
        tiles_pos = []
        for x_loc, y_loc, z_loc, tile in zip(x_pos/downsample, y_pos/downsample, z_pos/downsample, self.tiles):
            if (x &gt; x_loc) and (x &lt; x_loc+tile_shape[2]):
                tiles_to_load.append(tile)
                tiles_pos.append([z_loc, y_loc, x_loc])
        tiles_pos = np.array(tiles_pos).astype(&#39;uint64&#39;)

        if color:
            merged_data = np.ones((nz, ny, 3), dtype=&#39;uint16&#39;)
            merged_data[:, :, 2] = 0
        else:
            merged_data = np.zeros((nz, ny), dtype=&#39;uint16&#39;)

        for i, tile in enumerate(tqdm(tiles_to_load, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            x_tile = int(x - tiles_pos[i, 2])
            xf = min(x_tile+n_proj, tiles_pos[i, 2]+tile.lazy_data.shape[2])
            if xf &gt; x:
                data = tile.lazy_data[:, :, x_tile:xf].max(axis=2)
            else:
                data = tile.lazy_data[:, :, x_tile]

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :] = 2**16-1
                data[-1, :] = 2**16-1
                data[:, 0] = 2**16-1
                data[:, -1] = 2**16-1

            y1 = int(tiles_pos[i, 1])
            y2 = int(tiles_pos[i, 1] + data.shape[1])
            z1 = int(tiles_pos[i, 0])
            z2 = int(tiles_pos[i, 0] + data.shape[0])
            
            if color:
                if tile.col % 2:
                    if tile.row % 2:
                        merged_data[z1:z2, y1:y2, 0] = np.maximum(merged_data[z1:z2, y1:y2, 1], data)
                    else:
                        merged_data[z1:z2, y1:y2, 1] = np.maximum(merged_data[z1:z2, y1:y2, 0], data)
                else:
                    if tile.row % 2:
                        merged_data[z1:z2, y1:y2, 1] = np.maximum(merged_data[z1:z2, y1:y2, 1], data)
                    else:
                        merged_data[z1:z2, y1:y2, 0] = np.maximum(merged_data[z1:z2, y1:y2, 0], data)
            else:
                merged_data[z1:z2, y1:y2] = np.maximum(merged_data[z1:z2, y1:y2], data)

        if plot:
            plt.figure()
            if color:
                plt.imshow(self._process_RGB_for_display(merged_data))
            else:
                plt.imshow(np.log(merged_data), cmap=&#39;gray&#39;)

        return merged_data

    def _process_RGB_for_display(self, u):
        &#34;&#34;&#34;
        Process RGB data for correctly displaying it.

        Parameters
        ----------
        u: (array) RGB data

        Returns
        -------
        data_to_display: (array) RGB data displayable with correct contrast and colors.
        &#34;&#34;&#34;
        data_to_display = np.zeros_like(u, dtype=&#39;uint8&#39;)
        for i in range(2):
            tmp = np.log(u[:, :, i] + 200)
            vmin, vmax = np.percentile(tmp[tmp &gt; np.log(1 + 200)], (1, 99.9))
            data_to_display[:, :, i] = rescale_intensity(tmp, in_range=(vmin, vmax), out_range=&#39;uint8&#39;)

        return data_to_display

    def check_files_integrity(self):
        cnt = 0
        for tile in self.tiles:
            lazy = 1
            try:
                tile.lazy_load_tile()
            except:
                lazy = 0
                print(&#39;Lazy load failed on ({}, {})\n Trying normal loading...&#39;.format(tile.row, tile.col))

            if lazy == 0:
                try:
                    tile.load_tile()
                except:
                    cnt += 1
                    print(&#39;Problem detected with tile ({}, {})&#39;.format(tile.row, tile.col))

        if cnt == 0:
            print(&#39;All tiles are readable.&#39;)

    def _compute_shift(self, reference_image, moving_image):
        &#34;&#34;&#34;
        Backbone function to compute the registration and the registration error used for the global optimisation.
        This function can be replaced by experienced user to use their own registration and error estimation functions.

        Parameters
        ----------
        reference_image : array
            Reference image.
        moving_image : array
            Image to register. Must be same dimensionality as
            ``reference_image``.

        Returns
        -------
        d: (array) registration parameters found
        e: (float) error estimation for the registration (the higher the error the higher the uncertainty)
        &#34;&#34;&#34;

        d = phase_cross_correlation_cv(reference_image, moving_image)
        e = max_sum_over_single_max(reference_image, moving_image, d)

        return d, e

    def _get_max_proj_apr(self, apr, parts, patch, patch_yx=None, plot=False):
        &#34;&#34;&#34;
        Compute maximum projection on 3D APR data.

        Parameters
        ----------
        apr: (pyapr.APR) apr tree
        parts: (pyapr.ParticlData) apr particle
        patch: (pyapr.patch) patch for computing the projection only on the overlapping area.
        plot: (bool) control data plotting

        Returns
        -------
        (list of np.array) maximum intensity projection in each 3 dimension.

        &#34;&#34;&#34;
        proj = []
        if patch_yx is None:
            for d in range(3):
                # dim=0: project along Y to produce a ZY plane
                # dim=1: project along X to produce a ZX plane
                # dim=2: project along Z to produce an YX plane
                proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                                   dim=d, patch=patch, method=&#39;auto&#39;))
        else:
            proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                               dim=0, patch=patch, method=&#39;auto&#39;))
            proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                               dim=1, patch=patch, method=&#39;auto&#39;))
            proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                               dim=2, patch=patch_yx, method=&#39;auto&#39;))

        if plot:
            fig, ax = plt.subplots(1, 3)
            for i, title in enumerate([&#39;ZY&#39;, &#39;ZX&#39;, &#39;YX&#39;]):
                ax[i].imshow(proj[i], cmap=&#39;gray&#39;)
                ax[i].set_title(title)

        return proj[0], proj[1], proj[2]

    def _get_proj_shifts(self, proj1, proj2):
        &#34;&#34;&#34;
        This function computes shifts from max-projections on overlapping areas. It uses the phase cross-correlation
        to compute the shifts.

        Parameters
        ----------
        proj1: (list of np.array) max-projections for tile 1
        proj2: (list of np.array) max-projections for tile 2
        upsample_factor: (float) upsampling_factor for estimating the maximum phase cross-correlation position

        Returns
        -------
        shifts in (x, y, z) and error measure (0=reliable, 1=not reliable)

        &#34;&#34;&#34;

        # Compute phase cross-correlation to extract shifts
        dzy, error_zy = self._compute_shift(proj1[0], proj2[0])
        dzx, error_zx = self._compute_shift(proj1[1], proj2[1])
        dyx, error_yx = self._compute_shift(proj1[2], proj2[2])

        # Replace error == 0 with 1 otherwise the minimum spanning tree considers that vertex are not connected
        if error_zy == 0:
            error_zy = 1e-6
        if error_zx == 0:
            error_zx = 1e-6
        if error_yx == 0:
            error_yx = 1e-6

        # Keep only the most reliable registration
        # D/z
        if error_zx &lt; error_zy:
            dz = dzx[0]
            rz = error_zx
        else:
            dz = dzy[0]
            rz = error_zy

        # H/x
        if error_zx &lt; error_yx:
            dx = dzx[1]
            rx = error_zx
        else:
            dx = dyx[1]
            rx = error_yx

        # V/y
        if error_yx &lt; error_zy:
            dy = dyx[0]
            ry = error_yx
        else:
            dy = dzy[1]
            ry = error_zy

        # for i, title, vector, err in zip(range(3), [&#39;ZY&#39;, &#39;ZX&#39;, &#39;YX&#39;], [dzy, dzx, dyx], [error_zy, error_zx, error_yx]):
        #     fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
        #     ax[0].imshow(np.log(proj1[i]+1), cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;d={}, e={:0.3f}&#39;.format(vector, err))
        #     ax[1].imshow(np.log(proj2[i]+1), cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)
        #
        #     shifted = warp(proj1[i], AffineTransform(translation=[vector[1], vector[0]]), mode=&#39;wrap&#39;, preserve_range=True)
        #     rgb = np.dstack((np.log(proj2[i]+1), np.log(shifted+1), np.zeros_like(proj1[i])))
        #     ax[2].imshow((rescale_intensity(rgb, out_range=&#39;uint8&#39;)).astype(&#39;uint8&#39;))
        #
        # print(&#39;ok&#39;)

        return np.array([dz, dy, dx]), np.array([rz, ry, rx])

    def _get_masked_proj_shifts(self, proj1, proj2, threshold, upsample_factor=1):
        &#34;&#34;&#34;
        This function computes shifts from max-projections on overlapping areas with mask on brightest area.
        It uses the phase cross-correlation to compute the shifts.

        Parameters
        ----------
        proj1: (list of arrays) max-projections for tile 1
        proj2: (list of arrays) max-projections for tile 2
        upsample_factor: (float) upsampling_factor for estimating the maximum phase cross-correlation position

        Returns
        -------
        shifts in (x, y, z) and error measure (0=reliable, 1=not reliable)

        &#34;&#34;&#34;
        # Compute mask to discard very bright area that are likely bubbles or artefacts
        mask_ref = []
        mask_move = []
        for i in range(3):
            vmax = np.percentile(proj1[i], threshold)
            mask_ref.append(proj1[i] &lt; vmax)
            vmax = np.percentile(proj2[i], threshold)
            mask_move.append(proj2[i] &lt; vmax)

        # Compute phase cross-correlation to extract shifts
        dzy = self.phase_cross_correlation(proj1[0], proj2[0],
                                      return_error=True, upsample_factor=upsample_factor,
                                      reference_mask=mask_ref[0], moving_mask=mask_move[0])
        error_zy = np.sqrt(1 - correlate(proj1[0], proj2[0]).max() ** 2 / (np.sum(proj1 ** 2) * np.sum(proj2 ** 2)))
        dzx = self.phase_cross_correlation(proj1[1], proj2[1],
                                      return_error=True, upsample_factor=upsample_factor,
                                      reference_mask=mask_ref[1], moving_mask=mask_move[1])
        error_zx = np.sqrt(1 - correlate(proj1[1], proj2[1]).max() ** 2 / (np.sum(proj1 ** 2) * np.sum(proj2 ** 2)))
        dyx = self.phase_cross_correlation(proj1[2], proj2[2],
                                      return_error=True, upsample_factor=upsample_factor,
                                      reference_mask=mask_ref[2], moving_mask=mask_move[2])
        error_yx = np.sqrt(1 - correlate(proj1[2], proj2[2]).max() ** 2 / (np.sum(proj1 ** 2) * np.sum(proj2 ** 2)))

        # Replace error == 0 with 1e-6 otherwise the minimum spanning tree considers that vertex are not connected
        if error_zy == 0:
            error_zy = 1e-6
        if error_zx == 0:
            error_zx = 1e-6
        if error_yx == 0:
            error_yx = 1e-6

        # Keep only the most reliable registration
        # D/z
        if error_zx &lt; error_zy:
            dz = dzx[0]
            rz = error_zx
        else:
            dz = dzy[0]
            rz = error_zy

        # H/x
        if error_zx &lt; error_yx:
            dx = dzx[1]
            rx = error_zx
        else:
            dx = dyx[1]
            rx = error_yx

        # V/y
        if error_yx &lt; error_zy:
            dy = dyx[0]
            ry = error_yx
        else:
            dy = dzy[1]
            ry = error_zy

        # for i, title, vector in zip(range(3), [&#39;ZY&#39;, &#39;ZX&#39;, &#39;YX&#39;], [[dy, dz], [dx, dz], [dx, dy]]):
        #     fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
        #     ax[0].imshow(proj1[i], cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;dx={}, dy={}, dz={}&#39;.format(dx, dy, dz))
        #     ax[1].imshow(proj2[i], cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)
        #     from skimage.transform import warp, AffineTransform
        #     from skimage.exposure import rescale_intensity
        #     shifted = warp(proj1[i], AffineTransform(translation=vector), mode=&#39;wrap&#39;, preserve_range=True)
        #     rgb = np.dstack([proj2[i], shifted, np.zeros_like(proj1[i])])
        #     ax[2].imshow((rescale_intensity(rgb, out_range=&#39;uint8&#39;)).astype(&#39;uint8&#39;))
        # print(&#39;ok&#39;)

        return np.array([dz, dy, dx]), np.array([rz, ry, rx])

    def _regularize(self, reg, rel):
        &#34;&#34;&#34;
        Remove too large displacement and replace them with expected one with a large uncertainty.

        &#34;&#34;&#34;
        if np.abs(reg[2] - (self.overlap_h - self.expected_overlap_h)) &gt; self.reg_x:
            reg[2] = (self.overlap_h - self.expected_overlap_h)
            rel[2] = 2
        if np.abs(reg[1] - (self.overlap_v - self.expected_overlap_v)) &gt; self.reg_y:
            reg[1] = (self.overlap_v - self.expected_overlap_v)
            rel[1] = 2
        if np.abs(reg[0]) &gt; self.reg_z:
            reg[0] = 0
            rel[0] = 2

        return reg, rel


class tileStitcher(baseStitcher):
    &#34;&#34;&#34;
    Class used to perform the stitching. The stitching is performed in 4 steps:

    1. The pairwise registration parameters of each neighboring tile is computed on the max-projection
    2. A sparse graph (edges = tiles and vertex = registration between neighboring tiles) is constructed to store
       the registration parameters (displacements and reliability)
    3. The sparse graph is optimized to satisfy the constraints (every loop in the graph should sum to 0) using the
       maximum spanning tree on the reliability estimation.
    4. The maximum spanning tree is parsed to extract optimal tile positions solution.

    The beauty of this method is that it scales well with increasing dataset sizes and because the final optimization
    is very fast and does not require to reload the data.

    &#34;&#34;&#34;
    def __init__(self,
                 tiles: pipapr.parser.tileParser,
                 overlap_h: (int, float),
                 overlap_v: (int, float)):
        &#34;&#34;&#34;
        Constructor for the tileStitcher class.

        Parameters
        ----------
        tiles: (tileParser) tileParser object containing the dataset to stitch.
        overlap_h: (float) expected horizontal overlap in %
        overlap_v: (float) expected vertical overlap in %
        &#34;&#34;&#34;

        super().__init__(tiles, overlap_h, overlap_v)

        self.cgraph_from = []
        self.cgraph_to = []
        self.relia_H = []
        self.relia_V = []
        self.relia_D = []
        self.dH = []
        self.dV = []
        self.dD = []

        # Attributes below are set when the corresponding method are called.
        self.registration_map_rel = None
        self.registration_map_abs = None
        self.ctree_from_H = None
        self.ctree_from_V = None
        self.ctree_from_D = None
        self.ctree_to_H = None
        self.ctree_to_V = None
        self.ctree_to_D = None
        self.min_tree_H = None
        self.min_tree_V = None
        self.min_tree_D = None
        self.graph_relia_H = None
        self.graph_relia_V = None
        self.graph_relia_D = None
        self.database = None

    def compute_registration(self):
        &#34;&#34;&#34;
        Compute the pair-wise registration for all tiles. This implementation loads the data twice and is therefore
        not efficient.

        &#34;&#34;&#34;
        for tile in tqdm(self.tiles, desc=&#39;Computing stitching&#39;):
            tile.load_tile()
            tile.load_neighbors()

            if self.segment:
                self.segmenter.compute_segmentation(tile)

            for apr, parts, coords in zip(tile.apr_neighbors, tile.parts_neighbors, tile.neighbors):

                if tile.row == coords[0] and tile.col &lt; coords[1]:
                    # EAST
                    reg, rel = self._compute_east_registration(tile.apr, tile.parts, apr, parts)

                elif tile.col == coords[1] and tile.row &lt; coords[0]:
                    # SOUTH
                    reg, rel = self._compute_south_registration(tile.apr, tile.parts, apr, parts)

                else:
                    raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

                # Regularize in cas of aberrant displacements
                reg, rel = self._regularize(reg, rel)

                self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                    dims=(self.nrow, self.ncol)))
                self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                                  dims=(self.nrow, self.ncol)))
                # H=x, V=y, D=z
                self.dH.append(reg[2])
                self.dV.append(reg[1])
                self.dD.append(reg[0])
                self.relia_H.append(rel[2])
                self.relia_V.append(rel[1])
                self.relia_D.append(rel[0])

        self._build_sparse_graphs()
        self._optimize_sparse_graphs()
        _, _ = self._produce_registration_map()
        self._build_database()
        self._print_info()

    def compute_registration_fast(self, on_disk=False):
        &#34;&#34;&#34;
        Compute the pair-wise registration for all tiles. This implementation loads the data once by precomputing
        the max-proj and is therefore efficient.

        &#34;&#34;&#34;
        # First we pre-compute the max-projections and keep them in memory or save them on disk and load them up.
        if on_disk:
            self._save_max_projs()
            projs = self._load_max_projs()
        else:
            projs = self._precompute_max_projs()

        # Then we loop again through the tiles but now we have access to the max-proj
        for tile in tqdm(self.tiles, desc=&#39;Computing cross-correlations&#39;):
            proj1 = projs[tile.row, tile.col]

            for coords in tile.neighbors:
                proj2 = projs[coords[0], coords[1]]

                if tile.row == coords[0] and tile.col &lt; coords[1]:
                    # EAST
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;])

                elif tile.col == coords[1] and tile.row &lt; coords[0]:
                    # SOUTH
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;])

                else:
                    raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

                self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                    dims=(self.nrow, self.ncol)))
                self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                                  dims=(self.nrow, self.ncol)))

                # Regularize in case of aberrant displacements
                reg, rel = self._regularize(reg, rel)

                # H=x, V=y, D=z
                self.dH.append(reg[2])
                self.dV.append(reg[1])
                self.dD.append(reg[0])
                self.relia_H.append(rel[2])
                self.relia_V.append(rel[1])
                self.relia_D.append(rel[0])

        self._build_sparse_graphs()
        self._optimize_sparse_graphs()
        _, _ = self._produce_registration_map()
        self._build_database()
        self._print_info()

    def compute_registration_from_max_projs(self):
        &#39;&#39;&#39;
        Compute the registration directly from the max-projections. Max-projections must have been computed before.

        &#39;&#39;&#39;

        # First we pre-compute the max-projections and keep them in memory or save them on disk and load them up.
        projs = self._load_max_projs()

        # Then we loop again through the tiles but now we have access to the max-proj
        for tile in tqdm(self.tiles, desc=&#39;Compute cross-correlation&#39;):
            proj1 = projs[tile.row, tile.col]

            for coords in tile.neighbors:
                proj2 = projs[coords[0], coords[1]]

                if tile.row == coords[0] and tile.col &lt; coords[1]:
                    # EAST
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;])

                elif tile.col == coords[1] and tile.row &lt; coords[0]:
                    # SOUTH
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;])

                else:
                    raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

                self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                    dims=(self.nrow, self.ncol)))
                self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                                  dims=(self.nrow, self.ncol)))

                # Regularize in cas of aberrant displacements
                reg, rel = self._regularize(reg, rel)

                # H=x, V=y, D=z
                self.dH.append(reg[2])
                self.dV.append(reg[1])
                self.dD.append(reg[0])
                self.relia_H.append(rel[2])
                self.relia_V.append(rel[1])
                self.relia_D.append(rel[0])

        self._build_sparse_graphs()
        self._optimize_sparse_graphs()
        _, _ = self._produce_registration_map()
        self._build_database()
        self._print_info()

    def compute_expected_registration(self):
        &#34;&#34;&#34;
        Compute the expected registration if the expected overlap are correct.

        &#34;&#34;&#34;

        reg_rel_map = np.zeros((3, self.nrow, self.ncol))

        self.registration_map_rel = reg_rel_map

        reg_abs_map = np.zeros_like(reg_rel_map)
        # H
        for x in range(reg_abs_map.shape[2]):
            reg_abs_map[0, :, x] = reg_rel_map[0, :, x] + x * (self.frame_size - self.expected_overlap_h)
        # V
        for x in range(reg_abs_map.shape[1]):
            reg_abs_map[1, x, :] = reg_rel_map[1, x, :] + x * (self.frame_size - self.expected_overlap_v)
        # D
        reg_abs_map[2] = reg_rel_map[2]
        self.registration_map_abs = reg_abs_map

        self._build_database()

    def plot_graph(self, annotate=False):
        &#34;&#34;&#34;
        Plot the graph for each direction (H, D, V). This method needs to be called after the graph
        optimization.

        Parameters
        ----------
        annotate: (bool) control if annotation are drawn on the graph

        &#34;&#34;&#34;

        if self.graph_relia_H is None:
            raise TypeError(&#39;Error: graph not build yet, please use build_sparse_graph()&#39;
                            &#39;before trying to plot the graph.&#39;)

        fig, ax = plt.subplots(1, 3)
        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ind_from = getattr(self, &#39;cgraph_from&#39;)
            row, col = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
            V1 = np.vstack((row, col)).T

            ind_to = getattr(self, &#39;cgraph_to&#39;)
            row, col = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
            V2 = np.vstack((row, col)).T

            rel = getattr(self, &#39;relia_&#39; + d)
            dX = getattr(self, &#39;d&#39; + d)
            for ii in range(V1.shape[0]):
                ax[i].plot([V1[ii, 1], V2[ii, 1]], [V1[ii, 0], V2[ii, 0]], &#39;ko&#39;, markerfacecolor=&#39;r&#39;)
                if annotate:
                    p1 = ax[i].transData.transform_point([V1[ii, 1], V1[ii, 0]])
                    p2 = ax[i].transData.transform_point([V2[ii, 1], V2[ii, 0]])
                    dy = p2[1]-p1[1]
                    dx = p2[0]-p1[0]
                    rot = np.degrees(np.arctan2(dy, dx))
                    if rel[ii] &lt; 0.15:
                        color = &#39;g&#39;
                    elif rel[ii] &lt; 0.30:
                        color = &#39;orange&#39;
                    else:
                        color = &#39;r&#39;
                    ax[i].annotate(text=&#39;err={:.2f} d{}={:.2f}&#39;.format(rel[ii], d, dX[ii]),
                                   xy=((V1[ii, 1]+V2[ii, 1])/2, (V1[ii, 0]+V2[ii, 0])/2),
                                   ha=&#39;center&#39;,
                                   va=&#39;center&#39;,
                                   fontsize=8,
                                   rotation=rot,
                                   backgroundcolor=&#39;w&#39;,
                                   color=color)
            ax[i].set_title(d + &#39; tree&#39;)
            ax[i].invert_yaxis()

        return fig, ax

    def plot_min_trees(self, annotate=False):
        &#34;&#34;&#34;
        Plot the minimum spanning tree for each direction (H, D, V). This method needs to be called after the graph
        optimization.

        Parameters
        ----------
        annotate: (bool) control if annotation are drawn on the graph

        &#34;&#34;&#34;

        if self.min_tree_H is None:
            raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                            &#39;before trying to plot the trees.&#39;)

        fig, ax = self.plot_graph(annotate=annotate)

        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ind_from = getattr(self, &#39;ctree_from_&#39; + d)
            row, col = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
            V1 = np.vstack((row, col)).T

            ind_to = getattr(self, &#39;ctree_to_&#39; + d)
            row, col = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
            V2 = np.vstack((row, col)).T

            rel = getattr(self, &#39;relia_&#39; + d)
            dX = getattr(self, &#39;d&#39; + d)
            for ii in range(V1.shape[0]):
                ax[i].plot([V1[ii, 1], V2[ii, 1]], [V1[ii, 0], V2[ii, 0]], &#39;ko-&#39;, markerfacecolor=&#39;r&#39;, linewidth=2)
            ax[i].set_title(d + &#39; tree&#39;)

    def plot_stitching_info(self):


        if self.min_tree_H is None:
            raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                            &#39;before trying to plot stitching info.&#39;)

        rel_map = np.zeros((3, self.nrow, self.ncol))
        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ind_from = getattr(self, &#39;ctree_from_&#39; + d)
            ind_to = getattr(self, &#39;ctree_to_&#39; + d)
            graph = getattr(self, &#39;graph_relia_&#39; + d)
            rows, cols = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
            for row, col, i1, i2 in zip(rows, cols, ind_from, ind_to):
                rel = graph[i1, i2]
                rel_map[i, row, col] = np.max((rel_map[i, row, col], rel))
            rows, cols = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
            for row, col, i1, i2 in zip(rows, cols, ind_from, ind_to):
                rel = graph[i1, i2]
                rel_map[i, row, col] = np.max((rel_map[i, row, col], rel))

        fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
        for i in range(3):
            ax[i].imshow(rel_map[i], cmap=&#39;turbo&#39;, vmin=0, vmax=2)

        plt.figure()
        plt.imshow(np.mean(rel_map, axis=0), cmap=&#39;turbo&#39;)
        plt.colorbar()

        return rel_map

    def plot_registration_map(self):
        &#34;&#34;&#34;
        Display the registration map using matplotlib.

        &#34;&#34;&#34;

        if self.registration_map_abs is None:
            raise TypeError(&#39;Error: registration map not computed yet, please use produce_registration_map()&#39;
                            &#39;before trying to display the registration map.&#39;)

        fig, ax = plt.subplots(2, 3)
        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ax[0, i].imshow(self.registration_map_rel[i], cmap=&#39;gray&#39;)
            ax[0, i].set_title(&#39;Rel reg. map &#39; + d)
            ax[1, i].imshow(self.registration_map_abs[i], cmap=&#39;gray&#39;)
            ax[1, i].set_title(&#39;Abs reg. map &#39; + d)

    def dump_stitcher(self, path):
        &#34;&#34;&#34;
        Use dill to store a tgraph object.

        Parameters
        ----------
        path: (str) path to save the database.

        &#34;&#34;&#34;
        if path[-4:] != &#39;.pkl&#39;:
            path = path + &#39;.pkl&#39;

        with open(path, &#39;wb&#39;) as f:
            dill.dump(self, f)

    def set_overlap_margin(self, margin):
        &#34;&#34;&#34;
        Modify the overlaping area size. If the overlaping area is smaller than the true one, the stitching can&#39;t
        be performed properly. If the overlaping area area is more than twice the size of the true one it will also
        fail (due to the circular FFT in the phase cross correlation).

        Parameters
        ----------
        margin: (float) safety margin in % to take the overlaping area.

        Returns
        -------
        None
        &#34;&#34;&#34;
        if margin &gt; 45:
            raise ValueError(&#39;Error: overlap margin is too big and will make the stitching fail.&#39;)
        if margin &lt; 1:
            raise ValueError(&#39;Error: overlap margin is too small and may make the stitching fail.&#39;)

        self.overlap_h = int(self.expected_overlap_h*(1+margin/100))
        if self.expected_overlap_h &gt; self.frame_size:
            self.expected_overlap_h = self.frame_size
        self.overlap_v = int(self.expected_overlap_v*(1+margin/100))
        if self.expected_overlap_v &gt; self.frame_size:
            self.expected_overlap_v = self.frame_size

    def set_z_range(self, z_begin, z_end):
        &#34;&#34;&#34;
        Set a range of depth fo computing the stitching.


        Parameters
        ----------
        z_begin: (int) first depth to be included in the max-proj
        z_end: (int) last depth to be included in the max-proj

        Returns
        -------
        None
        &#34;&#34;&#34;

        self.z_begin = z_begin
        self.z_end = z_end

    def _print_info(self):
        &#34;&#34;&#34;
        Display stitching result information.

        &#34;&#34;&#34;
        overlap = np.median(np.diff(np.median(self.registration_map_abs[0], axis=0)))
        self.effective_overlap_h = (self.frame_size-overlap)/self.frame_size*100
        print(&#39;Effective horizontal overlap: {:0.2f}%&#39;.format(self.effective_overlap_h))
        overlap = np.median(np.diff(np.median(self.registration_map_abs[1], axis=1)))
        self.effective_overlap_v = (self.frame_size-overlap)/self.frame_size*100
        print(&#39;Effective vertical overlap: {:0.2f}%&#39;.format(self.effective_overlap_v))

        if np.abs(self.effective_overlap_v*self.frame_size/100-self.expected_overlap_v)&gt;0.2*self.expected_overlap_v:
            warnings.warn(&#39;Expected vertical overlap is very different from the computed one, the registration &#39;
                          &#39;might be wrong.&#39;)
        if np.abs(self.effective_overlap_h*self.frame_size/100-self.expected_overlap_h)&gt;0.2*self.expected_overlap_h:
            warnings.warn(&#39;Expected horizontal overlap is very different from the computed one, the registration &#39;
                          &#39;might be wrong.&#39;)

    def _save_max_projs(self):

        # Safely create folder to save max-projs
        Path(self.tiles.folder_max_projs).mkdir(parents=True, exist_ok=True)

        for tile in tqdm(self.tiles):
            tile.load_tile()
            proj = {}
            if tile.col + 1 &lt; self.tiles.ncol:
                if self.tiles.tiles_pattern[tile.row, tile.col + 1] == 1:
                    # EAST 1
                    patch = pyapr.ReconPatch()
                    patch.y_begin = self.frame_size - self.overlap_h
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_east_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])
            if tile.col - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row, tile.col - 1] == 1:
                    # EAST 2
                    patch = pyapr.ReconPatch()
                    patch.y_end = self.overlap_h
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_west_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])
            if tile.row + 1 &lt; self.tiles.nrow:
                if self.tiles.tiles_pattern[tile.row + 1, tile.col] == 1:
                    # SOUTH 1
                    patch = pyapr.ReconPatch()
                    patch.x_begin = self.frame_size - self.overlap_v
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_south_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])
            if tile.row - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row - 1, tile.col] == 1:
                    # SOUTH 2
                    patch = pyapr.ReconPatch()
                    patch.x_end = self.overlap_v
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_north_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])

    def _load_max_projs(self):

        projs = np.empty((self.nrow, self.ncol), dtype=object)

        for tile in self.tiles:
            proj = {}
            if tile.col + 1 &lt; self.tiles.ncol:
                if self.tiles.tiles_pattern[tile.row, tile.col + 1] == 1:
                    # EAST 1
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_east_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;east&#39;] = tmp
            if tile.col - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row, tile.col - 1] == 1:
                    # EAST 2
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_west_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;west&#39;] = tmp
            if tile.row + 1 &lt; self.tiles.nrow:
                if self.tiles.tiles_pattern[tile.row + 1, tile.col] == 1:
                    # SOUTH 1
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_south_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;south&#39;] = tmp
            if tile.row - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row - 1, tile.col] == 1:
                    # SOUTH 2
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_north_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;north&#39;] = tmp

            projs[tile.row, tile.col] = proj

        return projs

    def _precompute_max_projs(self):
        &#34;&#34;&#34;
        Precompute max-projections for loading the data only once during the stitching.

        &#34;&#34;&#34;

        projs = np.empty((self.nrow, self.ncol), dtype=object)
        for tile in tqdm(self.tiles, desc=&#39;Computing max. proj.&#39;):
            tile.load_tile()
            proj = {}
            if tile.col+1 &lt; self.tiles.ncol:
                if self.tiles.tiles_pattern[tile.row, tile.col+1] == 1:
                    # EAST 1
                    patch = pyapr.ReconPatch()
                    patch.y_begin = self.frame_size - self.overlap_h
                    if self.z_begin is None:
                        proj[&#39;east&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.y_begin = self.frame_size - self.overlap_h
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;east&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)
            if tile.col-1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row, tile.col-1] == 1:
                    # EAST 2
                    patch = pyapr.ReconPatch()
                    patch.y_end = self.overlap_h
                    if self.z_begin is None:
                        proj[&#39;west&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.y_end = self.overlap_h
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;west&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)
            if tile.row+1 &lt; self.tiles.nrow:
                if self.tiles.tiles_pattern[tile.row+1, tile.col] == 1:
                    # SOUTH 1
                    patch = pyapr.ReconPatch()
                    patch.x_begin = self.frame_size - self.overlap_v
                    if self.z_begin is None:
                        proj[&#39;south&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.x_begin = self.frame_size - self.overlap_v
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;south&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)
            if tile.row-1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row-1, tile.col] == 1:
                    # SOUTH 2
                    patch = pyapr.ReconPatch()
                    patch.x_end = self.overlap_v
                    if self.z_begin is None:
                        proj[&#39;north&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.x_end = self.overlap_v
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;north&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)

            projs[tile.row, tile.col] = proj

            if self.segment:
                self.segmenter.compute_segmentation(tile)

        return projs

    def _build_sparse_graphs(self):
        &#34;&#34;&#34;
        Build the sparse graph from the reliability and (row, col). This method needs to be called after the
        pair-wise registration has been performed for all neighbors pair.

        &#34;&#34;&#34;

        csr_matrix_size = self.ncol*self.nrow
        self.graph_relia_H = csr_matrix((self.relia_H, (self.cgraph_from, self.cgraph_to)),
                                        shape=(csr_matrix_size, csr_matrix_size))
        self.graph_relia_V = csr_matrix((self.relia_V, (self.cgraph_from, self.cgraph_to)),
                                        shape=(csr_matrix_size, csr_matrix_size))
        self.graph_relia_D = csr_matrix((self.relia_D, (self.cgraph_from, self.cgraph_to)),
                                        shape=(csr_matrix_size, csr_matrix_size))

    def _optimize_sparse_graphs(self):
        &#34;&#34;&#34;
        Optimize the sparse graph by computing the minimum spanning tree for each direction (H, D, V). This
        method needs to be called after the sparse graphs have been built.

        &#34;&#34;&#34;

        if self.graph_relia_H is None:
            raise TypeError(&#39;Error: sparse graph not build yet, please use build_sparse_graph() before trying to&#39;
                            &#39;perform the optimization.&#39;)

        for g in [&#39;graph_relia_H&#39;, &#39;graph_relia_V&#39;, &#39;graph_relia_D&#39;]:
            graph = getattr(self, g)
            # Minimum spanning tree
            min_tree = minimum_spanning_tree(graph)

            # Get the &#34;true&#34; neighbors
            min_tree = min_tree.tocoo()
            setattr(self, &#39;min_tree_&#39; + g[-1], min_tree)
            ctree_from = min_tree.row
            setattr(self, &#39;ctree_from_&#39; + g[-1], ctree_from)

            ctree_to = min_tree.col
            setattr(self, &#39;ctree_to_&#39; + g[-1], ctree_to)

    def _produce_registration_map(self):
        &#34;&#34;&#34;
        Produce the registration map where reg_rel_map[d, row, col] (d = H,V,D) is the relative tile
        position in pixel from the expected one. This method needs to be called after the optimization has been done.

        &#34;&#34;&#34;

        if self.min_tree_H is None:
            raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                            &#39;before trying to compute the registration map.&#39;)

        # Relative registration
        # Initialize relative registration map
        reg_rel_map = np.zeros((3, self.nrow, self.ncol)) # H, V, D

        for i, min_tree in enumerate([&#39;min_tree_H&#39;, &#39;min_tree_V&#39;, &#39;min_tree_D&#39;]):
            # Fill it by following the tree and getting the corresponding registration parameters
            node_array = depth_first_order(getattr(self, min_tree), i_start=self.cgraph_from[0],
                                           directed=False, return_predecessors=False)

            node_visited = [node_array[0]]

            tree = getattr(self, min_tree)
            row = tree.row
            col = tree.col

            for node_to in zip(node_array[1:]):
                # The previous node in the MST is a visited node with an edge to the current node
                neighbors = []
                for r, c in zip(row, col):
                    if r == node_to:
                        neighbors.append(c)
                    if c == node_to:
                        neighbors.append(r)
                node_from = [x for x in neighbors if x in node_visited]
                node_visited.append(node_to)

                # Get the previous neighbor local reg parameter
                ind1, ind2 = np.unravel_index(node_from, shape=(self.nrow, self.ncol))
                d_neighbor = reg_rel_map[i, ind1, ind2]

                # Get the current 2D tile position
                ind1, ind2 = np.unravel_index(node_to, shape=(self.nrow, self.ncol))
                # Get the associated ind position in the registration graph (as opposed to the reliability min_tree)
                ind_graph = self._get_ind(node_from, node_to)
                # Get the corresponding reg parameter
                d = getattr(self, &#39;d&#39; + min_tree[-1])[ind_graph]
                # Get the corresponding relia and print a warning if it was regularized:
                relia = getattr(self, &#39;relia_&#39; + min_tree[-1])[ind_graph]
                if relia == 2:
                    print(&#39;Aberrant pair-wise registration remaining after global optimization between tile ({},{}) &#39;
                          &#39;and tile ({},{})&#39;.format(*np.unravel_index(node_from, shape=(self.nrow, self.ncol)),
                                                    *np.unravel_index(node_to, shape=(self.nrow, self.ncol))))
                # Update the local reg parameter in the 2D matrix
                if node_to &gt; node_from[0]:
                    reg_rel_map[i, ind1, ind2] = d_neighbor + d
                else:
                    reg_rel_map[i, ind1, ind2] = d_neighbor - d
        self.registration_map_rel = reg_rel_map

        reg_abs_map = np.zeros_like(reg_rel_map)
        # H
        for x in range(reg_abs_map.shape[2]):
            reg_abs_map[0, :, x] = reg_rel_map[0, :, x] + x * (self.frame_size-self.overlap_h)
        # V
        for x in range(reg_abs_map.shape[1]):
            reg_abs_map[1, x, :] = reg_rel_map[1, x, :] + x * (self.frame_size-self.overlap_v)
        # D
        reg_abs_map[2] = reg_rel_map[2]
        self.registration_map_abs = reg_abs_map

        return reg_rel_map, reg_abs_map

    def _build_database(self):
        &#34;&#34;&#34;
        Build the database for storing the registration parameters. This method needs to be called after
        the registration map has been produced.

        &#34;&#34;&#34;

        if self.registration_map_rel is None:
            raise TypeError(&#39;Error: database can&#39;&#39;t be build if the registration map has not been computed.&#39;
                            &#39; Please use produce_registration_map() method first.&#39;)

        database_dict = {}
        for i in range(self.n_vertex):
            row = self.tiles[i].row
            col = self.tiles[i].col
            database_dict[i] = {&#39;path&#39;: self.tiles[i].path,
                                &#39;row&#39;: row,
                                &#39;col&#39;: col,
                                &#39;dH&#39;: self.registration_map_rel[0, row, col],
                                &#39;dV&#39;: self.registration_map_rel[1, row, col],
                                &#39;dD&#39;: self.registration_map_rel[2, row, col],
                                &#39;ABS_H&#39;: self.registration_map_abs[0, row, col],
                                &#39;ABS_V&#39;: self.registration_map_abs[1, row, col],
                                &#39;ABS_D&#39;: self.registration_map_abs[2, row, col]}

        self.database = pd.DataFrame.from_dict(database_dict, orient=&#39;index&#39;)

        # Finally set the origin so that tile on the edge have coordinate 0 (rather than negative):
        for i, d in enumerate([&#39;ABS_D&#39;, &#39;ABS_V&#39;, &#39;ABS_H&#39;]):
            self.database[d] = self.database[d] - self.database[d].min()

    def _get_ind(self, ind_from, ind_to):
        &#34;&#34;&#34;
        Returns the ind in the original graph which corresponds to (ind_from, ind_to) in the minimum spanning tree.

        Parameters
        ----------
        ind_from: (int) starting node in the directed graph
        ind_to: (int) ending node in the directed graph

        Returns
        ----------
        (int) corresponding ind in the original graph

        &#34;&#34;&#34;

        ind = None
        for i, f in enumerate(self.cgraph_from):
            if f == ind_from:
                if self.cgraph_to[i] == ind_to:
                    ind = i
        if ind is None:
            for i, f in enumerate(self.cgraph_to):
                if f == ind_from:
                    if self.cgraph_from[i] == ind_to:
                        ind = i
        if ind is None:
            raise ValueError(&#39;Error: can&#39;&#39;t find matching vertex pair.&#39;)
        return ind

    def _compute_east_registration(self, apr_1, parts_1, apr_2, parts_2):
        &#34;&#34;&#34;
        Compute the registration between the current tile and its eastern neighbor.

        Parameters
        ----------
        u: (list) current tile
        v: (list) neighboring tile

        Returns
        -------
        None

        &#34;&#34;&#34;

        patch = pyapr.ReconPatch()
        patch.y_begin = self.frame_size - self.overlap_h
        proj_zy1, proj_zx1, proj_yx1 = self._get_max_proj_apr(apr_1, parts_1, patch, plot=False)

        patch = pyapr.ReconPatch()
        patch.y_end = self.overlap_h
        proj_zy2, proj_zx2, proj_yx2 = self._get_max_proj_apr(apr_2, parts_2, patch, plot=False)

        # proj1, proj2 = [proj_zy1, proj_zx1, proj_yx1], [proj_zy2, proj_zx2, proj_yx2]
        # for i, title in enumerate([&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]):
        #     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
        #     ax[0].imshow(proj1[i], cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;EAST&#39;)
        #     ax[1].imshow(proj2[i], cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)

        if self.mask:
            return self._get_masked_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                           [proj_zy2, proj_zx2, proj_yx2],
                                           threshold=self.threshold)
        else:
            return self._get_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                    [proj_zy2, proj_zx2, proj_yx2])

    def _compute_south_registration(self, apr_1, parts_1, apr_2, parts_2):
        &#34;&#34;&#34;
        Compute the registration between the current tile and its southern neighbor.

        Parameters
        ----------
        u: (list) current tile
        v: (list) neighboring tile

        Returns
        -------
        None

        &#34;&#34;&#34;

        patch = pyapr.ReconPatch()
        patch.x_begin = self.frame_size - self.overlap_v
        proj_zy1, proj_zx1, proj_yx1 = self._get_max_proj_apr(apr_1, parts_1, patch, plot=False)

        patch = pyapr.ReconPatch()
        patch.x_end = self.overlap_v
        proj_zy2, proj_zx2, proj_yx2 = self._get_max_proj_apr(apr_2, parts_2, patch, plot=False)

        # proj1, proj2 = [proj_zy1, proj_zx1, proj_yx1], [proj_zy2, proj_zx2, proj_yx2]
        # for i, title in enumerate([&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]):
        #     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
        #     ax[0].imshow(proj1[i], cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;SOUTH&#39;)
        #     ax[1].imshow(proj2[i], cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)

        if self.mask:
            return self._get_masked_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                           [proj_zy2, proj_zx2, proj_yx2],
                                           threshold=self.threshold)
        else:
            return self._get_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                    [proj_zy2, proj_zx2, proj_yx2])


class channelStitcher(baseStitcher):
    &#34;&#34;&#34;
    Class used to perform the stitching between different channels. The registration must be performed first a single
    channel (typically auto-fluorescence)

    The stitching is performed between each corresponding tile and the relative displacement is added to the previously
    determined stitching parameters.

    The number and position of tile should matched for bot dataset.
    &#34;&#34;&#34;

    def __init__(self,
                 stitcher: tileStitcher,
                 ref: pipapr.parser.tileParser,
                 moving: pipapr.parser.tileParser):
        &#34;&#34;&#34;
        Constructor for the channelStitcher class.

        Parameters
        ----------
        stitcher: (tileStitcher) tileStitcher object with the multitile registration parameters
        tiles_stitched: (tileParser) tiles corresponding to the stitcher
        tiles_channel: (tileParser) tiles to be registered to tiles_stitched
        &#34;&#34;&#34;

        super().__init__(moving,
                         stitcher.overlap_h,
                         stitcher.overlap_v)

        self.stitcher = stitcher
        self.tiles_ref = ref
        self.database = stitcher.database.copy()
        # Change tiles path for the channel tiles
        self.database.path = self.tiles.path_list

        self.segment = False
        self.segmentation_verbose = None

    def compute_rigid_registration(self):

        for tile1, tile2 in zip(tqdm(self.tiles_ref), self.tiles):

            tile1.load_tile()
            tile2.load_tile()

            if self.segment:
                self.segmenter.compute_segmentation(tile2)

            patch = pyapr.ReconPatch()
            proj1 = self._get_max_proj_apr(tile1.apr, tile1.parts, patch)

            patch = pyapr.ReconPatch()
            proj2 = self._get_max_proj_apr(tile2.apr, tile2.parts, patch)

            if self.mask:
                reg, rel = self._get_masked_proj_shifts(proj1, proj2, self.threshold)
            else:
                reg, rel = self._get_proj_shifts(proj1, proj2)

            reg, rel = self._regularize(reg, rel)



            self._update_database(tile2.row, tile2.col, reg)

    def _update_database(self, row, col, d):
        d = np.concatenate([d, d])
        df = self.database
        for loc, value in zip([&#39;dD&#39;, &#39;dV&#39;, &#39;dH&#39;, &#39;ABS_D&#39;, &#39;ABS_V&#39;, &#39;ABS_H&#39;], d):
            df.loc[(df[&#39;row&#39;] == row) &amp; (df[&#39;col&#39;] == col), loc] += value


class tileMerger():
    &#34;&#34;&#34;
    Class to merge tiles and create a stitched volume. Typically used at a lower resolution for registering
    the sample to an Atlas.

    &#34;&#34;&#34;
    def __init__(self, tiles, database):
        &#34;&#34;&#34;
        Constructor for the tileMerger class.

        Parameters
        ----------
        tiles:  (tileParser) tileParser object containing the dataset to merge.
        database: (pd.DataFrame, str) database or path to the database containing the registered tile position
        n_planes: (int) number of planes per files.
        &#34;&#34;&#34;

        if isinstance(database, str):
            self.database = pd.read_csv(database)
        else:
            self.database = database
        self.tiles = tiles
        self.lazy = self._find_if_lazy()
        self.type = tiles.type
        self.frame_size = tiles.frame_size
        self.n_planes = self._get_n_planes()
        self.n_tiles = tiles.n_tiles
        self.n_row = tiles.nrow
        self.n_col = tiles.ncol

        # Size of the merged array (to be defined when the merged array is initialized).
        self.nx = None
        self.ny = None
        self.nz = None

        self.downsample = 1
        self.level_delta = 0
        self.merged_data = None

    def merge_additive(self, reconstruction_mode=&#39;constant&#39;, tree_mode=&#39;mean&#39;):
        &#34;&#34;&#34;
        Perform merging with a mean algorithm for overlapping areas. Maximum merging should be preferred to
        avoid integer overflowing and higher signals on the overlapping areas.

        Parameters
        ----------
        mode: (str) APR reconstruction type among (&#39;constant&#39;, &#39;smooth&#39;, &#39;level&#39;)

        Returns
        -------
        None

        &#34;&#34;&#34;

        if self.merged_data is None:
            self._initialize_merged_array()

        H_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        H_pos = (H_pos - H_pos.min())/self.downsample
        V_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        V_pos = (V_pos - V_pos.min())/self.downsample
        D_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        D_pos = (D_pos - D_pos.min())/self.downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):

            if self.type == &#39;apr&#39;:
                if self.lazy:
                    tile.lazy_load_tile(level_delta=self.level_delta)
                    data = tile.lazy_data[:, :, :]
                else:
                    tile.load_tile()
                    u = pyapr.data_containers.APRSlicer(tile.apr, tile.parts, level_delta=self.level_delta,
                                                        mode=reconstruction_mode, tree_mode=tree_mode)
                    data = u[:, :, :]
            else:
                tile.load_tile()
                data = tile.data

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + data.shape[2])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + data.shape[1])
            z1 = int(D_pos[i])
            z2 = int(D_pos[i] + data.shape[0])

            self.merged_data[z1:z2, y1:y2, x1:x2] = self.merged_data[z1:z2, y1:y2, x1:x2] + data
            self.merged_data = self.merged_data.astype(&#39;uint16&#39;)

    def merge_max(self, reconstruction_mode=&#39;constant&#39;, tree_mode=&#39;mean&#39;, debug=False):
        &#34;&#34;&#34;
        Perform merging with a maximum algorithm for overlapping areas.

        Parameters
        ----------
        mode: (str) APR reconstruction type among (&#39;constant&#39;, &#39;smooth&#39;, &#39;level&#39;)
        debug: (bool) add white border on the edge of each tile to see where it was overlapping.

        Returns
        -------
        None

        &#34;&#34;&#34;

        if self.merged_data is None:
            self._initialize_merged_array()

        H_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        H_pos = (H_pos - H_pos.min())/self.downsample
        V_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        V_pos = (V_pos - V_pos.min())/self.downsample
        D_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        D_pos = (D_pos - D_pos.min())/self.downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):

            if self.type == &#39;apr&#39;:
                if self.lazy:
                    tile.lazy_load_tile(level_delta=self.level_delta)
                    data = tile.lazy_data[:, :, :]
                else:
                    tile.load_tile()
                    u = pyapr.data_containers.APRSlicer(tile.apr, tile.parts, level_delta=self.level_delta,
                                                        mode=reconstruction_mode, tree_mode=tree_mode)
                    data = u[:, :, :]
            else:
                tile.load_tile()
                data = downscale_local_mean(tile.data, factors=(self.downsample, self.downsample, self.downsample))

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :, :] = 2**16-1
                data[-1, :, :] = 2 ** 16 - 1
                data[:, 0, :] = 2 ** 16 - 1
                data[:, -1, :] = 2 ** 16 - 1
                data[:, :, 0] = 2 ** 16 - 1
                data[:, :, -1] = 2 ** 16 - 1

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + data.shape[2])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + data.shape[1])
            z1 = int(D_pos[i])
            z2 = int(D_pos[i] + data.shape[0])

            self.merged_data[z1:z2, y1:y2, x1:x2] = np.maximum(self.merged_data[z1:z2, y1:y2, x1:x2], data)

    def crop(self, background=0, xlim=None, ylim=None, zlim=None):
        &#34;&#34;&#34;
        Add a black mask around the brain (rather than really cropping which makes the overlays complicated in
        a later stage).

        Parameters
        ----------
        background: (int) constant value to replace the cropped area with.
        xlim: (list) x limits for cropping
        ylim: (list) y limits for cropping
        zlim: (list) z limits for cropping

        Returns
        -------
        None
        &#34;&#34;&#34;

        if self.merged_data is None:
            raise TypeError(&#39;Error: please merge data before cropping.&#39;)

        if xlim is not None:
            if xlim[0] != 0:
                self.merged_data[:, :, :xlim[0]] = background
            if xlim[1] != self.merged_data.shape[2]:
                self.merged_data[:, :, xlim[1]:] = background
        if ylim is not None:
            if ylim[0] != 0:
                self.merged_data[:, :ylim[0], :] = background
            if ylim[1] != self.merged_data.shape[1]:
                self.merged_data[:, ylim[1]:, :] = background
        if zlim is not None:
            if zlim[0] != 0:
                self.merged_data[:zlim[0], :, :] = background
            if zlim[1] != self.merged_data.shape[0]:
                self.merged_data[zlim[1]:, :, :] = background

    def equalize_hist(self, method=&#39;skimage&#39;):
        &#34;&#34;&#34;
        Perform histogram equalization to improve the contrast on merged data.
        Both OpenCV (only 2D) and Skimage (3D but 10 times slower) are available.

        Parameters
        ----------
        method: (str) method for performing histogram equalization among &#39;skimage&#39; and &#39;opencv&#39;.

        Returns
        -------
        None
        &#34;&#34;&#34;

        if self.merged_data is None:
            raise TypeError(&#39;Error: please merge data before equalizing histogram.&#39;)

        if method == &#39;opencv&#39;:
            # clahe = cv.createCLAHE(tileGridSize=(8, 8))
            # for i in range(self.merged_data.shape[0]):
                # self.merged_data[i] = clahe.apply(self.merged_data[i])
            print(&#39;opencv not currently supported due to incompatibility with pyqt5&#39;)
        elif method == &#39;skimage&#39;:
            self.merged_data = equalize_adapthist(self.merged_data)
        else:
            raise ValueError(&#39;Error: unknown method for adaptive histogram normalization.&#39;)

    def set_downsample(self, downsample):
        &#34;&#34;&#34;
        Set the downsampling value for the merging reconstruction.

        Parameters
        ----------
        downsample: (int) downsample factor

        Returns
        -------
        None

        &#34;&#34;&#34;

        # TODO: find a more rigorous way of enforcing this. (Probably requires that the APR is loaded).
        if downsample not in [1, 2, 4, 8, 16, 32]:
            raise ValueError(&#39;Error: downsample value should be compatible with APR levels.&#39;)

        self.downsample = downsample
        self.level_delta = int(-np.log2(self.downsample))

    def _initialize_merged_array(self):
        &#34;&#34;&#34;
        Initialize the merged array in accordance with the asked downsampling.

        Returns
        -------
        None
        &#34;&#34;&#34;

        self.nx = int(np.ceil(self._get_nx() / self.downsample))
        self.ny = int(np.ceil(self._get_ny() / self.downsample))
        self.nz = int(np.ceil(self._get_nz() / self.downsample))

        self.merged_data = np.zeros((self.nz, self.ny, self.nx), dtype=&#39;uint16&#39;)

    def _get_nx(self):
        &#34;&#34;&#34;
        Compute the merged array size for x dimension.

        Returns
        -------
        (int) x size for merged array
        &#34;&#34;&#34;
        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        return x_pos.max() - x_pos.min() + self.frame_size

    def _get_ny(self):
        &#34;&#34;&#34;
        Compute the merged array size for y dimension.

        Returns
        -------
        (int) y size for merged array
        &#34;&#34;&#34;
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        return y_pos.max() - y_pos.min() + self.frame_size

    def _get_nz(self):
        &#34;&#34;&#34;
        Compute the merged array size for y dimension.

        Returns
        -------
        (int) y size for merged array
        &#34;&#34;&#34;
        z_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        return z_pos.max() - z_pos.min() + self.n_planes

    def _get_n_planes(self):
        &#34;&#34;&#34;
        Load a tile and check the number of planes per tile.

        Returns
        -------
        (int) Number of planes per tile;
        &#34;&#34;&#34;

        tile = self.tiles[0]
        if self.type == &#39;apr&#39;:
            if self.lazy:
                tile.lazy_load_tile()
                return tile.lazy_data.shape[0]
            else:
                tile.load_tile()
                return tile.apr.shape()[0]
        else:
            tile.load_tile()
            return tile.data.shape[0]

    def _find_if_lazy(self):
        &#34;&#34;&#34;
        Function to test if all tile can be lazy loaded.

        Returns
        -------
        (bool)
        &#34;&#34;&#34;

        try:
            for tile in self.tiles:
                tile.lazy_load_tile()
        except:
            return False

        return True</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="pipapr.stitcher.max_sum_over_single_max"><code class="name flex">
<span>def <span class="ident">max_sum_over_single_max</span></span>(<span>reference_image, moving_image, d)</span>
</code></dt>
<dd>
<div class="desc"><p>This function is a reliability metric which works well for sparse data. It computes the 99 percentile of the sum
of reference and shifted image divided by twice the 99 percentile of the reference image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>reference_image</code></strong> :&ensp;<code>(array) 2D array</code> of <code>the reference image</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>moving_image</code></strong> :&ensp;<code>(array) 2D array</code> of <code>the moving image image</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>d_correct</code></strong> :&ensp;<code>(array) registration parameters</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def max_sum_over_single_max(reference_image, moving_image, d):
    &#34;&#34;&#34;
    This function is a reliability metric which works well for sparse data. It computes the 99 percentile of the sum
    of reference and shifted image divided by twice the 99 percentile of the reference image.

    Parameters
    ----------
    reference_image: (array) 2D array of the reference image
    moving_image: (array) 2D array of the moving image image
    d_correct: (array) registration parameters

    Returns
    -------

    &#34;&#34;&#34;

    shifted_image = warp(moving_image, AffineTransform(translation=[d[1], d[0]]), mode=&#39;wrap&#39;, preserve_range=True)

    e = (2*np.percentile(reference_image, 99))/np.percentile(reference_image+shifted_image, 99)

    return e</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.mse"><code class="name flex">
<span>def <span class="ident">mse</span></span>(<span>reference_image, moving_image, d)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mse(reference_image, moving_image, d):

    shifted_image = warp(moving_image, AffineTransform(translation=[d[1], d[0]]), mode=&#39;wrap&#39;,
                         preserve_range=True)

    return normalized_root_mse(reference_image, shifted_image, normalization=&#39;mean&#39;)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.phase_cross_correlation"><code class="name flex">
<span>def <span class="ident">phase_cross_correlation</span></span>(<span>reference_image, moving_image, upsample_factor=1, return_error=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Phase cross correlation. Because skimage function compute the NORMAL cross correlation to estimate the shift I
modified it to compute the TRUE phase cross correlation, as per the standard definition.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>reference_image</code></strong> :&ensp;<code>array</code></dt>
<dd>Reference image.</dd>
<dt><strong><code>moving_image</code></strong> :&ensp;<code>array</code></dt>
<dd>Image to register. Must be same dimensionality as
<code>reference_image</code>.</dd>
<dt><strong><code>upsample_factor</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Upsampling factor. Images will be registered to within
<code>1 / upsample_factor</code> of a pixel. For example
<code>upsample_factor == 20</code> means the images will be registered
within 1/20th of a pixel. Default is 1 (no upsampling).
Not used if any of <code>reference_mask</code> or <code>moving_mask</code> is not None.</dd>
<dt><strong><code>return_error</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Returns error and phase difference if on, otherwise only
shifts are returned. Has noeffect if any of <code>reference_mask</code> or
<code>moving_mask</code> is not None. In this case only shifts is returned.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>shifts</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Shift vector (in pixels) required to register <code>moving_image</code>
with <code>reference_image</code>. Axis ordering is consistent with
numpy (e.g. Z, Y, X)</dd>
<dt><strong><code>error</code></strong> :&ensp;<code>float</code></dt>
<dd>Translation invariant normalized RMS error between
<code>reference_image</code> and <code>moving_image</code>.</dd>
<dt><strong><code>phasediff</code></strong> :&ensp;<code>float</code></dt>
<dd>Global phase difference between the two images (should be
zero if images are non-negative).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def phase_cross_correlation(reference_image,
                            moving_image,
                            upsample_factor=1,
                            return_error=True):
    &#34;&#34;&#34;
    Phase cross correlation. Because skimage function compute the NORMAL cross correlation to estimate the shift I
    modified it to compute the TRUE phase cross correlation, as per the standard definition.

    Parameters
    ----------
    reference_image : array
        Reference image.
    moving_image : array
        Image to register. Must be same dimensionality as
        ``reference_image``.
    upsample_factor : int, optional
        Upsampling factor. Images will be registered to within
        ``1 / upsample_factor`` of a pixel. For example
        ``upsample_factor == 20`` means the images will be registered
        within 1/20th of a pixel. Default is 1 (no upsampling).
        Not used if any of ``reference_mask`` or ``moving_mask`` is not None.
    return_error : bool, optional
        Returns error and phase difference if on, otherwise only
        shifts are returned. Has noeffect if any of ``reference_mask`` or
        ``moving_mask`` is not None. In this case only shifts is returned.

    Returns
    -------
    shifts : ndarray
        Shift vector (in pixels) required to register ``moving_image``
        with ``reference_image``. Axis ordering is consistent with
        numpy (e.g. Z, Y, X)
    error : float
        Translation invariant normalized RMS error between
        ``reference_image`` and ``moving_image``.
    phasediff : float
        Global phase difference between the two images (should be
        zero if images are non-negative).
    &#34;&#34;&#34;

    # images must be the same shape
    if reference_image.shape != moving_image.shape:
        raise ValueError(&#34;images must be same shape&#34;)

    src_freq = np.fft.fftn(reference_image)
    target_freq = np.fft.fftn(moving_image)

    # Whole-pixel shift - Compute cross-correlation by an IFFT
    shape = src_freq.shape
    image_product = src_freq * target_freq.conj()
    eps = np.finfo(image_product.real.dtype).eps
    image_product /= (np.abs(image_product) + eps)
    cross_correlation = np.fft.ifftn(image_product)

    # Locate maximum
    maxima = np.unravel_index(np.argmax(np.abs(cross_correlation)),
                              cross_correlation.shape)
    midpoints = np.array([np.fix(axis_size / 2) for axis_size in shape])

    shifts = np.stack(maxima).astype(np.float64)
    shifts[shifts &gt; midpoints] -= np.array(shape)[shifts &gt; midpoints]

    if upsample_factor == 1:
        if return_error:
            src_amp = np.sum(np.real(src_freq * src_freq.conj()))
            src_amp /= src_freq.size
            target_amp = np.sum(np.real(target_freq * target_freq.conj()))
            target_amp /= target_freq.size
            CCmax = cross_correlation[maxima]
    # If upsampling &gt; 1, then refine estimate with matrix multiply DFT
    else:
        raise ValueError(&#39;Error: upsampled phase cross corrrelation not implemented here, use skimage.&#39;)

    # If its only one row or column the shift along that dimension has no
    # effect. We set to zero.
    for dim in range(src_freq.ndim):
        if shape[dim] == 1:
            shifts[dim] = 0

    if return_error:
        error = np.real(1.0 - CCmax * CCmax.conj())
        phase_diff = np.arctan2(CCmax.imag, CCmax.real)
        return shifts, np.sqrt(np.abs(error)), phase_diff
    else:
        return shifts</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.phase_cross_correlation_cv"><code class="name flex">
<span>def <span class="ident">phase_cross_correlation_cv</span></span>(<span>reference_image, moving_image)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute openCV to compute the phase cross correlation. It is around 16 times faster than the implementation using
numpy FFT (same as skimage).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>reference_image</code></strong> :&ensp;<code>array</code></dt>
<dd>Reference image.</dd>
<dt><strong><code>moving_image</code></strong> :&ensp;<code>array</code></dt>
<dd>Image to register. Must be same dimensionality as
<code>reference_image</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>shifts</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Shift vector (in pixels) required to register <code>moving_image</code>
with <code>reference_image</code>. Axis ordering is consistent with
numpy (e.g. Z, Y, X)</dd>
<dt><strong><code>error</code></strong> :&ensp;<code>float</code></dt>
<dd>Peak response (see opencv description here:
<a href="https://docs.opencv.org/4.5.3/d7/df3/group__imgproc__motion.html#ga552420a2ace9ef3fb053cd630fdb4952">https://docs.opencv.org/4.5.3/d7/df3/group__imgproc__motion.html#ga552420a2ace9ef3fb053cd630fdb4952</a>)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def phase_cross_correlation_cv(reference_image, moving_image):
        &#34;&#34;&#34;
        Compute openCV to compute the phase cross correlation. It is around 16 times faster than the implementation using
        numpy FFT (same as skimage).

        Parameters
        ----------
        reference_image : array
            Reference image.
        moving_image : array
            Image to register. Must be same dimensionality as
            ``reference_image``.

        Returns
        -------
        shifts : ndarray
            Shift vector (in pixels) required to register ``moving_image``
            with ``reference_image``. Axis ordering is consistent with
            numpy (e.g. Z, Y, X)
        error : float
            Peak response (see opencv description here:
            https://docs.opencv.org/4.5.3/d7/df3/group__imgproc__motion.html#ga552420a2ace9ef3fb053cd630fdb4952)
        &#34;&#34;&#34;

        d, e = cv.phaseCorrelate(reference_image.astype(np.float32), moving_image.astype(np.float32))

        d_correct = [-np.round(d[1]).astype(np.int), -np.round(d[0]).astype(np.int)]

        return d_correct</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.reconstruct_middle_frame"><code class="name flex">
<span>def <span class="ident">reconstruct_middle_frame</span></span>(<span>tiles, database, downsample, debug=False, z=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reconstruct_middle_frame(tiles,
                             database,
                             downsample,
                             debug=False,
                             z=None):

    if isinstance(database, pipapr.stitcher.tileStitcher):
        database = database.database
    elif isinstance(database, pd.DataFrame):
        database = database
    elif isinstance(database, str):
        database = pd.read_csv(database)
    else:
        raise TypeError(&#39;Error: unknown type for database.&#39;)

    level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

    tile = tiles[0]
    tile.lazy_load_tile(level_delta=level_delta)
    frame_size = tile.lazy_data.shape[1:]
    x_pos = database[&#39;ABS_H&#39;].to_numpy()
    nx = int(np.ceil((x_pos.max() - x_pos.min())/downsample + frame_size[1]))
    y_pos = database[&#39;ABS_V&#39;].to_numpy()
    ny = int(np.ceil((y_pos.max() - y_pos.min())/downsample + frame_size[0]))

    if z is None:
        z = int(tile.lazy_data.shape[0] / 2)

    merged_data = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

    H_pos = database[&#39;ABS_H&#39;].to_numpy()
    H_pos = (H_pos - H_pos.min()) / downsample
    V_pos = database[&#39;ABS_V&#39;].to_numpy()
    V_pos = (V_pos - V_pos.min()) / downsample

    for i, tile in enumerate(tqdm(tiles), desc=&#39;Merging&#39;):
        tile.lazy_load_tile(level_delta=level_delta)
        data = tile.lazy_data[z]

        # In debug mode we highlight each tile edge to see where it was
        if debug:
            data[0, :] = 2 ** 16 - 1
            data[-1, :] = 2 ** 16 - 1
            data[:, 0] = 2 ** 16 - 1
            data[:, -1] = 2 ** 16 - 1

        x1 = int(H_pos[i])
        x2 = int(H_pos[i] + data.shape[1])
        y1 = int(V_pos[i])
        y2 = int(V_pos[i] + data.shape[0])

        merged_data[y1:y2, x1:x2] = np.maximum(merged_data[y1:y2, x1:x2], data)

    return merged_data</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pipapr.stitcher.baseStitcher"><code class="flex name class">
<span>class <span class="ident">baseStitcher</span></span>
<span>(</span><span>tiles: <a title="pipapr.parser.tileParser" href="parser.html#pipapr.parser.tileParser">tileParser</a>, overlap_h: (<class 'int'>, <class 'float'>), overlap_v: (<class 'int'>, <class 'float'>))</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for stitching multi-tile data.</p>
<p>Constructor for the baseStitcher class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>tiles: (tileParser) tileParser object containing the dataset to stitch.</dt>
<dt><strong><code>overlap_h</code></strong> :&ensp;<code>(float) expected horizontal overlap in %</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>overlap_v</code></strong> :&ensp;<code>(float) expected vertical overlap in %</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class baseStitcher():
    &#34;&#34;&#34;
    Base class for stitching multi-tile data.

    &#34;&#34;&#34;
    def __init__(self,
                 tiles: pipapr.parser.tileParser,
                 overlap_h: (int, float),
                 overlap_v: (int, float)):
        &#34;&#34;&#34;
        Constructor for the baseStitcher class.

        Parameters
        ----------
        tiles: (tileParser) tileParser object containing the dataset to stitch.
        overlap_h: (float) expected horizontal overlap in %
        overlap_v: (float) expected vertical overlap in %

        &#34;&#34;&#34;
        self.tiles = tiles
        self.ncol = tiles.ncol
        self.nrow = tiles.nrow
        self.n_vertex = tiles.n_tiles
        self.n_edges = tiles.n_edges
        self.frame_size = tiles.frame_size

        self.expected_overlap_h = int(overlap_h/100*self.frame_size)
        self.expected_overlap_v = int(overlap_v/100*self.frame_size)

        self.overlap_h = int(self.expected_overlap_h*1.2)
        if self.expected_overlap_h &gt; self.frame_size:
            self.expected_overlap_h = self.frame_size
        self.overlap_v = int(self.expected_overlap_v*1.2)
        if self.expected_overlap_v &gt; self.frame_size:
            self.expected_overlap_v = self.frame_size

        self.mask = False
        self.threshold = None

        self.segment = False
        self.segmenter = None

        self.reg_x = int(self.frame_size*0.05)
        self.reg_y = int(self.frame_size*0.05)
        self.reg_z = 20

        self.z_begin = None
        self.z_end = None

    def activate_mask(self, threshold):
        &#34;&#34;&#34;
        Activate the masked cross-correlation for the displacement estimation. Pixels above threshold are
        not taken into account.

        Parameters
        ----------
        threshold: (int) threshold for the cross-correlation mask as a percentage of pixel to keep (e.g. 95 will
                    create a mask removing the 5% brightest pixels).

        &#34;&#34;&#34;
        self.mask = True
        self.threshold = threshold

    def deactivate_mask(self):
        &#34;&#34;&#34;
        Deactivate the masked cross-correlation and uses a classical cross correlation.

        &#34;&#34;&#34;
        self.mask = False
        self.threshold = None

    def save_database(self, path):
        &#34;&#34;&#34;
        Save database at the given path. The database must be built before calling this method.

        Parameters
        ----------
        path: (str) path to save the database.

        &#34;&#34;&#34;

        if self.database is None:
            raise TypeError(&#39;Error: database can&#39;&#39;t be saved because it was not created. &#39;
                            &#39;Please call build_database() first.&#39;)

        self.database.to_csv(path)

    def activate_segmentation(self,
                              segmenter):
        &#34;&#34;&#34;
        Activate the segmentation. When a tile is loaded it is segmented before the stitching is done.

        Parameters
        ----------
        segmenter: (tileSegmenter) segmenter object for segmenting each tile.

        &#34;&#34;&#34;
        self.segment = True
        self.segmenter = segmenter

    def deactivate_segmentation(self):
        &#34;&#34;&#34;
        Deactivate tile segmentation.

        &#34;&#34;&#34;

        self.segment = False

    def reconstruct_slice(self, loc=None, n_proj=0, dim=0, downsample=1, color=False, debug=False, plot=True):

        if dim == 0:
            return self._reconstruct_z_slice(z=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)
        elif dim == 1:
            return self._reconstruct_y_slice(y=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)
        elif dim == 2:
            return self._reconstruct_x_slice(x=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)

    def set_regularization(self, reg_x, reg_y, reg_z):
        &#34;&#34;&#34;
        Set the regularization for the stitching to prevent aberrant displacements.

        Parameters
        ----------
        reg_x: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                     reg_x (in pixel unit) then the expected displacement (from motor position) is taken.
        reg_y: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                     reg_z (in pixel unit) then the expected displacement (from motor position) is taken.
        reg_z: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                     reg_z (in pixel unit) then the expected displacement (from motor position) is taken.

        Returns
        -------

        &#34;&#34;&#34;

        self.reg_x = reg_x
        self.reg_y = reg_y
        self.reg_z = reg_z

    def reconstruct_z_color(self, z=None, n_proj=0, downsample=1, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        z: (int) reconstruction depth
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at depth z.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)

        if z is None:
            z = int(tile.lazy_data.shape[0] / 2)

        if z &gt; tile.lazy_data.shape[0]:
            raise ValueError(&#39;Error: z is too large ({}), maximum depth at this downsample is {}.&#39;.format(z, tile.lazy_data.shape[0]))

        frame_size = tile.lazy_data.shape[1:]
        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + frame_size[1]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + frame_size[0]))

        H = np.zeros((ny, nx), dtype=&#39;uint16&#39;)
        S = np.ones((ny, nx), dtype=&#39;uint16&#39;) * 0.7
        V = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

        H_pos = (x_pos - x_pos.min()) / downsample
        V_pos = (y_pos - y_pos.min()) / downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            zf = min(z+n_proj, tile.lazy_data.shape[0])
            data = tile.lazy_data[z:zf]
            v = data.max(axis=0)
            h = np.argmax(data, axis=0)

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                v[0, :] = 2**16-1
                v[-1, :] = 2**16-1
                v[:, 0] = 2**16-1
                v[:, -1] = 2**16-1

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + v.shape[1])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + v.shape[0])

            V[y1:y2, x1:x2] = np.maximum(V[y1:y2, x1:x2], v)
            H[y1:y2, x1:x2] = np.maximum(H[y1:y2, x1:x2], h)

        H = rescale_intensity(gaussian(H, sigma=5), out_range=np.float64)*0.66
        V = np.log(V + 200)
        vmin, vmax = np.percentile(V[V &gt; np.log(100)], (1, 99.9))
        V = rescale_intensity(V, in_range=(vmin, vmax), out_range=np.float64)
        S = S * V
        rgb = hsv_to_rgb(np.dstack((H,S,V)))
        rescale_intensity(rgb, out_range=&#39;uint8&#39;)

        if plot:
            plt.figure()
            plt.imshow(rgb)

        return rgb

    def _reconstruct_z_slice(self, z=None, n_proj=0, downsample=1, color=False, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        z: (int) reconstruction depth
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at depth z.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)

        if z is None:
            z = int(tile.lazy_data.shape[0] / 2)

        if z &gt; tile.lazy_data.shape[0]:
            raise ValueError(&#39;Error: z is too large ({}), maximum depth at this downsample is {}.&#39;.format(z, tile.lazy_data.shape[0]))

        frame_size = tile.lazy_data.shape[1:]
        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + frame_size[1]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + frame_size[0]))

        if color:
            merged_data = np.ones((ny, nx, 3), dtype=&#39;uint16&#39;)
            merged_data[:, :, 2] = 0
        else:
            merged_data = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

        H_pos = (x_pos - x_pos.min()) / downsample
        V_pos = (y_pos - y_pos.min()) / downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            zf = min(z+n_proj, tile.lazy_data.shape[0])
            if zf &gt; z:
                data = tile.lazy_data[z:zf].max(axis=0)
            else:
                data = tile.lazy_data[z]

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :] = 2**16-1
                data[-1, :] = 2**16-1
                data[:, 0] = 2**16-1
                data[:, -1] = 2**16-1

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + data.shape[1])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + data.shape[0])

            if color:
                if tile.col % 2:
                    if tile.row % 2:
                        merged_data[y1:y2, x1:x2, 0] = np.maximum(merged_data[y1:y2, x1:x2, 1], data)
                    else:
                        merged_data[y1:y2, x1:x2, 1] = np.maximum(merged_data[y1:y2, x1:x2, 0], data)
                else:
                    if tile.row % 2:
                        merged_data[y1:y2, x1:x2, 1] = np.maximum(merged_data[y1:y2, x1:x2, 1], data)
                    else:
                        merged_data[y1:y2, x1:x2, 0] = np.maximum(merged_data[y1:y2, x1:x2, 0], data)
            else:
                merged_data[y1:y2, x1:x2] = np.maximum(merged_data[y1:y2, x1:x2], data)

        if plot:
            plt.figure()
            if color:
                plt.imshow(self._process_RGB_for_display(merged_data))
            else:
                plt.imshow(np.log(merged_data), cmap=&#39;gray&#39;)

        return merged_data

    def _reconstruct_y_slice(self, y=None, n_proj=0, downsample=1, color=False, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        y: (int) reconstruction location in y
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at position y.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)
        tile_shape = tile.lazy_data.shape

        if y is None:
            y = int(tile_shape[1]*self.tiles.nrow/2)

        if y &gt; tile.lazy_data.shape[1]*self.tiles.nrow:
            raise ValueError(&#39;Error: y is too large ({}), maximum depth at this downsample is {}.&#39;
                             .format(y, tile.lazy_data.shape[1]*self.tiles.nrow))

        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + tile_shape[2]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + tile_shape[1]))
        z_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        nz = int(np.ceil((z_pos.max() - z_pos.min()) / downsample + tile_shape[0]))

        # Determine tiles to load
        tiles_to_load = []
        tiles_pos = []
        for x_loc, y_loc, z_loc, tile in zip(x_pos/downsample, y_pos/downsample, z_pos/downsample, self.tiles):
            if (y &gt; y_loc) and (y &lt; y_loc+tile_shape[1]):
                tiles_to_load.append(tile)
                tiles_pos.append([z_loc, y_loc, x_loc])
        tiles_pos = np.array(tiles_pos).astype(&#39;uint64&#39;)

        if color:
            merged_data = np.ones((nz, nx, 3), dtype=&#39;uint16&#39;)
            merged_data[:, :, 2] = 0
        else:
            merged_data = np.zeros((nz, nx), dtype=&#39;uint16&#39;)

        for i, tile in enumerate(tqdm(tiles_to_load, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            y_tile = int(y - tiles_pos[i, 1])
            yf = min(y_tile+n_proj, tiles_pos[i, 1]+tile.lazy_data.shape[1])
            if yf &gt; y:
                data = tile.lazy_data[:, y_tile:yf, :].max(axis=1)
            else:
                data = tile.lazy_data[:, y_tile, :]

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :] = 2**16-1
                data[-1, :] = 2**16-1
                data[:, 0] = 2**16-1
                data[:, -1] = 2**16-1

            x1 = int(tiles_pos[i, 2])
            x2 = int(tiles_pos[i, 2] + data.shape[1])
            z1 = int(tiles_pos[i, 0])
            z2 = int(tiles_pos[i, 0] + data.shape[0])
            
            if color:
                if tile.col % 2:
                    if tile.row % 2:
                        merged_data[z1:z2, x1:x2, 0] = np.maximum(merged_data[z1:z2, x1:x2, 1], data)
                    else:
                        merged_data[z1:z2, x1:x2, 1] = np.maximum(merged_data[z1:z2, x1:x2, 0], data)
                else:
                    if tile.row % 2:
                        merged_data[z1:z2, x1:x2, 1] = np.maximum(merged_data[z1:z2, x1:x2, 1], data)
                    else:
                        merged_data[z1:z2, x1:x2, 0] = np.maximum(merged_data[z1:z2, x1:x2, 0], data)
            else:
                merged_data[z1:z2, x1:x2] = np.maximum(merged_data[z1:z2, x1:x2], data)

        if plot:
            plt.figure()
            if color:
                plt.imshow(self._process_RGB_for_display(merged_data))
            else:
                plt.imshow(np.log(merged_data), cmap=&#39;gray&#39;)

        return merged_data

    def _reconstruct_x_slice(self, x=None, n_proj=0, downsample=1, color=False, debug=False, plot=True):
        &#34;&#34;&#34;
        Reconstruct and merge the sample at a given depth z.

        Parameters
        ----------
        x: (int) reconstruction location in x
        downsample: (int) downsample for reconstruction (must be a power of 2)
        debug: (bool) if true the border of each tile will be highlighted

        Returns
        -------
        Merged frame at position x.
        &#34;&#34;&#34;

        level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

        tile = self.tiles[0]
        tile.lazy_load_tile(level_delta=level_delta)
        tile_shape = tile.lazy_data.shape

        if x is None:
            x = int(tile_shape[2]*self.tiles.ncol/2)

        if x &gt; tile.lazy_data.shape[2]*self.tiles.ncol:
            raise ValueError(&#39;Error: y is too large ({}), maximum depth at this downsample is {}.&#39;
                             .format(x, tile.lazy_data.shape[2]*self.tiles.ncol))

        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + tile_shape[2]))
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + tile_shape[1]))
        z_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        nz = int(np.ceil((z_pos.max() - z_pos.min()) / downsample + tile_shape[0]))

        # Determine tiles to load
        tiles_to_load = []
        tiles_pos = []
        for x_loc, y_loc, z_loc, tile in zip(x_pos/downsample, y_pos/downsample, z_pos/downsample, self.tiles):
            if (x &gt; x_loc) and (x &lt; x_loc+tile_shape[2]):
                tiles_to_load.append(tile)
                tiles_pos.append([z_loc, y_loc, x_loc])
        tiles_pos = np.array(tiles_pos).astype(&#39;uint64&#39;)

        if color:
            merged_data = np.ones((nz, ny, 3), dtype=&#39;uint16&#39;)
            merged_data[:, :, 2] = 0
        else:
            merged_data = np.zeros((nz, ny), dtype=&#39;uint16&#39;)

        for i, tile in enumerate(tqdm(tiles_to_load, desc=&#39;Merging&#39;)):
            tile.lazy_load_tile(level_delta=level_delta)
            x_tile = int(x - tiles_pos[i, 2])
            xf = min(x_tile+n_proj, tiles_pos[i, 2]+tile.lazy_data.shape[2])
            if xf &gt; x:
                data = tile.lazy_data[:, :, x_tile:xf].max(axis=2)
            else:
                data = tile.lazy_data[:, :, x_tile]

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :] = 2**16-1
                data[-1, :] = 2**16-1
                data[:, 0] = 2**16-1
                data[:, -1] = 2**16-1

            y1 = int(tiles_pos[i, 1])
            y2 = int(tiles_pos[i, 1] + data.shape[1])
            z1 = int(tiles_pos[i, 0])
            z2 = int(tiles_pos[i, 0] + data.shape[0])
            
            if color:
                if tile.col % 2:
                    if tile.row % 2:
                        merged_data[z1:z2, y1:y2, 0] = np.maximum(merged_data[z1:z2, y1:y2, 1], data)
                    else:
                        merged_data[z1:z2, y1:y2, 1] = np.maximum(merged_data[z1:z2, y1:y2, 0], data)
                else:
                    if tile.row % 2:
                        merged_data[z1:z2, y1:y2, 1] = np.maximum(merged_data[z1:z2, y1:y2, 1], data)
                    else:
                        merged_data[z1:z2, y1:y2, 0] = np.maximum(merged_data[z1:z2, y1:y2, 0], data)
            else:
                merged_data[z1:z2, y1:y2] = np.maximum(merged_data[z1:z2, y1:y2], data)

        if plot:
            plt.figure()
            if color:
                plt.imshow(self._process_RGB_for_display(merged_data))
            else:
                plt.imshow(np.log(merged_data), cmap=&#39;gray&#39;)

        return merged_data

    def _process_RGB_for_display(self, u):
        &#34;&#34;&#34;
        Process RGB data for correctly displaying it.

        Parameters
        ----------
        u: (array) RGB data

        Returns
        -------
        data_to_display: (array) RGB data displayable with correct contrast and colors.
        &#34;&#34;&#34;
        data_to_display = np.zeros_like(u, dtype=&#39;uint8&#39;)
        for i in range(2):
            tmp = np.log(u[:, :, i] + 200)
            vmin, vmax = np.percentile(tmp[tmp &gt; np.log(1 + 200)], (1, 99.9))
            data_to_display[:, :, i] = rescale_intensity(tmp, in_range=(vmin, vmax), out_range=&#39;uint8&#39;)

        return data_to_display

    def check_files_integrity(self):
        cnt = 0
        for tile in self.tiles:
            lazy = 1
            try:
                tile.lazy_load_tile()
            except:
                lazy = 0
                print(&#39;Lazy load failed on ({}, {})\n Trying normal loading...&#39;.format(tile.row, tile.col))

            if lazy == 0:
                try:
                    tile.load_tile()
                except:
                    cnt += 1
                    print(&#39;Problem detected with tile ({}, {})&#39;.format(tile.row, tile.col))

        if cnt == 0:
            print(&#39;All tiles are readable.&#39;)

    def _compute_shift(self, reference_image, moving_image):
        &#34;&#34;&#34;
        Backbone function to compute the registration and the registration error used for the global optimisation.
        This function can be replaced by experienced user to use their own registration and error estimation functions.

        Parameters
        ----------
        reference_image : array
            Reference image.
        moving_image : array
            Image to register. Must be same dimensionality as
            ``reference_image``.

        Returns
        -------
        d: (array) registration parameters found
        e: (float) error estimation for the registration (the higher the error the higher the uncertainty)
        &#34;&#34;&#34;

        d = phase_cross_correlation_cv(reference_image, moving_image)
        e = max_sum_over_single_max(reference_image, moving_image, d)

        return d, e

    def _get_max_proj_apr(self, apr, parts, patch, patch_yx=None, plot=False):
        &#34;&#34;&#34;
        Compute maximum projection on 3D APR data.

        Parameters
        ----------
        apr: (pyapr.APR) apr tree
        parts: (pyapr.ParticlData) apr particle
        patch: (pyapr.patch) patch for computing the projection only on the overlapping area.
        plot: (bool) control data plotting

        Returns
        -------
        (list of np.array) maximum intensity projection in each 3 dimension.

        &#34;&#34;&#34;
        proj = []
        if patch_yx is None:
            for d in range(3):
                # dim=0: project along Y to produce a ZY plane
                # dim=1: project along X to produce a ZX plane
                # dim=2: project along Z to produce an YX plane
                proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                                   dim=d, patch=patch, method=&#39;auto&#39;))
        else:
            proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                               dim=0, patch=patch, method=&#39;auto&#39;))
            proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                               dim=1, patch=patch, method=&#39;auto&#39;))
            proj.append(pyapr.numerics.transform.projection.maximum_projection(apr, parts,
                                                                               dim=2, patch=patch_yx, method=&#39;auto&#39;))

        if plot:
            fig, ax = plt.subplots(1, 3)
            for i, title in enumerate([&#39;ZY&#39;, &#39;ZX&#39;, &#39;YX&#39;]):
                ax[i].imshow(proj[i], cmap=&#39;gray&#39;)
                ax[i].set_title(title)

        return proj[0], proj[1], proj[2]

    def _get_proj_shifts(self, proj1, proj2):
        &#34;&#34;&#34;
        This function computes shifts from max-projections on overlapping areas. It uses the phase cross-correlation
        to compute the shifts.

        Parameters
        ----------
        proj1: (list of np.array) max-projections for tile 1
        proj2: (list of np.array) max-projections for tile 2
        upsample_factor: (float) upsampling_factor for estimating the maximum phase cross-correlation position

        Returns
        -------
        shifts in (x, y, z) and error measure (0=reliable, 1=not reliable)

        &#34;&#34;&#34;

        # Compute phase cross-correlation to extract shifts
        dzy, error_zy = self._compute_shift(proj1[0], proj2[0])
        dzx, error_zx = self._compute_shift(proj1[1], proj2[1])
        dyx, error_yx = self._compute_shift(proj1[2], proj2[2])

        # Replace error == 0 with 1 otherwise the minimum spanning tree considers that vertex are not connected
        if error_zy == 0:
            error_zy = 1e-6
        if error_zx == 0:
            error_zx = 1e-6
        if error_yx == 0:
            error_yx = 1e-6

        # Keep only the most reliable registration
        # D/z
        if error_zx &lt; error_zy:
            dz = dzx[0]
            rz = error_zx
        else:
            dz = dzy[0]
            rz = error_zy

        # H/x
        if error_zx &lt; error_yx:
            dx = dzx[1]
            rx = error_zx
        else:
            dx = dyx[1]
            rx = error_yx

        # V/y
        if error_yx &lt; error_zy:
            dy = dyx[0]
            ry = error_yx
        else:
            dy = dzy[1]
            ry = error_zy

        # for i, title, vector, err in zip(range(3), [&#39;ZY&#39;, &#39;ZX&#39;, &#39;YX&#39;], [dzy, dzx, dyx], [error_zy, error_zx, error_yx]):
        #     fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
        #     ax[0].imshow(np.log(proj1[i]+1), cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;d={}, e={:0.3f}&#39;.format(vector, err))
        #     ax[1].imshow(np.log(proj2[i]+1), cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)
        #
        #     shifted = warp(proj1[i], AffineTransform(translation=[vector[1], vector[0]]), mode=&#39;wrap&#39;, preserve_range=True)
        #     rgb = np.dstack((np.log(proj2[i]+1), np.log(shifted+1), np.zeros_like(proj1[i])))
        #     ax[2].imshow((rescale_intensity(rgb, out_range=&#39;uint8&#39;)).astype(&#39;uint8&#39;))
        #
        # print(&#39;ok&#39;)

        return np.array([dz, dy, dx]), np.array([rz, ry, rx])

    def _get_masked_proj_shifts(self, proj1, proj2, threshold, upsample_factor=1):
        &#34;&#34;&#34;
        This function computes shifts from max-projections on overlapping areas with mask on brightest area.
        It uses the phase cross-correlation to compute the shifts.

        Parameters
        ----------
        proj1: (list of arrays) max-projections for tile 1
        proj2: (list of arrays) max-projections for tile 2
        upsample_factor: (float) upsampling_factor for estimating the maximum phase cross-correlation position

        Returns
        -------
        shifts in (x, y, z) and error measure (0=reliable, 1=not reliable)

        &#34;&#34;&#34;
        # Compute mask to discard very bright area that are likely bubbles or artefacts
        mask_ref = []
        mask_move = []
        for i in range(3):
            vmax = np.percentile(proj1[i], threshold)
            mask_ref.append(proj1[i] &lt; vmax)
            vmax = np.percentile(proj2[i], threshold)
            mask_move.append(proj2[i] &lt; vmax)

        # Compute phase cross-correlation to extract shifts
        dzy = self.phase_cross_correlation(proj1[0], proj2[0],
                                      return_error=True, upsample_factor=upsample_factor,
                                      reference_mask=mask_ref[0], moving_mask=mask_move[0])
        error_zy = np.sqrt(1 - correlate(proj1[0], proj2[0]).max() ** 2 / (np.sum(proj1 ** 2) * np.sum(proj2 ** 2)))
        dzx = self.phase_cross_correlation(proj1[1], proj2[1],
                                      return_error=True, upsample_factor=upsample_factor,
                                      reference_mask=mask_ref[1], moving_mask=mask_move[1])
        error_zx = np.sqrt(1 - correlate(proj1[1], proj2[1]).max() ** 2 / (np.sum(proj1 ** 2) * np.sum(proj2 ** 2)))
        dyx = self.phase_cross_correlation(proj1[2], proj2[2],
                                      return_error=True, upsample_factor=upsample_factor,
                                      reference_mask=mask_ref[2], moving_mask=mask_move[2])
        error_yx = np.sqrt(1 - correlate(proj1[2], proj2[2]).max() ** 2 / (np.sum(proj1 ** 2) * np.sum(proj2 ** 2)))

        # Replace error == 0 with 1e-6 otherwise the minimum spanning tree considers that vertex are not connected
        if error_zy == 0:
            error_zy = 1e-6
        if error_zx == 0:
            error_zx = 1e-6
        if error_yx == 0:
            error_yx = 1e-6

        # Keep only the most reliable registration
        # D/z
        if error_zx &lt; error_zy:
            dz = dzx[0]
            rz = error_zx
        else:
            dz = dzy[0]
            rz = error_zy

        # H/x
        if error_zx &lt; error_yx:
            dx = dzx[1]
            rx = error_zx
        else:
            dx = dyx[1]
            rx = error_yx

        # V/y
        if error_yx &lt; error_zy:
            dy = dyx[0]
            ry = error_yx
        else:
            dy = dzy[1]
            ry = error_zy

        # for i, title, vector in zip(range(3), [&#39;ZY&#39;, &#39;ZX&#39;, &#39;YX&#39;], [[dy, dz], [dx, dz], [dx, dy]]):
        #     fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
        #     ax[0].imshow(proj1[i], cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;dx={}, dy={}, dz={}&#39;.format(dx, dy, dz))
        #     ax[1].imshow(proj2[i], cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)
        #     from skimage.transform import warp, AffineTransform
        #     from skimage.exposure import rescale_intensity
        #     shifted = warp(proj1[i], AffineTransform(translation=vector), mode=&#39;wrap&#39;, preserve_range=True)
        #     rgb = np.dstack([proj2[i], shifted, np.zeros_like(proj1[i])])
        #     ax[2].imshow((rescale_intensity(rgb, out_range=&#39;uint8&#39;)).astype(&#39;uint8&#39;))
        # print(&#39;ok&#39;)

        return np.array([dz, dy, dx]), np.array([rz, ry, rx])

    def _regularize(self, reg, rel):
        &#34;&#34;&#34;
        Remove too large displacement and replace them with expected one with a large uncertainty.

        &#34;&#34;&#34;
        if np.abs(reg[2] - (self.overlap_h - self.expected_overlap_h)) &gt; self.reg_x:
            reg[2] = (self.overlap_h - self.expected_overlap_h)
            rel[2] = 2
        if np.abs(reg[1] - (self.overlap_v - self.expected_overlap_v)) &gt; self.reg_y:
            reg[1] = (self.overlap_v - self.expected_overlap_v)
            rel[1] = 2
        if np.abs(reg[0]) &gt; self.reg_z:
            reg[0] = 0
            rel[0] = 2

        return reg, rel</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="pipapr.stitcher.channelStitcher" href="#pipapr.stitcher.channelStitcher">channelStitcher</a></li>
<li><a title="pipapr.stitcher.tileStitcher" href="#pipapr.stitcher.tileStitcher">tileStitcher</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pipapr.stitcher.baseStitcher.activate_mask"><code class="name flex">
<span>def <span class="ident">activate_mask</span></span>(<span>self, threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>Activate the masked cross-correlation for the displacement estimation. Pixels above threshold are
not taken into account.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>threshold</code></strong> :&ensp;<code>(int) threshold for the cross-correlation mask as a percentage</code> of <code>pixel to keep (e.g. 95 will</code></dt>
<dd>create a mask removing the 5% brightest pixels).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def activate_mask(self, threshold):
    &#34;&#34;&#34;
    Activate the masked cross-correlation for the displacement estimation. Pixels above threshold are
    not taken into account.

    Parameters
    ----------
    threshold: (int) threshold for the cross-correlation mask as a percentage of pixel to keep (e.g. 95 will
                create a mask removing the 5% brightest pixels).

    &#34;&#34;&#34;
    self.mask = True
    self.threshold = threshold</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.activate_segmentation"><code class="name flex">
<span>def <span class="ident">activate_segmentation</span></span>(<span>self, segmenter)</span>
</code></dt>
<dd>
<div class="desc"><p>Activate the segmentation. When a tile is loaded it is segmented before the stitching is done.</p>
<h2 id="parameters">Parameters</h2>
<p>segmenter: (tileSegmenter) segmenter object for segmenting each tile.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def activate_segmentation(self,
                          segmenter):
    &#34;&#34;&#34;
    Activate the segmentation. When a tile is loaded it is segmented before the stitching is done.

    Parameters
    ----------
    segmenter: (tileSegmenter) segmenter object for segmenting each tile.

    &#34;&#34;&#34;
    self.segment = True
    self.segmenter = segmenter</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.check_files_integrity"><code class="name flex">
<span>def <span class="ident">check_files_integrity</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_files_integrity(self):
    cnt = 0
    for tile in self.tiles:
        lazy = 1
        try:
            tile.lazy_load_tile()
        except:
            lazy = 0
            print(&#39;Lazy load failed on ({}, {})\n Trying normal loading...&#39;.format(tile.row, tile.col))

        if lazy == 0:
            try:
                tile.load_tile()
            except:
                cnt += 1
                print(&#39;Problem detected with tile ({}, {})&#39;.format(tile.row, tile.col))

    if cnt == 0:
        print(&#39;All tiles are readable.&#39;)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.deactivate_mask"><code class="name flex">
<span>def <span class="ident">deactivate_mask</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Deactivate the masked cross-correlation and uses a classical cross correlation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deactivate_mask(self):
    &#34;&#34;&#34;
    Deactivate the masked cross-correlation and uses a classical cross correlation.

    &#34;&#34;&#34;
    self.mask = False
    self.threshold = None</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.deactivate_segmentation"><code class="name flex">
<span>def <span class="ident">deactivate_segmentation</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Deactivate tile segmentation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deactivate_segmentation(self):
    &#34;&#34;&#34;
    Deactivate tile segmentation.

    &#34;&#34;&#34;

    self.segment = False</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.reconstruct_slice"><code class="name flex">
<span>def <span class="ident">reconstruct_slice</span></span>(<span>self, loc=None, n_proj=0, dim=0, downsample=1, color=False, debug=False, plot=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reconstruct_slice(self, loc=None, n_proj=0, dim=0, downsample=1, color=False, debug=False, plot=True):

    if dim == 0:
        return self._reconstruct_z_slice(z=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)
    elif dim == 1:
        return self._reconstruct_y_slice(y=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)
    elif dim == 2:
        return self._reconstruct_x_slice(x=loc, n_proj=n_proj, downsample=downsample, color=color, debug=debug, plot=plot)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.reconstruct_z_color"><code class="name flex">
<span>def <span class="ident">reconstruct_z_color</span></span>(<span>self, z=None, n_proj=0, downsample=1, debug=False, plot=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Reconstruct and merge the sample at a given depth z.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>z</code></strong> :&ensp;<code>(int) reconstruction depth</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>downsample</code></strong> :&ensp;<code>(int) downsample for reconstruction (must be a power</code> of <code>2)</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>debug</code></strong> :&ensp;<code>(bool) if true the border</code> of <code>each tile will be highlighted</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Merged frame at depth z.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reconstruct_z_color(self, z=None, n_proj=0, downsample=1, debug=False, plot=True):
    &#34;&#34;&#34;
    Reconstruct and merge the sample at a given depth z.

    Parameters
    ----------
    z: (int) reconstruction depth
    downsample: (int) downsample for reconstruction (must be a power of 2)
    debug: (bool) if true the border of each tile will be highlighted

    Returns
    -------
    Merged frame at depth z.
    &#34;&#34;&#34;

    level_delta = int(-np.sign(downsample) * np.log2(np.abs(downsample)))

    tile = self.tiles[0]
    tile.lazy_load_tile(level_delta=level_delta)

    if z is None:
        z = int(tile.lazy_data.shape[0] / 2)

    if z &gt; tile.lazy_data.shape[0]:
        raise ValueError(&#39;Error: z is too large ({}), maximum depth at this downsample is {}.&#39;.format(z, tile.lazy_data.shape[0]))

    frame_size = tile.lazy_data.shape[1:]
    x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
    nx = int(np.ceil((x_pos.max() - x_pos.min()) / downsample + frame_size[1]))
    y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
    ny = int(np.ceil((y_pos.max() - y_pos.min()) / downsample + frame_size[0]))

    H = np.zeros((ny, nx), dtype=&#39;uint16&#39;)
    S = np.ones((ny, nx), dtype=&#39;uint16&#39;) * 0.7
    V = np.zeros((ny, nx), dtype=&#39;uint16&#39;)

    H_pos = (x_pos - x_pos.min()) / downsample
    V_pos = (y_pos - y_pos.min()) / downsample

    for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):
        tile.lazy_load_tile(level_delta=level_delta)
        zf = min(z+n_proj, tile.lazy_data.shape[0])
        data = tile.lazy_data[z:zf]
        v = data.max(axis=0)
        h = np.argmax(data, axis=0)

        # In debug mode we highlight each tile edge to see where it was
        if debug:
            v[0, :] = 2**16-1
            v[-1, :] = 2**16-1
            v[:, 0] = 2**16-1
            v[:, -1] = 2**16-1

        x1 = int(H_pos[i])
        x2 = int(H_pos[i] + v.shape[1])
        y1 = int(V_pos[i])
        y2 = int(V_pos[i] + v.shape[0])

        V[y1:y2, x1:x2] = np.maximum(V[y1:y2, x1:x2], v)
        H[y1:y2, x1:x2] = np.maximum(H[y1:y2, x1:x2], h)

    H = rescale_intensity(gaussian(H, sigma=5), out_range=np.float64)*0.66
    V = np.log(V + 200)
    vmin, vmax = np.percentile(V[V &gt; np.log(100)], (1, 99.9))
    V = rescale_intensity(V, in_range=(vmin, vmax), out_range=np.float64)
    S = S * V
    rgb = hsv_to_rgb(np.dstack((H,S,V)))
    rescale_intensity(rgb, out_range=&#39;uint8&#39;)

    if plot:
        plt.figure()
        plt.imshow(rgb)

    return rgb</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.save_database"><code class="name flex">
<span>def <span class="ident">save_database</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Save database at the given path. The database must be built before calling this method.</p>
<h2 id="parameters">Parameters</h2>
<p>path: (str) path to save the database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_database(self, path):
    &#34;&#34;&#34;
    Save database at the given path. The database must be built before calling this method.

    Parameters
    ----------
    path: (str) path to save the database.

    &#34;&#34;&#34;

    if self.database is None:
        raise TypeError(&#39;Error: database can&#39;&#39;t be saved because it was not created. &#39;
                        &#39;Please call build_database() first.&#39;)

    self.database.to_csv(path)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.baseStitcher.set_regularization"><code class="name flex">
<span>def <span class="ident">set_regularization</span></span>(<span>self, reg_x, reg_y, reg_z)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the regularization for the stitching to prevent aberrant displacements.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>reg_x</code></strong> :&ensp;<code>(int) if the horizontal displacement computed in the pairwise registration for any tile is greater than</code></dt>
<dd>reg_x (in pixel unit) then the expected displacement (from motor position) is taken.</dd>
<dt><strong><code>reg_y</code></strong> :&ensp;<code>(int) if the horizontal displacement computed in the pairwise registration for any tile is greater than</code></dt>
<dd>reg_z (in pixel unit) then the expected displacement (from motor position) is taken.</dd>
<dt><strong><code>reg_z</code></strong> :&ensp;<code>(int) if the horizontal displacement computed in the pairwise registration for any tile is greater than</code></dt>
<dd>reg_z (in pixel unit) then the expected displacement (from motor position) is taken.</dd>
</dl>
<h2 id="returns">Returns</h2></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_regularization(self, reg_x, reg_y, reg_z):
    &#34;&#34;&#34;
    Set the regularization for the stitching to prevent aberrant displacements.

    Parameters
    ----------
    reg_x: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                 reg_x (in pixel unit) then the expected displacement (from motor position) is taken.
    reg_y: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                 reg_z (in pixel unit) then the expected displacement (from motor position) is taken.
    reg_z: (int) if the horizontal displacement computed in the pairwise registration for any tile is greater than
                 reg_z (in pixel unit) then the expected displacement (from motor position) is taken.

    Returns
    -------

    &#34;&#34;&#34;

    self.reg_x = reg_x
    self.reg_y = reg_y
    self.reg_z = reg_z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pipapr.stitcher.channelStitcher"><code class="flex name class">
<span>class <span class="ident">channelStitcher</span></span>
<span>(</span><span>stitcher: <a title="pipapr.stitcher.tileStitcher" href="#pipapr.stitcher.tileStitcher">tileStitcher</a>, ref: <a title="pipapr.parser.tileParser" href="parser.html#pipapr.parser.tileParser">tileParser</a>, moving: <a title="pipapr.parser.tileParser" href="parser.html#pipapr.parser.tileParser">tileParser</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Class used to perform the stitching between different channels. The registration must be performed first a single
channel (typically auto-fluorescence)</p>
<p>The stitching is performed between each corresponding tile and the relative displacement is added to the previously
determined stitching parameters.</p>
<p>The number and position of tile should matched for bot dataset.</p>
<p>Constructor for the channelStitcher class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>stitcher</code></strong> :&ensp;<code>(<a title="pipapr.stitcher.tileStitcher" href="#pipapr.stitcher.tileStitcher">tileStitcher</a>) <a title="pipapr.stitcher.tileStitcher" href="#pipapr.stitcher.tileStitcher">tileStitcher</a> object with the multitile registration parameters</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>tiles_stitched</code></strong> :&ensp;<code>(tileParser) tiles corresponding to the stitcher</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>tiles_channel</code></strong> :&ensp;<code>(tileParser) tiles to be registered to tiles_stitched</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class channelStitcher(baseStitcher):
    &#34;&#34;&#34;
    Class used to perform the stitching between different channels. The registration must be performed first a single
    channel (typically auto-fluorescence)

    The stitching is performed between each corresponding tile and the relative displacement is added to the previously
    determined stitching parameters.

    The number and position of tile should matched for bot dataset.
    &#34;&#34;&#34;

    def __init__(self,
                 stitcher: tileStitcher,
                 ref: pipapr.parser.tileParser,
                 moving: pipapr.parser.tileParser):
        &#34;&#34;&#34;
        Constructor for the channelStitcher class.

        Parameters
        ----------
        stitcher: (tileStitcher) tileStitcher object with the multitile registration parameters
        tiles_stitched: (tileParser) tiles corresponding to the stitcher
        tiles_channel: (tileParser) tiles to be registered to tiles_stitched
        &#34;&#34;&#34;

        super().__init__(moving,
                         stitcher.overlap_h,
                         stitcher.overlap_v)

        self.stitcher = stitcher
        self.tiles_ref = ref
        self.database = stitcher.database.copy()
        # Change tiles path for the channel tiles
        self.database.path = self.tiles.path_list

        self.segment = False
        self.segmentation_verbose = None

    def compute_rigid_registration(self):

        for tile1, tile2 in zip(tqdm(self.tiles_ref), self.tiles):

            tile1.load_tile()
            tile2.load_tile()

            if self.segment:
                self.segmenter.compute_segmentation(tile2)

            patch = pyapr.ReconPatch()
            proj1 = self._get_max_proj_apr(tile1.apr, tile1.parts, patch)

            patch = pyapr.ReconPatch()
            proj2 = self._get_max_proj_apr(tile2.apr, tile2.parts, patch)

            if self.mask:
                reg, rel = self._get_masked_proj_shifts(proj1, proj2, self.threshold)
            else:
                reg, rel = self._get_proj_shifts(proj1, proj2)

            reg, rel = self._regularize(reg, rel)



            self._update_database(tile2.row, tile2.col, reg)

    def _update_database(self, row, col, d):
        d = np.concatenate([d, d])
        df = self.database
        for loc, value in zip([&#39;dD&#39;, &#39;dV&#39;, &#39;dH&#39;, &#39;ABS_D&#39;, &#39;ABS_V&#39;, &#39;ABS_H&#39;], d):
            df.loc[(df[&#39;row&#39;] == row) &amp; (df[&#39;col&#39;] == col), loc] += value</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pipapr.stitcher.baseStitcher" href="#pipapr.stitcher.baseStitcher">baseStitcher</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pipapr.stitcher.channelStitcher.compute_rigid_registration"><code class="name flex">
<span>def <span class="ident">compute_rigid_registration</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_rigid_registration(self):

    for tile1, tile2 in zip(tqdm(self.tiles_ref), self.tiles):

        tile1.load_tile()
        tile2.load_tile()

        if self.segment:
            self.segmenter.compute_segmentation(tile2)

        patch = pyapr.ReconPatch()
        proj1 = self._get_max_proj_apr(tile1.apr, tile1.parts, patch)

        patch = pyapr.ReconPatch()
        proj2 = self._get_max_proj_apr(tile2.apr, tile2.parts, patch)

        if self.mask:
            reg, rel = self._get_masked_proj_shifts(proj1, proj2, self.threshold)
        else:
            reg, rel = self._get_proj_shifts(proj1, proj2)

        reg, rel = self._regularize(reg, rel)



        self._update_database(tile2.row, tile2.col, reg)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pipapr.stitcher.baseStitcher" href="#pipapr.stitcher.baseStitcher">baseStitcher</a></b></code>:
<ul class="hlist">
<li><code><a title="pipapr.stitcher.baseStitcher.activate_mask" href="#pipapr.stitcher.baseStitcher.activate_mask">activate_mask</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.activate_segmentation" href="#pipapr.stitcher.baseStitcher.activate_segmentation">activate_segmentation</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.deactivate_mask" href="#pipapr.stitcher.baseStitcher.deactivate_mask">deactivate_mask</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.deactivate_segmentation" href="#pipapr.stitcher.baseStitcher.deactivate_segmentation">deactivate_segmentation</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.reconstruct_z_color" href="#pipapr.stitcher.baseStitcher.reconstruct_z_color">reconstruct_z_color</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.save_database" href="#pipapr.stitcher.baseStitcher.save_database">save_database</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.set_regularization" href="#pipapr.stitcher.baseStitcher.set_regularization">set_regularization</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="pipapr.stitcher.tileMerger"><code class="flex name class">
<span>class <span class="ident">tileMerger</span></span>
<span>(</span><span>tiles, database)</span>
</code></dt>
<dd>
<div class="desc"><p>Class to merge tiles and create a stitched volume. Typically used at a lower resolution for registering
the sample to an Atlas.</p>
<p>Constructor for the tileMerger class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>tiles:
(tileParser) tileParser object containing the dataset to merge.</dt>
<dt><strong><code>database</code></strong> :&ensp;<code>(pd.DataFrame, str) database</code> or <code>path to the database containing the registered tile position</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>n_planes: (int) number of planes per files.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class tileMerger():
    &#34;&#34;&#34;
    Class to merge tiles and create a stitched volume. Typically used at a lower resolution for registering
    the sample to an Atlas.

    &#34;&#34;&#34;
    def __init__(self, tiles, database):
        &#34;&#34;&#34;
        Constructor for the tileMerger class.

        Parameters
        ----------
        tiles:  (tileParser) tileParser object containing the dataset to merge.
        database: (pd.DataFrame, str) database or path to the database containing the registered tile position
        n_planes: (int) number of planes per files.
        &#34;&#34;&#34;

        if isinstance(database, str):
            self.database = pd.read_csv(database)
        else:
            self.database = database
        self.tiles = tiles
        self.lazy = self._find_if_lazy()
        self.type = tiles.type
        self.frame_size = tiles.frame_size
        self.n_planes = self._get_n_planes()
        self.n_tiles = tiles.n_tiles
        self.n_row = tiles.nrow
        self.n_col = tiles.ncol

        # Size of the merged array (to be defined when the merged array is initialized).
        self.nx = None
        self.ny = None
        self.nz = None

        self.downsample = 1
        self.level_delta = 0
        self.merged_data = None

    def merge_additive(self, reconstruction_mode=&#39;constant&#39;, tree_mode=&#39;mean&#39;):
        &#34;&#34;&#34;
        Perform merging with a mean algorithm for overlapping areas. Maximum merging should be preferred to
        avoid integer overflowing and higher signals on the overlapping areas.

        Parameters
        ----------
        mode: (str) APR reconstruction type among (&#39;constant&#39;, &#39;smooth&#39;, &#39;level&#39;)

        Returns
        -------
        None

        &#34;&#34;&#34;

        if self.merged_data is None:
            self._initialize_merged_array()

        H_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        H_pos = (H_pos - H_pos.min())/self.downsample
        V_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        V_pos = (V_pos - V_pos.min())/self.downsample
        D_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        D_pos = (D_pos - D_pos.min())/self.downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):

            if self.type == &#39;apr&#39;:
                if self.lazy:
                    tile.lazy_load_tile(level_delta=self.level_delta)
                    data = tile.lazy_data[:, :, :]
                else:
                    tile.load_tile()
                    u = pyapr.data_containers.APRSlicer(tile.apr, tile.parts, level_delta=self.level_delta,
                                                        mode=reconstruction_mode, tree_mode=tree_mode)
                    data = u[:, :, :]
            else:
                tile.load_tile()
                data = tile.data

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + data.shape[2])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + data.shape[1])
            z1 = int(D_pos[i])
            z2 = int(D_pos[i] + data.shape[0])

            self.merged_data[z1:z2, y1:y2, x1:x2] = self.merged_data[z1:z2, y1:y2, x1:x2] + data
            self.merged_data = self.merged_data.astype(&#39;uint16&#39;)

    def merge_max(self, reconstruction_mode=&#39;constant&#39;, tree_mode=&#39;mean&#39;, debug=False):
        &#34;&#34;&#34;
        Perform merging with a maximum algorithm for overlapping areas.

        Parameters
        ----------
        mode: (str) APR reconstruction type among (&#39;constant&#39;, &#39;smooth&#39;, &#39;level&#39;)
        debug: (bool) add white border on the edge of each tile to see where it was overlapping.

        Returns
        -------
        None

        &#34;&#34;&#34;

        if self.merged_data is None:
            self._initialize_merged_array()

        H_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        H_pos = (H_pos - H_pos.min())/self.downsample
        V_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        V_pos = (V_pos - V_pos.min())/self.downsample
        D_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        D_pos = (D_pos - D_pos.min())/self.downsample

        for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):

            if self.type == &#39;apr&#39;:
                if self.lazy:
                    tile.lazy_load_tile(level_delta=self.level_delta)
                    data = tile.lazy_data[:, :, :]
                else:
                    tile.load_tile()
                    u = pyapr.data_containers.APRSlicer(tile.apr, tile.parts, level_delta=self.level_delta,
                                                        mode=reconstruction_mode, tree_mode=tree_mode)
                    data = u[:, :, :]
            else:
                tile.load_tile()
                data = downscale_local_mean(tile.data, factors=(self.downsample, self.downsample, self.downsample))

            # In debug mode we highlight each tile edge to see where it was
            if debug:
                data[0, :, :] = 2**16-1
                data[-1, :, :] = 2 ** 16 - 1
                data[:, 0, :] = 2 ** 16 - 1
                data[:, -1, :] = 2 ** 16 - 1
                data[:, :, 0] = 2 ** 16 - 1
                data[:, :, -1] = 2 ** 16 - 1

            x1 = int(H_pos[i])
            x2 = int(H_pos[i] + data.shape[2])
            y1 = int(V_pos[i])
            y2 = int(V_pos[i] + data.shape[1])
            z1 = int(D_pos[i])
            z2 = int(D_pos[i] + data.shape[0])

            self.merged_data[z1:z2, y1:y2, x1:x2] = np.maximum(self.merged_data[z1:z2, y1:y2, x1:x2], data)

    def crop(self, background=0, xlim=None, ylim=None, zlim=None):
        &#34;&#34;&#34;
        Add a black mask around the brain (rather than really cropping which makes the overlays complicated in
        a later stage).

        Parameters
        ----------
        background: (int) constant value to replace the cropped area with.
        xlim: (list) x limits for cropping
        ylim: (list) y limits for cropping
        zlim: (list) z limits for cropping

        Returns
        -------
        None
        &#34;&#34;&#34;

        if self.merged_data is None:
            raise TypeError(&#39;Error: please merge data before cropping.&#39;)

        if xlim is not None:
            if xlim[0] != 0:
                self.merged_data[:, :, :xlim[0]] = background
            if xlim[1] != self.merged_data.shape[2]:
                self.merged_data[:, :, xlim[1]:] = background
        if ylim is not None:
            if ylim[0] != 0:
                self.merged_data[:, :ylim[0], :] = background
            if ylim[1] != self.merged_data.shape[1]:
                self.merged_data[:, ylim[1]:, :] = background
        if zlim is not None:
            if zlim[0] != 0:
                self.merged_data[:zlim[0], :, :] = background
            if zlim[1] != self.merged_data.shape[0]:
                self.merged_data[zlim[1]:, :, :] = background

    def equalize_hist(self, method=&#39;skimage&#39;):
        &#34;&#34;&#34;
        Perform histogram equalization to improve the contrast on merged data.
        Both OpenCV (only 2D) and Skimage (3D but 10 times slower) are available.

        Parameters
        ----------
        method: (str) method for performing histogram equalization among &#39;skimage&#39; and &#39;opencv&#39;.

        Returns
        -------
        None
        &#34;&#34;&#34;

        if self.merged_data is None:
            raise TypeError(&#39;Error: please merge data before equalizing histogram.&#39;)

        if method == &#39;opencv&#39;:
            # clahe = cv.createCLAHE(tileGridSize=(8, 8))
            # for i in range(self.merged_data.shape[0]):
                # self.merged_data[i] = clahe.apply(self.merged_data[i])
            print(&#39;opencv not currently supported due to incompatibility with pyqt5&#39;)
        elif method == &#39;skimage&#39;:
            self.merged_data = equalize_adapthist(self.merged_data)
        else:
            raise ValueError(&#39;Error: unknown method for adaptive histogram normalization.&#39;)

    def set_downsample(self, downsample):
        &#34;&#34;&#34;
        Set the downsampling value for the merging reconstruction.

        Parameters
        ----------
        downsample: (int) downsample factor

        Returns
        -------
        None

        &#34;&#34;&#34;

        # TODO: find a more rigorous way of enforcing this. (Probably requires that the APR is loaded).
        if downsample not in [1, 2, 4, 8, 16, 32]:
            raise ValueError(&#39;Error: downsample value should be compatible with APR levels.&#39;)

        self.downsample = downsample
        self.level_delta = int(-np.log2(self.downsample))

    def _initialize_merged_array(self):
        &#34;&#34;&#34;
        Initialize the merged array in accordance with the asked downsampling.

        Returns
        -------
        None
        &#34;&#34;&#34;

        self.nx = int(np.ceil(self._get_nx() / self.downsample))
        self.ny = int(np.ceil(self._get_ny() / self.downsample))
        self.nz = int(np.ceil(self._get_nz() / self.downsample))

        self.merged_data = np.zeros((self.nz, self.ny, self.nx), dtype=&#39;uint16&#39;)

    def _get_nx(self):
        &#34;&#34;&#34;
        Compute the merged array size for x dimension.

        Returns
        -------
        (int) x size for merged array
        &#34;&#34;&#34;
        x_pos = self.database[&#39;ABS_H&#39;].to_numpy()
        return x_pos.max() - x_pos.min() + self.frame_size

    def _get_ny(self):
        &#34;&#34;&#34;
        Compute the merged array size for y dimension.

        Returns
        -------
        (int) y size for merged array
        &#34;&#34;&#34;
        y_pos = self.database[&#39;ABS_V&#39;].to_numpy()
        return y_pos.max() - y_pos.min() + self.frame_size

    def _get_nz(self):
        &#34;&#34;&#34;
        Compute the merged array size for y dimension.

        Returns
        -------
        (int) y size for merged array
        &#34;&#34;&#34;
        z_pos = self.database[&#39;ABS_D&#39;].to_numpy()
        return z_pos.max() - z_pos.min() + self.n_planes

    def _get_n_planes(self):
        &#34;&#34;&#34;
        Load a tile and check the number of planes per tile.

        Returns
        -------
        (int) Number of planes per tile;
        &#34;&#34;&#34;

        tile = self.tiles[0]
        if self.type == &#39;apr&#39;:
            if self.lazy:
                tile.lazy_load_tile()
                return tile.lazy_data.shape[0]
            else:
                tile.load_tile()
                return tile.apr.shape()[0]
        else:
            tile.load_tile()
            return tile.data.shape[0]

    def _find_if_lazy(self):
        &#34;&#34;&#34;
        Function to test if all tile can be lazy loaded.

        Returns
        -------
        (bool)
        &#34;&#34;&#34;

        try:
            for tile in self.tiles:
                tile.lazy_load_tile()
        except:
            return False

        return True</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pipapr.stitcher.tileMerger.crop"><code class="name flex">
<span>def <span class="ident">crop</span></span>(<span>self, background=0, xlim=None, ylim=None, zlim=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Add a black mask around the brain (rather than really cropping which makes the overlays complicated in
a later stage).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>background: (int) constant value to replace the cropped area with.</dt>
<dt><strong><code>xlim</code></strong> :&ensp;<code>(list) x limits for cropping</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>ylim</code></strong> :&ensp;<code>(list) y limits for cropping</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>zlim</code></strong> :&ensp;<code>(list) z limits for cropping</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def crop(self, background=0, xlim=None, ylim=None, zlim=None):
    &#34;&#34;&#34;
    Add a black mask around the brain (rather than really cropping which makes the overlays complicated in
    a later stage).

    Parameters
    ----------
    background: (int) constant value to replace the cropped area with.
    xlim: (list) x limits for cropping
    ylim: (list) y limits for cropping
    zlim: (list) z limits for cropping

    Returns
    -------
    None
    &#34;&#34;&#34;

    if self.merged_data is None:
        raise TypeError(&#39;Error: please merge data before cropping.&#39;)

    if xlim is not None:
        if xlim[0] != 0:
            self.merged_data[:, :, :xlim[0]] = background
        if xlim[1] != self.merged_data.shape[2]:
            self.merged_data[:, :, xlim[1]:] = background
    if ylim is not None:
        if ylim[0] != 0:
            self.merged_data[:, :ylim[0], :] = background
        if ylim[1] != self.merged_data.shape[1]:
            self.merged_data[:, ylim[1]:, :] = background
    if zlim is not None:
        if zlim[0] != 0:
            self.merged_data[:zlim[0], :, :] = background
        if zlim[1] != self.merged_data.shape[0]:
            self.merged_data[zlim[1]:, :, :] = background</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileMerger.equalize_hist"><code class="name flex">
<span>def <span class="ident">equalize_hist</span></span>(<span>self, method='skimage')</span>
</code></dt>
<dd>
<div class="desc"><p>Perform histogram equalization to improve the contrast on merged data.
Both OpenCV (only 2D) and Skimage (3D but 10 times slower) are available.</p>
<h2 id="parameters">Parameters</h2>
<p>method: (str) method for performing histogram equalization among 'skimage' and 'opencv'.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def equalize_hist(self, method=&#39;skimage&#39;):
    &#34;&#34;&#34;
    Perform histogram equalization to improve the contrast on merged data.
    Both OpenCV (only 2D) and Skimage (3D but 10 times slower) are available.

    Parameters
    ----------
    method: (str) method for performing histogram equalization among &#39;skimage&#39; and &#39;opencv&#39;.

    Returns
    -------
    None
    &#34;&#34;&#34;

    if self.merged_data is None:
        raise TypeError(&#39;Error: please merge data before equalizing histogram.&#39;)

    if method == &#39;opencv&#39;:
        # clahe = cv.createCLAHE(tileGridSize=(8, 8))
        # for i in range(self.merged_data.shape[0]):
            # self.merged_data[i] = clahe.apply(self.merged_data[i])
        print(&#39;opencv not currently supported due to incompatibility with pyqt5&#39;)
    elif method == &#39;skimage&#39;:
        self.merged_data = equalize_adapthist(self.merged_data)
    else:
        raise ValueError(&#39;Error: unknown method for adaptive histogram normalization.&#39;)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileMerger.merge_additive"><code class="name flex">
<span>def <span class="ident">merge_additive</span></span>(<span>self, reconstruction_mode='constant', tree_mode='mean')</span>
</code></dt>
<dd>
<div class="desc"><p>Perform merging with a mean algorithm for overlapping areas. Maximum merging should be preferred to
avoid integer overflowing and higher signals on the overlapping areas.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>(str) APR reconstruction type among ('constant', 'smooth', 'level')</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_additive(self, reconstruction_mode=&#39;constant&#39;, tree_mode=&#39;mean&#39;):
    &#34;&#34;&#34;
    Perform merging with a mean algorithm for overlapping areas. Maximum merging should be preferred to
    avoid integer overflowing and higher signals on the overlapping areas.

    Parameters
    ----------
    mode: (str) APR reconstruction type among (&#39;constant&#39;, &#39;smooth&#39;, &#39;level&#39;)

    Returns
    -------
    None

    &#34;&#34;&#34;

    if self.merged_data is None:
        self._initialize_merged_array()

    H_pos = self.database[&#39;ABS_H&#39;].to_numpy()
    H_pos = (H_pos - H_pos.min())/self.downsample
    V_pos = self.database[&#39;ABS_V&#39;].to_numpy()
    V_pos = (V_pos - V_pos.min())/self.downsample
    D_pos = self.database[&#39;ABS_D&#39;].to_numpy()
    D_pos = (D_pos - D_pos.min())/self.downsample

    for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):

        if self.type == &#39;apr&#39;:
            if self.lazy:
                tile.lazy_load_tile(level_delta=self.level_delta)
                data = tile.lazy_data[:, :, :]
            else:
                tile.load_tile()
                u = pyapr.data_containers.APRSlicer(tile.apr, tile.parts, level_delta=self.level_delta,
                                                    mode=reconstruction_mode, tree_mode=tree_mode)
                data = u[:, :, :]
        else:
            tile.load_tile()
            data = tile.data

        x1 = int(H_pos[i])
        x2 = int(H_pos[i] + data.shape[2])
        y1 = int(V_pos[i])
        y2 = int(V_pos[i] + data.shape[1])
        z1 = int(D_pos[i])
        z2 = int(D_pos[i] + data.shape[0])

        self.merged_data[z1:z2, y1:y2, x1:x2] = self.merged_data[z1:z2, y1:y2, x1:x2] + data
        self.merged_data = self.merged_data.astype(&#39;uint16&#39;)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileMerger.merge_max"><code class="name flex">
<span>def <span class="ident">merge_max</span></span>(<span>self, reconstruction_mode='constant', tree_mode='mean', debug=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform merging with a maximum algorithm for overlapping areas.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mode</code></strong> :&ensp;<code>(str) APR reconstruction type among ('constant', 'smooth', 'level')</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>debug: (bool) add white border on the edge of each tile to see where it was overlapping.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def merge_max(self, reconstruction_mode=&#39;constant&#39;, tree_mode=&#39;mean&#39;, debug=False):
    &#34;&#34;&#34;
    Perform merging with a maximum algorithm for overlapping areas.

    Parameters
    ----------
    mode: (str) APR reconstruction type among (&#39;constant&#39;, &#39;smooth&#39;, &#39;level&#39;)
    debug: (bool) add white border on the edge of each tile to see where it was overlapping.

    Returns
    -------
    None

    &#34;&#34;&#34;

    if self.merged_data is None:
        self._initialize_merged_array()

    H_pos = self.database[&#39;ABS_H&#39;].to_numpy()
    H_pos = (H_pos - H_pos.min())/self.downsample
    V_pos = self.database[&#39;ABS_V&#39;].to_numpy()
    V_pos = (V_pos - V_pos.min())/self.downsample
    D_pos = self.database[&#39;ABS_D&#39;].to_numpy()
    D_pos = (D_pos - D_pos.min())/self.downsample

    for i, tile in enumerate(tqdm(self.tiles, desc=&#39;Merging&#39;)):

        if self.type == &#39;apr&#39;:
            if self.lazy:
                tile.lazy_load_tile(level_delta=self.level_delta)
                data = tile.lazy_data[:, :, :]
            else:
                tile.load_tile()
                u = pyapr.data_containers.APRSlicer(tile.apr, tile.parts, level_delta=self.level_delta,
                                                    mode=reconstruction_mode, tree_mode=tree_mode)
                data = u[:, :, :]
        else:
            tile.load_tile()
            data = downscale_local_mean(tile.data, factors=(self.downsample, self.downsample, self.downsample))

        # In debug mode we highlight each tile edge to see where it was
        if debug:
            data[0, :, :] = 2**16-1
            data[-1, :, :] = 2 ** 16 - 1
            data[:, 0, :] = 2 ** 16 - 1
            data[:, -1, :] = 2 ** 16 - 1
            data[:, :, 0] = 2 ** 16 - 1
            data[:, :, -1] = 2 ** 16 - 1

        x1 = int(H_pos[i])
        x2 = int(H_pos[i] + data.shape[2])
        y1 = int(V_pos[i])
        y2 = int(V_pos[i] + data.shape[1])
        z1 = int(D_pos[i])
        z2 = int(D_pos[i] + data.shape[0])

        self.merged_data[z1:z2, y1:y2, x1:x2] = np.maximum(self.merged_data[z1:z2, y1:y2, x1:x2], data)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileMerger.set_downsample"><code class="name flex">
<span>def <span class="ident">set_downsample</span></span>(<span>self, downsample)</span>
</code></dt>
<dd>
<div class="desc"><p>Set the downsampling value for the merging reconstruction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>downsample</code></strong> :&ensp;<code>(int) downsample factor</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_downsample(self, downsample):
    &#34;&#34;&#34;
    Set the downsampling value for the merging reconstruction.

    Parameters
    ----------
    downsample: (int) downsample factor

    Returns
    -------
    None

    &#34;&#34;&#34;

    # TODO: find a more rigorous way of enforcing this. (Probably requires that the APR is loaded).
    if downsample not in [1, 2, 4, 8, 16, 32]:
        raise ValueError(&#39;Error: downsample value should be compatible with APR levels.&#39;)

    self.downsample = downsample
    self.level_delta = int(-np.log2(self.downsample))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pipapr.stitcher.tileStitcher"><code class="flex name class">
<span>class <span class="ident">tileStitcher</span></span>
<span>(</span><span>tiles: <a title="pipapr.parser.tileParser" href="parser.html#pipapr.parser.tileParser">tileParser</a>, overlap_h: (<class 'int'>, <class 'float'>), overlap_v: (<class 'int'>, <class 'float'>))</span>
</code></dt>
<dd>
<div class="desc"><p>Class used to perform the stitching. The stitching is performed in 4 steps:</p>
<ol>
<li>The pairwise registration parameters of each neighboring tile is computed on the max-projection</li>
<li>A sparse graph (edges = tiles and vertex = registration between neighboring tiles) is constructed to store
the registration parameters (displacements and reliability)</li>
<li>The sparse graph is optimized to satisfy the constraints (every loop in the graph should sum to 0) using the
maximum spanning tree on the reliability estimation.</li>
<li>The maximum spanning tree is parsed to extract optimal tile positions solution.</li>
</ol>
<p>The beauty of this method is that it scales well with increasing dataset sizes and because the final optimization
is very fast and does not require to reload the data.</p>
<p>Constructor for the tileStitcher class.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt>tiles: (tileParser) tileParser object containing the dataset to stitch.</dt>
<dt><strong><code>overlap_h</code></strong> :&ensp;<code>(float) expected horizontal overlap in %</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>overlap_v</code></strong> :&ensp;<code>(float) expected vertical overlap in %</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class tileStitcher(baseStitcher):
    &#34;&#34;&#34;
    Class used to perform the stitching. The stitching is performed in 4 steps:

    1. The pairwise registration parameters of each neighboring tile is computed on the max-projection
    2. A sparse graph (edges = tiles and vertex = registration between neighboring tiles) is constructed to store
       the registration parameters (displacements and reliability)
    3. The sparse graph is optimized to satisfy the constraints (every loop in the graph should sum to 0) using the
       maximum spanning tree on the reliability estimation.
    4. The maximum spanning tree is parsed to extract optimal tile positions solution.

    The beauty of this method is that it scales well with increasing dataset sizes and because the final optimization
    is very fast and does not require to reload the data.

    &#34;&#34;&#34;
    def __init__(self,
                 tiles: pipapr.parser.tileParser,
                 overlap_h: (int, float),
                 overlap_v: (int, float)):
        &#34;&#34;&#34;
        Constructor for the tileStitcher class.

        Parameters
        ----------
        tiles: (tileParser) tileParser object containing the dataset to stitch.
        overlap_h: (float) expected horizontal overlap in %
        overlap_v: (float) expected vertical overlap in %
        &#34;&#34;&#34;

        super().__init__(tiles, overlap_h, overlap_v)

        self.cgraph_from = []
        self.cgraph_to = []
        self.relia_H = []
        self.relia_V = []
        self.relia_D = []
        self.dH = []
        self.dV = []
        self.dD = []

        # Attributes below are set when the corresponding method are called.
        self.registration_map_rel = None
        self.registration_map_abs = None
        self.ctree_from_H = None
        self.ctree_from_V = None
        self.ctree_from_D = None
        self.ctree_to_H = None
        self.ctree_to_V = None
        self.ctree_to_D = None
        self.min_tree_H = None
        self.min_tree_V = None
        self.min_tree_D = None
        self.graph_relia_H = None
        self.graph_relia_V = None
        self.graph_relia_D = None
        self.database = None

    def compute_registration(self):
        &#34;&#34;&#34;
        Compute the pair-wise registration for all tiles. This implementation loads the data twice and is therefore
        not efficient.

        &#34;&#34;&#34;
        for tile in tqdm(self.tiles, desc=&#39;Computing stitching&#39;):
            tile.load_tile()
            tile.load_neighbors()

            if self.segment:
                self.segmenter.compute_segmentation(tile)

            for apr, parts, coords in zip(tile.apr_neighbors, tile.parts_neighbors, tile.neighbors):

                if tile.row == coords[0] and tile.col &lt; coords[1]:
                    # EAST
                    reg, rel = self._compute_east_registration(tile.apr, tile.parts, apr, parts)

                elif tile.col == coords[1] and tile.row &lt; coords[0]:
                    # SOUTH
                    reg, rel = self._compute_south_registration(tile.apr, tile.parts, apr, parts)

                else:
                    raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

                # Regularize in cas of aberrant displacements
                reg, rel = self._regularize(reg, rel)

                self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                    dims=(self.nrow, self.ncol)))
                self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                                  dims=(self.nrow, self.ncol)))
                # H=x, V=y, D=z
                self.dH.append(reg[2])
                self.dV.append(reg[1])
                self.dD.append(reg[0])
                self.relia_H.append(rel[2])
                self.relia_V.append(rel[1])
                self.relia_D.append(rel[0])

        self._build_sparse_graphs()
        self._optimize_sparse_graphs()
        _, _ = self._produce_registration_map()
        self._build_database()
        self._print_info()

    def compute_registration_fast(self, on_disk=False):
        &#34;&#34;&#34;
        Compute the pair-wise registration for all tiles. This implementation loads the data once by precomputing
        the max-proj and is therefore efficient.

        &#34;&#34;&#34;
        # First we pre-compute the max-projections and keep them in memory or save them on disk and load them up.
        if on_disk:
            self._save_max_projs()
            projs = self._load_max_projs()
        else:
            projs = self._precompute_max_projs()

        # Then we loop again through the tiles but now we have access to the max-proj
        for tile in tqdm(self.tiles, desc=&#39;Computing cross-correlations&#39;):
            proj1 = projs[tile.row, tile.col]

            for coords in tile.neighbors:
                proj2 = projs[coords[0], coords[1]]

                if tile.row == coords[0] and tile.col &lt; coords[1]:
                    # EAST
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;])

                elif tile.col == coords[1] and tile.row &lt; coords[0]:
                    # SOUTH
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;])

                else:
                    raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

                self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                    dims=(self.nrow, self.ncol)))
                self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                                  dims=(self.nrow, self.ncol)))

                # Regularize in case of aberrant displacements
                reg, rel = self._regularize(reg, rel)

                # H=x, V=y, D=z
                self.dH.append(reg[2])
                self.dV.append(reg[1])
                self.dD.append(reg[0])
                self.relia_H.append(rel[2])
                self.relia_V.append(rel[1])
                self.relia_D.append(rel[0])

        self._build_sparse_graphs()
        self._optimize_sparse_graphs()
        _, _ = self._produce_registration_map()
        self._build_database()
        self._print_info()

    def compute_registration_from_max_projs(self):
        &#39;&#39;&#39;
        Compute the registration directly from the max-projections. Max-projections must have been computed before.

        &#39;&#39;&#39;

        # First we pre-compute the max-projections and keep them in memory or save them on disk and load them up.
        projs = self._load_max_projs()

        # Then we loop again through the tiles but now we have access to the max-proj
        for tile in tqdm(self.tiles, desc=&#39;Compute cross-correlation&#39;):
            proj1 = projs[tile.row, tile.col]

            for coords in tile.neighbors:
                proj2 = projs[coords[0], coords[1]]

                if tile.row == coords[0] and tile.col &lt; coords[1]:
                    # EAST
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;])

                elif tile.col == coords[1] and tile.row &lt; coords[0]:
                    # SOUTH
                    if self.mask:
                        reg, rel = self._get_masked_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;], threshold=self.threshold)
                    else:
                        reg, rel = self._get_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;])

                else:
                    raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

                self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                    dims=(self.nrow, self.ncol)))
                self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                                  dims=(self.nrow, self.ncol)))

                # Regularize in cas of aberrant displacements
                reg, rel = self._regularize(reg, rel)

                # H=x, V=y, D=z
                self.dH.append(reg[2])
                self.dV.append(reg[1])
                self.dD.append(reg[0])
                self.relia_H.append(rel[2])
                self.relia_V.append(rel[1])
                self.relia_D.append(rel[0])

        self._build_sparse_graphs()
        self._optimize_sparse_graphs()
        _, _ = self._produce_registration_map()
        self._build_database()
        self._print_info()

    def compute_expected_registration(self):
        &#34;&#34;&#34;
        Compute the expected registration if the expected overlap are correct.

        &#34;&#34;&#34;

        reg_rel_map = np.zeros((3, self.nrow, self.ncol))

        self.registration_map_rel = reg_rel_map

        reg_abs_map = np.zeros_like(reg_rel_map)
        # H
        for x in range(reg_abs_map.shape[2]):
            reg_abs_map[0, :, x] = reg_rel_map[0, :, x] + x * (self.frame_size - self.expected_overlap_h)
        # V
        for x in range(reg_abs_map.shape[1]):
            reg_abs_map[1, x, :] = reg_rel_map[1, x, :] + x * (self.frame_size - self.expected_overlap_v)
        # D
        reg_abs_map[2] = reg_rel_map[2]
        self.registration_map_abs = reg_abs_map

        self._build_database()

    def plot_graph(self, annotate=False):
        &#34;&#34;&#34;
        Plot the graph for each direction (H, D, V). This method needs to be called after the graph
        optimization.

        Parameters
        ----------
        annotate: (bool) control if annotation are drawn on the graph

        &#34;&#34;&#34;

        if self.graph_relia_H is None:
            raise TypeError(&#39;Error: graph not build yet, please use build_sparse_graph()&#39;
                            &#39;before trying to plot the graph.&#39;)

        fig, ax = plt.subplots(1, 3)
        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ind_from = getattr(self, &#39;cgraph_from&#39;)
            row, col = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
            V1 = np.vstack((row, col)).T

            ind_to = getattr(self, &#39;cgraph_to&#39;)
            row, col = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
            V2 = np.vstack((row, col)).T

            rel = getattr(self, &#39;relia_&#39; + d)
            dX = getattr(self, &#39;d&#39; + d)
            for ii in range(V1.shape[0]):
                ax[i].plot([V1[ii, 1], V2[ii, 1]], [V1[ii, 0], V2[ii, 0]], &#39;ko&#39;, markerfacecolor=&#39;r&#39;)
                if annotate:
                    p1 = ax[i].transData.transform_point([V1[ii, 1], V1[ii, 0]])
                    p2 = ax[i].transData.transform_point([V2[ii, 1], V2[ii, 0]])
                    dy = p2[1]-p1[1]
                    dx = p2[0]-p1[0]
                    rot = np.degrees(np.arctan2(dy, dx))
                    if rel[ii] &lt; 0.15:
                        color = &#39;g&#39;
                    elif rel[ii] &lt; 0.30:
                        color = &#39;orange&#39;
                    else:
                        color = &#39;r&#39;
                    ax[i].annotate(text=&#39;err={:.2f} d{}={:.2f}&#39;.format(rel[ii], d, dX[ii]),
                                   xy=((V1[ii, 1]+V2[ii, 1])/2, (V1[ii, 0]+V2[ii, 0])/2),
                                   ha=&#39;center&#39;,
                                   va=&#39;center&#39;,
                                   fontsize=8,
                                   rotation=rot,
                                   backgroundcolor=&#39;w&#39;,
                                   color=color)
            ax[i].set_title(d + &#39; tree&#39;)
            ax[i].invert_yaxis()

        return fig, ax

    def plot_min_trees(self, annotate=False):
        &#34;&#34;&#34;
        Plot the minimum spanning tree for each direction (H, D, V). This method needs to be called after the graph
        optimization.

        Parameters
        ----------
        annotate: (bool) control if annotation are drawn on the graph

        &#34;&#34;&#34;

        if self.min_tree_H is None:
            raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                            &#39;before trying to plot the trees.&#39;)

        fig, ax = self.plot_graph(annotate=annotate)

        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ind_from = getattr(self, &#39;ctree_from_&#39; + d)
            row, col = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
            V1 = np.vstack((row, col)).T

            ind_to = getattr(self, &#39;ctree_to_&#39; + d)
            row, col = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
            V2 = np.vstack((row, col)).T

            rel = getattr(self, &#39;relia_&#39; + d)
            dX = getattr(self, &#39;d&#39; + d)
            for ii in range(V1.shape[0]):
                ax[i].plot([V1[ii, 1], V2[ii, 1]], [V1[ii, 0], V2[ii, 0]], &#39;ko-&#39;, markerfacecolor=&#39;r&#39;, linewidth=2)
            ax[i].set_title(d + &#39; tree&#39;)

    def plot_stitching_info(self):


        if self.min_tree_H is None:
            raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                            &#39;before trying to plot stitching info.&#39;)

        rel_map = np.zeros((3, self.nrow, self.ncol))
        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ind_from = getattr(self, &#39;ctree_from_&#39; + d)
            ind_to = getattr(self, &#39;ctree_to_&#39; + d)
            graph = getattr(self, &#39;graph_relia_&#39; + d)
            rows, cols = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
            for row, col, i1, i2 in zip(rows, cols, ind_from, ind_to):
                rel = graph[i1, i2]
                rel_map[i, row, col] = np.max((rel_map[i, row, col], rel))
            rows, cols = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
            for row, col, i1, i2 in zip(rows, cols, ind_from, ind_to):
                rel = graph[i1, i2]
                rel_map[i, row, col] = np.max((rel_map[i, row, col], rel))

        fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
        for i in range(3):
            ax[i].imshow(rel_map[i], cmap=&#39;turbo&#39;, vmin=0, vmax=2)

        plt.figure()
        plt.imshow(np.mean(rel_map, axis=0), cmap=&#39;turbo&#39;)
        plt.colorbar()

        return rel_map

    def plot_registration_map(self):
        &#34;&#34;&#34;
        Display the registration map using matplotlib.

        &#34;&#34;&#34;

        if self.registration_map_abs is None:
            raise TypeError(&#39;Error: registration map not computed yet, please use produce_registration_map()&#39;
                            &#39;before trying to display the registration map.&#39;)

        fig, ax = plt.subplots(2, 3)
        for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
            ax[0, i].imshow(self.registration_map_rel[i], cmap=&#39;gray&#39;)
            ax[0, i].set_title(&#39;Rel reg. map &#39; + d)
            ax[1, i].imshow(self.registration_map_abs[i], cmap=&#39;gray&#39;)
            ax[1, i].set_title(&#39;Abs reg. map &#39; + d)

    def dump_stitcher(self, path):
        &#34;&#34;&#34;
        Use dill to store a tgraph object.

        Parameters
        ----------
        path: (str) path to save the database.

        &#34;&#34;&#34;
        if path[-4:] != &#39;.pkl&#39;:
            path = path + &#39;.pkl&#39;

        with open(path, &#39;wb&#39;) as f:
            dill.dump(self, f)

    def set_overlap_margin(self, margin):
        &#34;&#34;&#34;
        Modify the overlaping area size. If the overlaping area is smaller than the true one, the stitching can&#39;t
        be performed properly. If the overlaping area area is more than twice the size of the true one it will also
        fail (due to the circular FFT in the phase cross correlation).

        Parameters
        ----------
        margin: (float) safety margin in % to take the overlaping area.

        Returns
        -------
        None
        &#34;&#34;&#34;
        if margin &gt; 45:
            raise ValueError(&#39;Error: overlap margin is too big and will make the stitching fail.&#39;)
        if margin &lt; 1:
            raise ValueError(&#39;Error: overlap margin is too small and may make the stitching fail.&#39;)

        self.overlap_h = int(self.expected_overlap_h*(1+margin/100))
        if self.expected_overlap_h &gt; self.frame_size:
            self.expected_overlap_h = self.frame_size
        self.overlap_v = int(self.expected_overlap_v*(1+margin/100))
        if self.expected_overlap_v &gt; self.frame_size:
            self.expected_overlap_v = self.frame_size

    def set_z_range(self, z_begin, z_end):
        &#34;&#34;&#34;
        Set a range of depth fo computing the stitching.


        Parameters
        ----------
        z_begin: (int) first depth to be included in the max-proj
        z_end: (int) last depth to be included in the max-proj

        Returns
        -------
        None
        &#34;&#34;&#34;

        self.z_begin = z_begin
        self.z_end = z_end

    def _print_info(self):
        &#34;&#34;&#34;
        Display stitching result information.

        &#34;&#34;&#34;
        overlap = np.median(np.diff(np.median(self.registration_map_abs[0], axis=0)))
        self.effective_overlap_h = (self.frame_size-overlap)/self.frame_size*100
        print(&#39;Effective horizontal overlap: {:0.2f}%&#39;.format(self.effective_overlap_h))
        overlap = np.median(np.diff(np.median(self.registration_map_abs[1], axis=1)))
        self.effective_overlap_v = (self.frame_size-overlap)/self.frame_size*100
        print(&#39;Effective vertical overlap: {:0.2f}%&#39;.format(self.effective_overlap_v))

        if np.abs(self.effective_overlap_v*self.frame_size/100-self.expected_overlap_v)&gt;0.2*self.expected_overlap_v:
            warnings.warn(&#39;Expected vertical overlap is very different from the computed one, the registration &#39;
                          &#39;might be wrong.&#39;)
        if np.abs(self.effective_overlap_h*self.frame_size/100-self.expected_overlap_h)&gt;0.2*self.expected_overlap_h:
            warnings.warn(&#39;Expected horizontal overlap is very different from the computed one, the registration &#39;
                          &#39;might be wrong.&#39;)

    def _save_max_projs(self):

        # Safely create folder to save max-projs
        Path(self.tiles.folder_max_projs).mkdir(parents=True, exist_ok=True)

        for tile in tqdm(self.tiles):
            tile.load_tile()
            proj = {}
            if tile.col + 1 &lt; self.tiles.ncol:
                if self.tiles.tiles_pattern[tile.row, tile.col + 1] == 1:
                    # EAST 1
                    patch = pyapr.ReconPatch()
                    patch.y_begin = self.frame_size - self.overlap_h
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_east_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])
            if tile.col - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row, tile.col - 1] == 1:
                    # EAST 2
                    patch = pyapr.ReconPatch()
                    patch.y_end = self.overlap_h
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_west_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])
            if tile.row + 1 &lt; self.tiles.nrow:
                if self.tiles.tiles_pattern[tile.row + 1, tile.col] == 1:
                    # SOUTH 1
                    patch = pyapr.ReconPatch()
                    patch.x_begin = self.frame_size - self.overlap_v
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_south_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])
            if tile.row - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row - 1, tile.col] == 1:
                    # SOUTH 2
                    patch = pyapr.ReconPatch()
                    patch.x_end = self.overlap_v
                    proj = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        np.save(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_north_{}.npy&#39;.format(tile.row, tile.col, d)), proj[i])

    def _load_max_projs(self):

        projs = np.empty((self.nrow, self.ncol), dtype=object)

        for tile in self.tiles:
            proj = {}
            if tile.col + 1 &lt; self.tiles.ncol:
                if self.tiles.tiles_pattern[tile.row, tile.col + 1] == 1:
                    # EAST 1
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_east_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;east&#39;] = tmp
            if tile.col - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row, tile.col - 1] == 1:
                    # EAST 2
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_west_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;west&#39;] = tmp
            if tile.row + 1 &lt; self.tiles.nrow:
                if self.tiles.tiles_pattern[tile.row + 1, tile.col] == 1:
                    # SOUTH 1
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_south_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;south&#39;] = tmp
            if tile.row - 1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row - 1, tile.col] == 1:
                    # SOUTH 2
                    tmp = []
                    for i, d in enumerate([&#39;zy&#39;, &#39;zx&#39;, &#39;yx&#39;]):
                        tmp.append(np.load(os.path.join(self.tiles.folder_max_projs,
                                             &#39;{}_{}_north_{}.npy&#39;.format(tile.row, tile.col, d))))
                    proj[&#39;north&#39;] = tmp

            projs[tile.row, tile.col] = proj

        return projs

    def _precompute_max_projs(self):
        &#34;&#34;&#34;
        Precompute max-projections for loading the data only once during the stitching.

        &#34;&#34;&#34;

        projs = np.empty((self.nrow, self.ncol), dtype=object)
        for tile in tqdm(self.tiles, desc=&#39;Computing max. proj.&#39;):
            tile.load_tile()
            proj = {}
            if tile.col+1 &lt; self.tiles.ncol:
                if self.tiles.tiles_pattern[tile.row, tile.col+1] == 1:
                    # EAST 1
                    patch = pyapr.ReconPatch()
                    patch.y_begin = self.frame_size - self.overlap_h
                    if self.z_begin is None:
                        proj[&#39;east&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.y_begin = self.frame_size - self.overlap_h
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;east&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)
            if tile.col-1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row, tile.col-1] == 1:
                    # EAST 2
                    patch = pyapr.ReconPatch()
                    patch.y_end = self.overlap_h
                    if self.z_begin is None:
                        proj[&#39;west&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.y_end = self.overlap_h
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;west&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)
            if tile.row+1 &lt; self.tiles.nrow:
                if self.tiles.tiles_pattern[tile.row+1, tile.col] == 1:
                    # SOUTH 1
                    patch = pyapr.ReconPatch()
                    patch.x_begin = self.frame_size - self.overlap_v
                    if self.z_begin is None:
                        proj[&#39;south&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.x_begin = self.frame_size - self.overlap_v
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;south&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)
            if tile.row-1 &gt;= 0:
                if self.tiles.tiles_pattern[tile.row-1, tile.col] == 1:
                    # SOUTH 2
                    patch = pyapr.ReconPatch()
                    patch.x_end = self.overlap_v
                    if self.z_begin is None:
                        proj[&#39;north&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch, plot=False)
                    else:
                        patch_yx = pyapr.ReconPatch()
                        patch_yx.x_end = self.overlap_v
                        patch_yx.z_begin = self.z_begin
                        patch_yx.z_end = self.z_end
                        proj[&#39;north&#39;] = self._get_max_proj_apr(tile.apr, tile.parts, patch=patch, patch_yx=patch_yx,
                                                         plot=False)

            projs[tile.row, tile.col] = proj

            if self.segment:
                self.segmenter.compute_segmentation(tile)

        return projs

    def _build_sparse_graphs(self):
        &#34;&#34;&#34;
        Build the sparse graph from the reliability and (row, col). This method needs to be called after the
        pair-wise registration has been performed for all neighbors pair.

        &#34;&#34;&#34;

        csr_matrix_size = self.ncol*self.nrow
        self.graph_relia_H = csr_matrix((self.relia_H, (self.cgraph_from, self.cgraph_to)),
                                        shape=(csr_matrix_size, csr_matrix_size))
        self.graph_relia_V = csr_matrix((self.relia_V, (self.cgraph_from, self.cgraph_to)),
                                        shape=(csr_matrix_size, csr_matrix_size))
        self.graph_relia_D = csr_matrix((self.relia_D, (self.cgraph_from, self.cgraph_to)),
                                        shape=(csr_matrix_size, csr_matrix_size))

    def _optimize_sparse_graphs(self):
        &#34;&#34;&#34;
        Optimize the sparse graph by computing the minimum spanning tree for each direction (H, D, V). This
        method needs to be called after the sparse graphs have been built.

        &#34;&#34;&#34;

        if self.graph_relia_H is None:
            raise TypeError(&#39;Error: sparse graph not build yet, please use build_sparse_graph() before trying to&#39;
                            &#39;perform the optimization.&#39;)

        for g in [&#39;graph_relia_H&#39;, &#39;graph_relia_V&#39;, &#39;graph_relia_D&#39;]:
            graph = getattr(self, g)
            # Minimum spanning tree
            min_tree = minimum_spanning_tree(graph)

            # Get the &#34;true&#34; neighbors
            min_tree = min_tree.tocoo()
            setattr(self, &#39;min_tree_&#39; + g[-1], min_tree)
            ctree_from = min_tree.row
            setattr(self, &#39;ctree_from_&#39; + g[-1], ctree_from)

            ctree_to = min_tree.col
            setattr(self, &#39;ctree_to_&#39; + g[-1], ctree_to)

    def _produce_registration_map(self):
        &#34;&#34;&#34;
        Produce the registration map where reg_rel_map[d, row, col] (d = H,V,D) is the relative tile
        position in pixel from the expected one. This method needs to be called after the optimization has been done.

        &#34;&#34;&#34;

        if self.min_tree_H is None:
            raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                            &#39;before trying to compute the registration map.&#39;)

        # Relative registration
        # Initialize relative registration map
        reg_rel_map = np.zeros((3, self.nrow, self.ncol)) # H, V, D

        for i, min_tree in enumerate([&#39;min_tree_H&#39;, &#39;min_tree_V&#39;, &#39;min_tree_D&#39;]):
            # Fill it by following the tree and getting the corresponding registration parameters
            node_array = depth_first_order(getattr(self, min_tree), i_start=self.cgraph_from[0],
                                           directed=False, return_predecessors=False)

            node_visited = [node_array[0]]

            tree = getattr(self, min_tree)
            row = tree.row
            col = tree.col

            for node_to in zip(node_array[1:]):
                # The previous node in the MST is a visited node with an edge to the current node
                neighbors = []
                for r, c in zip(row, col):
                    if r == node_to:
                        neighbors.append(c)
                    if c == node_to:
                        neighbors.append(r)
                node_from = [x for x in neighbors if x in node_visited]
                node_visited.append(node_to)

                # Get the previous neighbor local reg parameter
                ind1, ind2 = np.unravel_index(node_from, shape=(self.nrow, self.ncol))
                d_neighbor = reg_rel_map[i, ind1, ind2]

                # Get the current 2D tile position
                ind1, ind2 = np.unravel_index(node_to, shape=(self.nrow, self.ncol))
                # Get the associated ind position in the registration graph (as opposed to the reliability min_tree)
                ind_graph = self._get_ind(node_from, node_to)
                # Get the corresponding reg parameter
                d = getattr(self, &#39;d&#39; + min_tree[-1])[ind_graph]
                # Get the corresponding relia and print a warning if it was regularized:
                relia = getattr(self, &#39;relia_&#39; + min_tree[-1])[ind_graph]
                if relia == 2:
                    print(&#39;Aberrant pair-wise registration remaining after global optimization between tile ({},{}) &#39;
                          &#39;and tile ({},{})&#39;.format(*np.unravel_index(node_from, shape=(self.nrow, self.ncol)),
                                                    *np.unravel_index(node_to, shape=(self.nrow, self.ncol))))
                # Update the local reg parameter in the 2D matrix
                if node_to &gt; node_from[0]:
                    reg_rel_map[i, ind1, ind2] = d_neighbor + d
                else:
                    reg_rel_map[i, ind1, ind2] = d_neighbor - d
        self.registration_map_rel = reg_rel_map

        reg_abs_map = np.zeros_like(reg_rel_map)
        # H
        for x in range(reg_abs_map.shape[2]):
            reg_abs_map[0, :, x] = reg_rel_map[0, :, x] + x * (self.frame_size-self.overlap_h)
        # V
        for x in range(reg_abs_map.shape[1]):
            reg_abs_map[1, x, :] = reg_rel_map[1, x, :] + x * (self.frame_size-self.overlap_v)
        # D
        reg_abs_map[2] = reg_rel_map[2]
        self.registration_map_abs = reg_abs_map

        return reg_rel_map, reg_abs_map

    def _build_database(self):
        &#34;&#34;&#34;
        Build the database for storing the registration parameters. This method needs to be called after
        the registration map has been produced.

        &#34;&#34;&#34;

        if self.registration_map_rel is None:
            raise TypeError(&#39;Error: database can&#39;&#39;t be build if the registration map has not been computed.&#39;
                            &#39; Please use produce_registration_map() method first.&#39;)

        database_dict = {}
        for i in range(self.n_vertex):
            row = self.tiles[i].row
            col = self.tiles[i].col
            database_dict[i] = {&#39;path&#39;: self.tiles[i].path,
                                &#39;row&#39;: row,
                                &#39;col&#39;: col,
                                &#39;dH&#39;: self.registration_map_rel[0, row, col],
                                &#39;dV&#39;: self.registration_map_rel[1, row, col],
                                &#39;dD&#39;: self.registration_map_rel[2, row, col],
                                &#39;ABS_H&#39;: self.registration_map_abs[0, row, col],
                                &#39;ABS_V&#39;: self.registration_map_abs[1, row, col],
                                &#39;ABS_D&#39;: self.registration_map_abs[2, row, col]}

        self.database = pd.DataFrame.from_dict(database_dict, orient=&#39;index&#39;)

        # Finally set the origin so that tile on the edge have coordinate 0 (rather than negative):
        for i, d in enumerate([&#39;ABS_D&#39;, &#39;ABS_V&#39;, &#39;ABS_H&#39;]):
            self.database[d] = self.database[d] - self.database[d].min()

    def _get_ind(self, ind_from, ind_to):
        &#34;&#34;&#34;
        Returns the ind in the original graph which corresponds to (ind_from, ind_to) in the minimum spanning tree.

        Parameters
        ----------
        ind_from: (int) starting node in the directed graph
        ind_to: (int) ending node in the directed graph

        Returns
        ----------
        (int) corresponding ind in the original graph

        &#34;&#34;&#34;

        ind = None
        for i, f in enumerate(self.cgraph_from):
            if f == ind_from:
                if self.cgraph_to[i] == ind_to:
                    ind = i
        if ind is None:
            for i, f in enumerate(self.cgraph_to):
                if f == ind_from:
                    if self.cgraph_from[i] == ind_to:
                        ind = i
        if ind is None:
            raise ValueError(&#39;Error: can&#39;&#39;t find matching vertex pair.&#39;)
        return ind

    def _compute_east_registration(self, apr_1, parts_1, apr_2, parts_2):
        &#34;&#34;&#34;
        Compute the registration between the current tile and its eastern neighbor.

        Parameters
        ----------
        u: (list) current tile
        v: (list) neighboring tile

        Returns
        -------
        None

        &#34;&#34;&#34;

        patch = pyapr.ReconPatch()
        patch.y_begin = self.frame_size - self.overlap_h
        proj_zy1, proj_zx1, proj_yx1 = self._get_max_proj_apr(apr_1, parts_1, patch, plot=False)

        patch = pyapr.ReconPatch()
        patch.y_end = self.overlap_h
        proj_zy2, proj_zx2, proj_yx2 = self._get_max_proj_apr(apr_2, parts_2, patch, plot=False)

        # proj1, proj2 = [proj_zy1, proj_zx1, proj_yx1], [proj_zy2, proj_zx2, proj_yx2]
        # for i, title in enumerate([&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]):
        #     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
        #     ax[0].imshow(proj1[i], cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;EAST&#39;)
        #     ax[1].imshow(proj2[i], cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)

        if self.mask:
            return self._get_masked_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                           [proj_zy2, proj_zx2, proj_yx2],
                                           threshold=self.threshold)
        else:
            return self._get_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                    [proj_zy2, proj_zx2, proj_yx2])

    def _compute_south_registration(self, apr_1, parts_1, apr_2, parts_2):
        &#34;&#34;&#34;
        Compute the registration between the current tile and its southern neighbor.

        Parameters
        ----------
        u: (list) current tile
        v: (list) neighboring tile

        Returns
        -------
        None

        &#34;&#34;&#34;

        patch = pyapr.ReconPatch()
        patch.x_begin = self.frame_size - self.overlap_v
        proj_zy1, proj_zx1, proj_yx1 = self._get_max_proj_apr(apr_1, parts_1, patch, plot=False)

        patch = pyapr.ReconPatch()
        patch.x_end = self.overlap_v
        proj_zy2, proj_zx2, proj_yx2 = self._get_max_proj_apr(apr_2, parts_2, patch, plot=False)

        # proj1, proj2 = [proj_zy1, proj_zx1, proj_yx1], [proj_zy2, proj_zx2, proj_yx2]
        # for i, title in enumerate([&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]):
        #     fig, ax = plt.subplots(1, 2, sharex=True, sharey=True)
        #     ax[0].imshow(proj1[i], cmap=&#39;gray&#39;)
        #     ax[0].set_title(&#39;SOUTH&#39;)
        #     ax[1].imshow(proj2[i], cmap=&#39;gray&#39;)
        #     ax[1].set_title(title)

        if self.mask:
            return self._get_masked_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                           [proj_zy2, proj_zx2, proj_yx2],
                                           threshold=self.threshold)
        else:
            return self._get_proj_shifts([proj_zy1, proj_zx1, proj_yx1],
                                    [proj_zy2, proj_zx2, proj_yx2])</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="pipapr.stitcher.baseStitcher" href="#pipapr.stitcher.baseStitcher">baseStitcher</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="pipapr.stitcher.tileStitcher.compute_expected_registration"><code class="name flex">
<span>def <span class="ident">compute_expected_registration</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the expected registration if the expected overlap are correct.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_expected_registration(self):
    &#34;&#34;&#34;
    Compute the expected registration if the expected overlap are correct.

    &#34;&#34;&#34;

    reg_rel_map = np.zeros((3, self.nrow, self.ncol))

    self.registration_map_rel = reg_rel_map

    reg_abs_map = np.zeros_like(reg_rel_map)
    # H
    for x in range(reg_abs_map.shape[2]):
        reg_abs_map[0, :, x] = reg_rel_map[0, :, x] + x * (self.frame_size - self.expected_overlap_h)
    # V
    for x in range(reg_abs_map.shape[1]):
        reg_abs_map[1, x, :] = reg_rel_map[1, x, :] + x * (self.frame_size - self.expected_overlap_v)
    # D
    reg_abs_map[2] = reg_rel_map[2]
    self.registration_map_abs = reg_abs_map

    self._build_database()</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.compute_registration"><code class="name flex">
<span>def <span class="ident">compute_registration</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the pair-wise registration for all tiles. This implementation loads the data twice and is therefore
not efficient.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_registration(self):
    &#34;&#34;&#34;
    Compute the pair-wise registration for all tiles. This implementation loads the data twice and is therefore
    not efficient.

    &#34;&#34;&#34;
    for tile in tqdm(self.tiles, desc=&#39;Computing stitching&#39;):
        tile.load_tile()
        tile.load_neighbors()

        if self.segment:
            self.segmenter.compute_segmentation(tile)

        for apr, parts, coords in zip(tile.apr_neighbors, tile.parts_neighbors, tile.neighbors):

            if tile.row == coords[0] and tile.col &lt; coords[1]:
                # EAST
                reg, rel = self._compute_east_registration(tile.apr, tile.parts, apr, parts)

            elif tile.col == coords[1] and tile.row &lt; coords[0]:
                # SOUTH
                reg, rel = self._compute_south_registration(tile.apr, tile.parts, apr, parts)

            else:
                raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

            # Regularize in cas of aberrant displacements
            reg, rel = self._regularize(reg, rel)

            self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                dims=(self.nrow, self.ncol)))
            self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                              dims=(self.nrow, self.ncol)))
            # H=x, V=y, D=z
            self.dH.append(reg[2])
            self.dV.append(reg[1])
            self.dD.append(reg[0])
            self.relia_H.append(rel[2])
            self.relia_V.append(rel[1])
            self.relia_D.append(rel[0])

    self._build_sparse_graphs()
    self._optimize_sparse_graphs()
    _, _ = self._produce_registration_map()
    self._build_database()
    self._print_info()</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.compute_registration_fast"><code class="name flex">
<span>def <span class="ident">compute_registration_fast</span></span>(<span>self, on_disk=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the pair-wise registration for all tiles. This implementation loads the data once by precomputing
the max-proj and is therefore efficient.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_registration_fast(self, on_disk=False):
    &#34;&#34;&#34;
    Compute the pair-wise registration for all tiles. This implementation loads the data once by precomputing
    the max-proj and is therefore efficient.

    &#34;&#34;&#34;
    # First we pre-compute the max-projections and keep them in memory or save them on disk and load them up.
    if on_disk:
        self._save_max_projs()
        projs = self._load_max_projs()
    else:
        projs = self._precompute_max_projs()

    # Then we loop again through the tiles but now we have access to the max-proj
    for tile in tqdm(self.tiles, desc=&#39;Computing cross-correlations&#39;):
        proj1 = projs[tile.row, tile.col]

        for coords in tile.neighbors:
            proj2 = projs[coords[0], coords[1]]

            if tile.row == coords[0] and tile.col &lt; coords[1]:
                # EAST
                if self.mask:
                    reg, rel = self._get_masked_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;], threshold=self.threshold)
                else:
                    reg, rel = self._get_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;])

            elif tile.col == coords[1] and tile.row &lt; coords[0]:
                # SOUTH
                if self.mask:
                    reg, rel = self._get_masked_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;], threshold=self.threshold)
                else:
                    reg, rel = self._get_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;])

            else:
                raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

            self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                dims=(self.nrow, self.ncol)))
            self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                              dims=(self.nrow, self.ncol)))

            # Regularize in case of aberrant displacements
            reg, rel = self._regularize(reg, rel)

            # H=x, V=y, D=z
            self.dH.append(reg[2])
            self.dV.append(reg[1])
            self.dD.append(reg[0])
            self.relia_H.append(rel[2])
            self.relia_V.append(rel[1])
            self.relia_D.append(rel[0])

    self._build_sparse_graphs()
    self._optimize_sparse_graphs()
    _, _ = self._produce_registration_map()
    self._build_database()
    self._print_info()</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.compute_registration_from_max_projs"><code class="name flex">
<span>def <span class="ident">compute_registration_from_max_projs</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute the registration directly from the max-projections. Max-projections must have been computed before.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_registration_from_max_projs(self):
    &#39;&#39;&#39;
    Compute the registration directly from the max-projections. Max-projections must have been computed before.

    &#39;&#39;&#39;

    # First we pre-compute the max-projections and keep them in memory or save them on disk and load them up.
    projs = self._load_max_projs()

    # Then we loop again through the tiles but now we have access to the max-proj
    for tile in tqdm(self.tiles, desc=&#39;Compute cross-correlation&#39;):
        proj1 = projs[tile.row, tile.col]

        for coords in tile.neighbors:
            proj2 = projs[coords[0], coords[1]]

            if tile.row == coords[0] and tile.col &lt; coords[1]:
                # EAST
                if self.mask:
                    reg, rel = self._get_masked_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;], threshold=self.threshold)
                else:
                    reg, rel = self._get_proj_shifts(proj1[&#39;east&#39;], proj2[&#39;west&#39;])

            elif tile.col == coords[1] and tile.row &lt; coords[0]:
                # SOUTH
                if self.mask:
                    reg, rel = self._get_masked_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;], threshold=self.threshold)
                else:
                    reg, rel = self._get_proj_shifts(proj1[&#39;south&#39;], proj2[&#39;north&#39;])

            else:
                raise TypeError(&#39;Error: couldn&#39;&#39;t determine registration to perform.&#39;)

            self.cgraph_from.append(np.ravel_multi_index([tile.row, tile.col],
                                                                dims=(self.nrow, self.ncol)))
            self.cgraph_to.append(np.ravel_multi_index([coords[0], coords[1]],
                                                              dims=(self.nrow, self.ncol)))

            # Regularize in cas of aberrant displacements
            reg, rel = self._regularize(reg, rel)

            # H=x, V=y, D=z
            self.dH.append(reg[2])
            self.dV.append(reg[1])
            self.dD.append(reg[0])
            self.relia_H.append(rel[2])
            self.relia_V.append(rel[1])
            self.relia_D.append(rel[0])

    self._build_sparse_graphs()
    self._optimize_sparse_graphs()
    _, _ = self._produce_registration_map()
    self._build_database()
    self._print_info()</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.dump_stitcher"><code class="name flex">
<span>def <span class="ident">dump_stitcher</span></span>(<span>self, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Use dill to store a tgraph object.</p>
<h2 id="parameters">Parameters</h2>
<p>path: (str) path to save the database.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_stitcher(self, path):
    &#34;&#34;&#34;
    Use dill to store a tgraph object.

    Parameters
    ----------
    path: (str) path to save the database.

    &#34;&#34;&#34;
    if path[-4:] != &#39;.pkl&#39;:
        path = path + &#39;.pkl&#39;

    with open(path, &#39;wb&#39;) as f:
        dill.dump(self, f)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.plot_graph"><code class="name flex">
<span>def <span class="ident">plot_graph</span></span>(<span>self, annotate=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the graph for each direction (H, D, V). This method needs to be called after the graph
optimization.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>annotate</code></strong> :&ensp;<code>(bool) control if annotation are drawn on the graph</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_graph(self, annotate=False):
    &#34;&#34;&#34;
    Plot the graph for each direction (H, D, V). This method needs to be called after the graph
    optimization.

    Parameters
    ----------
    annotate: (bool) control if annotation are drawn on the graph

    &#34;&#34;&#34;

    if self.graph_relia_H is None:
        raise TypeError(&#39;Error: graph not build yet, please use build_sparse_graph()&#39;
                        &#39;before trying to plot the graph.&#39;)

    fig, ax = plt.subplots(1, 3)
    for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
        ind_from = getattr(self, &#39;cgraph_from&#39;)
        row, col = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
        V1 = np.vstack((row, col)).T

        ind_to = getattr(self, &#39;cgraph_to&#39;)
        row, col = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
        V2 = np.vstack((row, col)).T

        rel = getattr(self, &#39;relia_&#39; + d)
        dX = getattr(self, &#39;d&#39; + d)
        for ii in range(V1.shape[0]):
            ax[i].plot([V1[ii, 1], V2[ii, 1]], [V1[ii, 0], V2[ii, 0]], &#39;ko&#39;, markerfacecolor=&#39;r&#39;)
            if annotate:
                p1 = ax[i].transData.transform_point([V1[ii, 1], V1[ii, 0]])
                p2 = ax[i].transData.transform_point([V2[ii, 1], V2[ii, 0]])
                dy = p2[1]-p1[1]
                dx = p2[0]-p1[0]
                rot = np.degrees(np.arctan2(dy, dx))
                if rel[ii] &lt; 0.15:
                    color = &#39;g&#39;
                elif rel[ii] &lt; 0.30:
                    color = &#39;orange&#39;
                else:
                    color = &#39;r&#39;
                ax[i].annotate(text=&#39;err={:.2f} d{}={:.2f}&#39;.format(rel[ii], d, dX[ii]),
                               xy=((V1[ii, 1]+V2[ii, 1])/2, (V1[ii, 0]+V2[ii, 0])/2),
                               ha=&#39;center&#39;,
                               va=&#39;center&#39;,
                               fontsize=8,
                               rotation=rot,
                               backgroundcolor=&#39;w&#39;,
                               color=color)
        ax[i].set_title(d + &#39; tree&#39;)
        ax[i].invert_yaxis()

    return fig, ax</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.plot_min_trees"><code class="name flex">
<span>def <span class="ident">plot_min_trees</span></span>(<span>self, annotate=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the minimum spanning tree for each direction (H, D, V). This method needs to be called after the graph
optimization.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>annotate</code></strong> :&ensp;<code>(bool) control if annotation are drawn on the graph</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_min_trees(self, annotate=False):
    &#34;&#34;&#34;
    Plot the minimum spanning tree for each direction (H, D, V). This method needs to be called after the graph
    optimization.

    Parameters
    ----------
    annotate: (bool) control if annotation are drawn on the graph

    &#34;&#34;&#34;

    if self.min_tree_H is None:
        raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                        &#39;before trying to plot the trees.&#39;)

    fig, ax = self.plot_graph(annotate=annotate)

    for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
        ind_from = getattr(self, &#39;ctree_from_&#39; + d)
        row, col = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
        V1 = np.vstack((row, col)).T

        ind_to = getattr(self, &#39;ctree_to_&#39; + d)
        row, col = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
        V2 = np.vstack((row, col)).T

        rel = getattr(self, &#39;relia_&#39; + d)
        dX = getattr(self, &#39;d&#39; + d)
        for ii in range(V1.shape[0]):
            ax[i].plot([V1[ii, 1], V2[ii, 1]], [V1[ii, 0], V2[ii, 0]], &#39;ko-&#39;, markerfacecolor=&#39;r&#39;, linewidth=2)
        ax[i].set_title(d + &#39; tree&#39;)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.plot_registration_map"><code class="name flex">
<span>def <span class="ident">plot_registration_map</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Display the registration map using matplotlib.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_registration_map(self):
    &#34;&#34;&#34;
    Display the registration map using matplotlib.

    &#34;&#34;&#34;

    if self.registration_map_abs is None:
        raise TypeError(&#39;Error: registration map not computed yet, please use produce_registration_map()&#39;
                        &#39;before trying to display the registration map.&#39;)

    fig, ax = plt.subplots(2, 3)
    for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
        ax[0, i].imshow(self.registration_map_rel[i], cmap=&#39;gray&#39;)
        ax[0, i].set_title(&#39;Rel reg. map &#39; + d)
        ax[1, i].imshow(self.registration_map_abs[i], cmap=&#39;gray&#39;)
        ax[1, i].set_title(&#39;Abs reg. map &#39; + d)</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.plot_stitching_info"><code class="name flex">
<span>def <span class="ident">plot_stitching_info</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_stitching_info(self):


    if self.min_tree_H is None:
        raise TypeError(&#39;Error: minimum spanning tree not computed yet, please use optimize_sparse_graph()&#39;
                        &#39;before trying to plot stitching info.&#39;)

    rel_map = np.zeros((3, self.nrow, self.ncol))
    for i, d in enumerate([&#39;H&#39;, &#39;V&#39;, &#39;D&#39;]):
        ind_from = getattr(self, &#39;ctree_from_&#39; + d)
        ind_to = getattr(self, &#39;ctree_to_&#39; + d)
        graph = getattr(self, &#39;graph_relia_&#39; + d)
        rows, cols = np.unravel_index(ind_to, shape=(self.nrow, self.ncol))
        for row, col, i1, i2 in zip(rows, cols, ind_from, ind_to):
            rel = graph[i1, i2]
            rel_map[i, row, col] = np.max((rel_map[i, row, col], rel))
        rows, cols = np.unravel_index(ind_from, shape=(self.nrow, self.ncol))
        for row, col, i1, i2 in zip(rows, cols, ind_from, ind_to):
            rel = graph[i1, i2]
            rel_map[i, row, col] = np.max((rel_map[i, row, col], rel))

    fig, ax = plt.subplots(1, 3, sharex=True, sharey=True)
    for i in range(3):
        ax[i].imshow(rel_map[i], cmap=&#39;turbo&#39;, vmin=0, vmax=2)

    plt.figure()
    plt.imshow(np.mean(rel_map, axis=0), cmap=&#39;turbo&#39;)
    plt.colorbar()

    return rel_map</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.set_overlap_margin"><code class="name flex">
<span>def <span class="ident">set_overlap_margin</span></span>(<span>self, margin)</span>
</code></dt>
<dd>
<div class="desc"><p>Modify the overlaping area size. If the overlaping area is smaller than the true one, the stitching can't
be performed properly. If the overlaping area area is more than twice the size of the true one it will also
fail (due to the circular FFT in the phase cross correlation).</p>
<h2 id="parameters">Parameters</h2>
<p>margin: (float) safety margin in % to take the overlaping area.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_overlap_margin(self, margin):
    &#34;&#34;&#34;
    Modify the overlaping area size. If the overlaping area is smaller than the true one, the stitching can&#39;t
    be performed properly. If the overlaping area area is more than twice the size of the true one it will also
    fail (due to the circular FFT in the phase cross correlation).

    Parameters
    ----------
    margin: (float) safety margin in % to take the overlaping area.

    Returns
    -------
    None
    &#34;&#34;&#34;
    if margin &gt; 45:
        raise ValueError(&#39;Error: overlap margin is too big and will make the stitching fail.&#39;)
    if margin &lt; 1:
        raise ValueError(&#39;Error: overlap margin is too small and may make the stitching fail.&#39;)

    self.overlap_h = int(self.expected_overlap_h*(1+margin/100))
    if self.expected_overlap_h &gt; self.frame_size:
        self.expected_overlap_h = self.frame_size
    self.overlap_v = int(self.expected_overlap_v*(1+margin/100))
    if self.expected_overlap_v &gt; self.frame_size:
        self.expected_overlap_v = self.frame_size</code></pre>
</details>
</dd>
<dt id="pipapr.stitcher.tileStitcher.set_z_range"><code class="name flex">
<span>def <span class="ident">set_z_range</span></span>(<span>self, z_begin, z_end)</span>
</code></dt>
<dd>
<div class="desc"><p>Set a range of depth fo computing the stitching.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>z_begin</code></strong> :&ensp;<code>(int) first depth to be included in the max-proj</code></dt>
<dd>&nbsp;</dd>
<dt><strong><code>z_end</code></strong> :&ensp;<code>(int) last depth to be included in the max-proj</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_z_range(self, z_begin, z_end):
    &#34;&#34;&#34;
    Set a range of depth fo computing the stitching.


    Parameters
    ----------
    z_begin: (int) first depth to be included in the max-proj
    z_end: (int) last depth to be included in the max-proj

    Returns
    -------
    None
    &#34;&#34;&#34;

    self.z_begin = z_begin
    self.z_end = z_end</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="pipapr.stitcher.baseStitcher" href="#pipapr.stitcher.baseStitcher">baseStitcher</a></b></code>:
<ul class="hlist">
<li><code><a title="pipapr.stitcher.baseStitcher.activate_mask" href="#pipapr.stitcher.baseStitcher.activate_mask">activate_mask</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.activate_segmentation" href="#pipapr.stitcher.baseStitcher.activate_segmentation">activate_segmentation</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.deactivate_mask" href="#pipapr.stitcher.baseStitcher.deactivate_mask">deactivate_mask</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.deactivate_segmentation" href="#pipapr.stitcher.baseStitcher.deactivate_segmentation">deactivate_segmentation</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.reconstruct_z_color" href="#pipapr.stitcher.baseStitcher.reconstruct_z_color">reconstruct_z_color</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.save_database" href="#pipapr.stitcher.baseStitcher.save_database">save_database</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.set_regularization" href="#pipapr.stitcher.baseStitcher.set_regularization">set_regularization</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pipapr" href="index.html">pipapr</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="pipapr.stitcher.max_sum_over_single_max" href="#pipapr.stitcher.max_sum_over_single_max">max_sum_over_single_max</a></code></li>
<li><code><a title="pipapr.stitcher.mse" href="#pipapr.stitcher.mse">mse</a></code></li>
<li><code><a title="pipapr.stitcher.phase_cross_correlation" href="#pipapr.stitcher.phase_cross_correlation">phase_cross_correlation</a></code></li>
<li><code><a title="pipapr.stitcher.phase_cross_correlation_cv" href="#pipapr.stitcher.phase_cross_correlation_cv">phase_cross_correlation_cv</a></code></li>
<li><code><a title="pipapr.stitcher.reconstruct_middle_frame" href="#pipapr.stitcher.reconstruct_middle_frame">reconstruct_middle_frame</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pipapr.stitcher.baseStitcher" href="#pipapr.stitcher.baseStitcher">baseStitcher</a></code></h4>
<ul class="">
<li><code><a title="pipapr.stitcher.baseStitcher.activate_mask" href="#pipapr.stitcher.baseStitcher.activate_mask">activate_mask</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.activate_segmentation" href="#pipapr.stitcher.baseStitcher.activate_segmentation">activate_segmentation</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.check_files_integrity" href="#pipapr.stitcher.baseStitcher.check_files_integrity">check_files_integrity</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.deactivate_mask" href="#pipapr.stitcher.baseStitcher.deactivate_mask">deactivate_mask</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.deactivate_segmentation" href="#pipapr.stitcher.baseStitcher.deactivate_segmentation">deactivate_segmentation</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.reconstruct_slice" href="#pipapr.stitcher.baseStitcher.reconstruct_slice">reconstruct_slice</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.reconstruct_z_color" href="#pipapr.stitcher.baseStitcher.reconstruct_z_color">reconstruct_z_color</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.save_database" href="#pipapr.stitcher.baseStitcher.save_database">save_database</a></code></li>
<li><code><a title="pipapr.stitcher.baseStitcher.set_regularization" href="#pipapr.stitcher.baseStitcher.set_regularization">set_regularization</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pipapr.stitcher.channelStitcher" href="#pipapr.stitcher.channelStitcher">channelStitcher</a></code></h4>
<ul class="">
<li><code><a title="pipapr.stitcher.channelStitcher.compute_rigid_registration" href="#pipapr.stitcher.channelStitcher.compute_rigid_registration">compute_rigid_registration</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pipapr.stitcher.tileMerger" href="#pipapr.stitcher.tileMerger">tileMerger</a></code></h4>
<ul class="">
<li><code><a title="pipapr.stitcher.tileMerger.crop" href="#pipapr.stitcher.tileMerger.crop">crop</a></code></li>
<li><code><a title="pipapr.stitcher.tileMerger.equalize_hist" href="#pipapr.stitcher.tileMerger.equalize_hist">equalize_hist</a></code></li>
<li><code><a title="pipapr.stitcher.tileMerger.merge_additive" href="#pipapr.stitcher.tileMerger.merge_additive">merge_additive</a></code></li>
<li><code><a title="pipapr.stitcher.tileMerger.merge_max" href="#pipapr.stitcher.tileMerger.merge_max">merge_max</a></code></li>
<li><code><a title="pipapr.stitcher.tileMerger.set_downsample" href="#pipapr.stitcher.tileMerger.set_downsample">set_downsample</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pipapr.stitcher.tileStitcher" href="#pipapr.stitcher.tileStitcher">tileStitcher</a></code></h4>
<ul class="">
<li><code><a title="pipapr.stitcher.tileStitcher.compute_expected_registration" href="#pipapr.stitcher.tileStitcher.compute_expected_registration">compute_expected_registration</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.compute_registration" href="#pipapr.stitcher.tileStitcher.compute_registration">compute_registration</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.compute_registration_fast" href="#pipapr.stitcher.tileStitcher.compute_registration_fast">compute_registration_fast</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.compute_registration_from_max_projs" href="#pipapr.stitcher.tileStitcher.compute_registration_from_max_projs">compute_registration_from_max_projs</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.dump_stitcher" href="#pipapr.stitcher.tileStitcher.dump_stitcher">dump_stitcher</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.plot_graph" href="#pipapr.stitcher.tileStitcher.plot_graph">plot_graph</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.plot_min_trees" href="#pipapr.stitcher.tileStitcher.plot_min_trees">plot_min_trees</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.plot_registration_map" href="#pipapr.stitcher.tileStitcher.plot_registration_map">plot_registration_map</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.plot_stitching_info" href="#pipapr.stitcher.tileStitcher.plot_stitching_info">plot_stitching_info</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.set_overlap_margin" href="#pipapr.stitcher.tileStitcher.set_overlap_margin">set_overlap_margin</a></code></li>
<li><code><a title="pipapr.stitcher.tileStitcher.set_z_range" href="#pipapr.stitcher.tileStitcher.set_z_range">set_z_range</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>